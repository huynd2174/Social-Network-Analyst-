"""
Script crawl infobox t·ª´ Wikipedia cho c√°c node Group/Artist.

Quy tr√¨nh:
1. Load file korean_artists_graph_bfs.json
2. T√¨m c√°c node c√≥ label = "Group" ho·∫∑c "Artist"
3. L·∫•y URL Wikipedia c·ªßa t·ª´ng node
4. Truy c·∫≠p Wikipedia API ƒë·ªÉ l·∫•y wikitext
5. Parse infobox, l·∫•y c√°c tr∆∞·ªùng c·∫ßn thi·∫øt:
   - Group: Th√†nh vi√™n, C·ª±u th√†nh vi√™n
   - Artist: Th√†nh vi√™n c·ªßa, C·ª±u th√†nh vi√™n c·ªßa
6. L∆∞u k·∫øt qu·∫£ v√†o file m·ªõi: infobox_members.json
"""

import json
import re
import time
from urllib.parse import urlparse, unquote

import requests


# Wikipedia API endpoint
WIKI_API = "https://vi.wikipedia.org/w/api.php"

# User-Agent b·∫Øt bu·ªôc ƒë·ªÉ Wikipedia kh√¥ng ch·∫∑n 403
HEADERS = {
    "User-Agent": "KpopNetworkAnalyzer/1.0 (Educational project) Python/requests"
}

# C√°c tr∆∞·ªùng c·∫ßn l·∫•y cho GROUP
GROUP_KEYS = [
    "Th√†nh vi√™n",
    "C·ª±u th√†nh vi√™n",
    "Th√†nh vi√™n hi·ªán t·∫°i",
    "Th√†nh vi√™n c≈©",
    "Th√†nh vi√™n ban ƒë·∫ßu",
    "Members",
    "Former members",
    "Current members",
    "Past members",
]

# C√°c tr∆∞·ªùng c·∫ßn l·∫•y cho ARTIST
ARTIST_KEYS = [
    "Th√†nh vi√™n c·ªßa",
    "C·ª±u th√†nh vi√™n c·ªßa",
    "Nh√≥m nh·∫°c",
    "Group",
    "Groups",
    "Associated acts",
]


def get_title_from_url(url: str) -> str | None:
    """L·∫•y title Wikipedia t·ª´ URL."""
    if not url:
        return None
    try:
        path = urlparse(url).path  # /wiki/BTS
        if "/wiki/" not in path:
            return None
        title = path.split("/wiki/")[-1]
        return unquote(title) if title else None
    except Exception:
        return None


def fetch_wikitext(url: str) -> str | None:
    """
    Truy c·∫≠p Wikipedia API ƒë·ªÉ l·∫•y wikitext c·ªßa trang.
    """
    title = get_title_from_url(url)
    if not title:
        return None

    params = {
        "action": "query",
        "prop": "revisions",
        "rvprop": "content",
        "rvslots": "main",
        "format": "json",
        "formatversion": "2",
        "titles": title,
    }

    try:
        resp = requests.get(WIKI_API, params=params, headers=HEADERS, timeout=15)
        resp.raise_for_status()
        data = resp.json()

        pages = data.get("query", {}).get("pages", [])
        if not pages:
            return None

        revs = pages[0].get("revisions", [])
        if not revs:
            return None

        slots = revs[0].get("slots", {})
        content = slots.get("main", {}).get("*") or slots.get("main", {}).get("content")
        return content

    except Exception as e:
        print(f"    [!] L·ªói: {e}")
        return None


def clean_value(value: str) -> str:
    """L√†m s·∫°ch gi√° tr·ªã infobox."""
    text = value or ""

    # B·ªè <ref>...</ref>
    text = re.sub(r"<ref[^>]*>.*?</ref>", "", text, flags=re.DOTALL | re.IGNORECASE)
    text = re.sub(r"<ref[^/>]*/>", "", text, flags=re.IGNORECASE)

    # Thay <br> b·∫±ng d·∫•u ph·∫©y
    text = re.sub(r"<br\s*/?>", ", ", text, flags=re.IGNORECASE)

    # B·ªè HTML tags kh√°c
    text = re.sub(r"</?[^>]+>", "", text)

    # B·ªè templates {{...}} (l·∫∑p nhi·ªÅu l·∫ßn ƒë·ªÉ x·ª≠ l√Ω nested)
    for _ in range(5):
        text = re.sub(r"\{\{[^{}]*\}\}", "", text)

    # X·ª≠ l√Ω wiki links [[...|display]] ho·∫∑c [[link]]
    text = re.sub(r"\[\[([^|\]]*\|)?([^\]]+)\]\]", r"\2", text)

    # C·∫Øt b·ªè ph·∫ßn d∆∞ sau '}}' ho·∫∑c '|module=...' / '|Past_members'... (template th·ª´a)
    text = re.split(r"\}\}", text, 1)[0]
    text = re.split(r"\|\s*(module|Past_members|child|embed)\b", text, 1)[0]

    # Thay d·∫•u * (bullet) th√†nh d·∫•u ph·∫©y
    # V√≠ d·ª•: "* Jin * Suga * J-Hope" -> "Jin, Suga, J-Hope"
    text = re.sub(r"^\s*\*\s*", "", text)         # b·ªè * ƒë·∫ßu d√≤ng
    text = re.sub(r"\s*\*\s*", ", ", text)        # c√°c * c√≤n l·∫°i -> d·∫•u ph·∫©y

    # Chu·∫©n h√≥a d·∫•u ph·∫©y v√† kho·∫£ng tr·∫Øng
    text = re.sub(r"\s*,\s*", ", ", text)
    text = re.sub(r",\s*,+", ", ", text)

    # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
    text = re.sub(r"\s+", " ", text).strip(" ,")

    # S·ª≠a m·ªôt s·ªë l·ªói ƒë·∫∑c bi·ªát
    fixes = {
        "New , Jeans": "NewJeans",
        "New, Jeans": "NewJeans",
        "i , KON": "iKON",
        "i, KON": "iKON",
        "Gugudan Se , Mi , Na": "Gugudan SeMiNa",
        "Gugudan Se, Mi, Na": "Gugudan SeMiNa",
    }
    for bad, good in fixes.items():
        text = text.replace(bad, good)

    return text


def parse_infobox(wikitext: str, keys: list[str]) -> dict[str, str]:
    """
    Parse infobox t·ª´ wikitext, l·∫•y c√°c tr∆∞·ªùng theo keys.
    """
    if not wikitext:
        return {}

    # T√¨m v·ªã tr√≠ b·∫Øt ƒë·∫ßu c·ªßa infobox
    match = re.search(r"\{\{[Ii]nfobox", wikitext)
    if not match:
        return {}

    # ƒê·∫øm ngo·∫∑c ƒë·ªÉ t√¨m ƒëi·ªÉm k·∫øt th√∫c
    start = match.start()
    depth = 0
    end = start
    i = start

    while i < len(wikitext):
        if wikitext[i:i+2] == "{{":
            depth += 1
            i += 2
        elif wikitext[i:i+2] == "}}":
            depth -= 1
            i += 2
            if depth == 0:
                end = i
                break
        else:
            i += 1

    if end <= start:
        infobox_text = wikitext[start:start+3000]
    else:
        infobox_text = wikitext[start:end]

    # Parse c√°c tham s·ªë | key = value
    params = {}
    pattern = r"^\|\s*([^=\n]+?)\s*=\s*"
    lines = infobox_text.split("\n")
    current_key = None
    current_value_lines = []

    for line in lines:
        m = re.match(pattern, line)
        if m:
            if current_key:
                params[current_key.strip()] = "\n".join(current_value_lines).strip()
            current_key = m.group(1)
            rest = line[m.end():]
            current_value_lines = [rest]
        elif current_key:
            current_value_lines.append(line)

    if current_key:
        params[current_key.strip()] = "\n".join(current_value_lines).strip()

    # L·∫•y c√°c tr∆∞·ªùng c·∫ßn thi·∫øt
    result = {}
    for key in keys:
        variants = [key, key.lower(), key.replace(" ", "_"), key.replace(" ", "_").lower()]
        for var in variants:
            for param_key, param_val in params.items():
                if param_key.lower().strip() == var.lower().strip():
                    cleaned = clean_value(param_val)
                    if cleaned:
                        result[key] = cleaned
                    break
            if key in result:
                break

    return result


def main():
    print("=" * 60)
    print("CRAWL INFOBOX TH√ÄNH VI√äN T·ª™ WIKIPEDIA")
    print("=" * 60)

    # 1. Load file g·ªëc
    print("\nüìÇ B∆∞·ªõc 1: Load file korean_artists_graph_bfs.json...")
    with open("korean_artists_graph_bfs.json", "r", encoding="utf-8") as f:
        graph = json.load(f)

    nodes = graph.get("nodes", graph)
    print(f"   ‚úì T·ªïng c·ªông {len(nodes)} nodes")

    # 2. T√¨m c√°c node Group v√† Artist c√≥ URL
    print("\nüîç B∆∞·ªõc 2: T√¨m c√°c node Group/Artist c√≥ URL...")
    groups_to_crawl = []
    artists_to_crawl = []

    for node_id, node in nodes.items():
        if not isinstance(node, dict):
            continue

        label = node.get("label")
        url = node.get("url")

        if not url:
            continue

        if label == "Group":
            groups_to_crawl.append((node_id, url))
        elif label == "Artist":
            artists_to_crawl.append((node_id, url))

    print(f"   ‚úì T√¨m th·∫•y {len(groups_to_crawl)} Groups c√≥ URL")
    print(f"   ‚úì T√¨m th·∫•y {len(artists_to_crawl)} Artists c√≥ URL")

    # 3. Crawl infobox
    print("\nüåê B∆∞·ªõc 3: Crawl infobox t·ª´ Wikipedia...")
    results = {
        "groups": {},
        "artists": {},
    }

    # Crawl Groups
    print(f"\n   --- Crawling {len(groups_to_crawl)} Groups ---")
    for idx, (node_id, url) in enumerate(groups_to_crawl, 1):
        if idx % 20 == 0 or idx == 1:
            print(f"   [{idx}/{len(groups_to_crawl)}] {node_id[:40]}...")

        wikitext = fetch_wikitext(url)
        if wikitext:
            info = parse_infobox(wikitext, GROUP_KEYS)
            if info:
                results["groups"][node_id] = {
                    "url": url,
                    "infobox": info,
                }

        time.sleep(0.5)  # Tr√°nh spam API

    # Crawl Artists
    print(f"\n   --- Crawling {len(artists_to_crawl)} Artists ---")
    for idx, (node_id, url) in enumerate(artists_to_crawl, 1):
        if idx % 20 == 0 or idx == 1:
            print(f"   [{idx}/{len(artists_to_crawl)}] {node_id[:40]}...")

        wikitext = fetch_wikitext(url)
        if wikitext:
            info = parse_infobox(wikitext, ARTIST_KEYS)
            if info:
                results["artists"][node_id] = {
                    "url": url,
                    "infobox": info,
                }

        time.sleep(0.5)

    # 4. L∆∞u k·∫øt qu·∫£
    print("\nüíæ B∆∞·ªõc 4: L∆∞u k·∫øt qu·∫£...")
    output_file = "infobox_members.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"   ‚úì ƒê√£ l∆∞u v√†o {output_file}")
    print(f"\nüìä K·∫øt qu·∫£:")
    print(f"   - Groups c√≥ infobox th√†nh vi√™n: {len(results['groups'])}")
    print(f"   - Artists c√≥ infobox th√†nh vi√™n c·ªßa: {len(results['artists'])}")
    print("\nüéâ Ho√†n t·∫•t!")


if __name__ == "__main__":
    main()
