# -*- coding: utf-8 -*-
"""
H·ªá th·ªëng l√†m gi√†u d·ªØ li·ªáu ƒë·ªì th·ªã - Phi√™n b·∫£n ƒë·∫ßy ƒë·ªß
- Thu th·∫≠p d·ªØ li·ªáu: Wikipedia (intro, l·ªãch s·ª≠, s·ª± nghi·ªáp) + c√°c ngu·ªìn kh√°c
- NER: Ph√°t hi·ªán th·ª±c th·ªÉ M·ªöI (ngh·ªá sƒ©, nh√≥m nh·∫°c, album, b√†i h√°t, c√¥ng ty...)
- Relation Extraction: Ph√°t hi·ªán quan h·ªá M·ªöI gi·ªØa c√°c th·ª±c th·ªÉ
- M·ª•c ti√™u: T·∫°o c·∫£ NODES M·ªöI v√† RELATIONSHIPS M·ªöI ƒë·ªÉ l√†m gi√†u ƒë·ªì th·ªã
"""
import sys
import io
import time
import re
import argparse
from typing import Dict, Any, List, Tuple, Optional, Set
from collections import defaultdict
from urllib.parse import unquote, quote

import requests
from bs4 import BeautifulSoup
from neo4j import GraphDatabase

# Robust UTF-8 console output on Windows
if sys.platform == 'win32':
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    except Exception:
        pass


class TextDataCollector:
    """Thu th·∫≠p d·ªØ li·ªáu vƒÉn b·∫£n t·ª´ Wikipedia v√† c√°c ngu·ªìn kh√°c"""
    
    def __init__(self, request_timeout: int = 10, request_delay: float = 0.3):
        self.base_url = "https://vi.wikipedia.org/wiki/"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Python-requests'
        })
        self.request_timeout = request_timeout
        self.request_delay = request_delay
        
    def fetch_wikipedia_full_text(self, title: str) -> Dict[str, str]:
        """
        L·∫•y to√†n b·ªô vƒÉn b·∫£n t·ª´ Wikipedia bao g·ªìm:
        - Intro/Description
        - L·ªãch s·ª≠ ho·∫°t ƒë·ªông
        - S·ª± nghi·ªáp/Album/√Çm nh·∫°c
        - Infobox
        """
        try:
            url = self.base_url + quote(title.replace(' ', '_'))
            response = self.session.get(url, timeout=self.request_timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            result = {
                'intro': '',
                'career': '',
                'albums': '',
                'full_text': ''
            }
            
            # L·∫•y intro (ƒëo·∫°n ƒë·∫ßu ti√™n)
            intro_para = soup.find('div', class_='mw-parser-output')
            if intro_para:
                first_p = intro_para.find('p')
                if first_p:
                    result['intro'] = first_p.get_text(strip=True)
            
            # L·∫•y c√°c section v·ªÅ s·ª± nghi·ªáp, album, √¢m nh·∫°c
            sections = soup.find_all(['h2', 'h3'])
            for section in sections:
                section_text = section.get_text(strip=True).lower()
                if any(keyword in section_text for keyword in ['s·ª± nghi·ªáp', 'career', 'album', '√¢m nh·∫°c', 'music', 'discography']):
                    content = []
                    next_elem = section.find_next_sibling()
                    while next_elem and next_elem.name not in ['h2', 'h3']:
                        if next_elem.name == 'p':
                            content.append(next_elem.get_text(strip=True))
                        next_elem = next_elem.find_next_sibling()
                    if content:
                        result['career'] = ' '.join(content)
            
            # L·∫•y full text (lo·∫°i b·ªè references, navbox)
            content_div = soup.find('div', id='mw-content-text')
            if content_div:
                # Lo·∫°i b·ªè c√°c ph·∫ßn kh√¥ng c·∫ßn thi·∫øt
                for element in content_div.find_all(['table', 'div', 'span'], 
                                                   class_=['navbox', 'reference', 'mw-references-wrap', 'mw-editsection']):
                    element.decompose()
                
                full_text = content_div.get_text(separator=' ', strip=True)
                full_text = re.sub(r'\s+', ' ', full_text)
                full_text = re.sub(r'\[\d+\]', '', full_text)
                result['full_text'] = full_text
            
            # L·∫•y infobox
            infobox = soup.find('table', class_='infobox')
            if infobox:
                result['infobox'] = infobox.get_text(separator=' ', strip=True)
            
            time.sleep(self.request_delay)
            return result
            
        except Exception as e:
            print(f"  ‚ö† L·ªói khi l·∫•y Wikipedia cho '{title}': {e}")
            return {'intro': '', 'career': '', 'albums': '', 'full_text': ''}


class KoreanMusicNER:
    """
    M√¥ h√¨nh NER ƒë·ªÉ ph√°t hi·ªán th·ª±c th·ªÉ M·ªöI trong vƒÉn b·∫£n
    T·∫≠p trung v√†o c√°c th·ª±c th·ªÉ li√™n quan ƒë·∫øn √¢m nh·∫°c H√†n Qu·ªëc
    """
    
    def __init__(self):
        # T·ª´ kh√≥a ƒë·ªÉ nh·∫≠n di·ªán c√°c lo·∫°i th·ª±c th·ªÉ
        self.artist_keywords = ['ca sƒ©', 'singer', 'ngh·ªá sƒ©', 'artist', 'soloist', 'rapper', 'idol']
        self.group_keywords = ['nh√≥m nh·∫°c', 'group', 'band', 'ban nh·∫°c', 'boy group', 'girl group']
        self.album_keywords = ['album', 'mini album', 'ep', 'single album', 'full album', 'studio album']
        self.song_keywords = ['b√†i h√°t', 'song', 'ca kh√∫c', 'ƒëƒ©a ƒë∆°n', 'single', 'track', 'ost']
        self.company_keywords = ['entertainment', 'c√¥ng ty', 'company', 'label', 'agency', 'smtown', 'yg', 'jyp', 'hybe']
        self.genre_keywords = ['th·ªÉ lo·∫°i', 'genre', 'd√≤ng nh·∫°c', 'k-pop', 'ballad', 'hip hop', 'r&b', 'edm']
        
        # Pattern cho t√™n ti·∫øng H√†n (Hangul)
        self.hangul_pattern = re.compile(r'[\uac00-\ud7af]+')
        
        # Pattern cho t√™n ngh·ªá sƒ©/nh√≥m nh·∫°c (th∆∞·ªùng c√≥ ch·ªØ c√°i v√† s·ªë)
        self.name_pattern = re.compile(r'\b[A-Z][A-Z0-9\s&\.\']+\b')
        
    def extract_entities(self, text: str, existing_nodes: Dict[str, Dict] = None) -> List[Dict[str, Any]]:
        """
        Tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ M·ªöI t·ª´ vƒÉn b·∫£n
        Tr·∫£ v·ªÅ danh s√°ch c√°c entities v·ªõi label v√† confidence
        """
        entities = []
        text_lower = text.lower()
        
        # 1. T√¨m c√°c ngh·ªá sƒ© (Artist)
        artists = self._extract_artists(text, text_lower)
        entities.extend(artists)
        
        # 2. T√¨m c√°c nh√≥m nh·∫°c (Group)
        groups = self._extract_groups(text, text_lower)
        entities.extend(groups)
        
        # 3. T√¨m c√°c album
        albums = self._extract_albums(text, text_lower)
        entities.extend(albums)
        
        # 4. T√¨m c√°c b√†i h√°t
        songs = self._extract_songs(text, text_lower)
        entities.extend(songs)
        
        # 5. T√¨m c√°c c√¥ng ty
        companies = self._extract_companies(text, text_lower)
        entities.extend(companies)
        
        # 6. T√¨m c√°c th·ªÉ lo·∫°i
        genres = self._extract_genres(text, text_lower)
        entities.extend(genres)
        
        # Lo·∫°i b·ªè tr√πng l·∫∑p v√† entities ƒë√£ t·ªìn t·∫°i
        if existing_nodes:
            existing_names = {node['name'].lower() for node in existing_nodes.values() if node.get('name')}
            entities = [e for e in entities if e['text'].lower() not in existing_names]
        
        # Merge tr√πng l·∫∑p
        return self._merge_duplicates(entities)
    
    def _extract_artists(self, text: str, text_lower: str) -> List[Dict]:
        """Tr√≠ch xu·∫•t ngh·ªá sƒ©"""
        entities = []
        
        # T√¨m c√°c c√¢u c√≥ t·ª´ kh√≥a ngh·ªá sƒ©
        sentences = re.split(r'[.!?]\s+', text)
        for sentence in sentences:
            sentence_lower = sentence.lower()
            if any(kw in sentence_lower for kw in self.artist_keywords):
                # T√¨m t√™n c√≥ ch·ªØ H√†n ho·∫∑c t√™n ti·∫øng Anh
                # Pattern: "t√™n" + keyword ho·∫∑c keyword + "t√™n"
                for kw in self.artist_keywords:
                    if kw in sentence_lower:
                        # T√¨m t√™n tr∆∞·ªõc ho·∫∑c sau keyword
                        parts = sentence_lower.split(kw)
                        for part in parts:
                            # T√¨m t√™n trong ph·∫ßn n√†y
                            name_match = self._extract_name_near_keyword(sentence, part, kw)
                            if name_match:
                                entities.append({
                                    'text': name_match,
                                    'label': 'Artist',
                                    'confidence': 0.75,
                                    'context': sentence[:200]
                                })
        
        # T√¨m c√°c t√™n c√≥ ch·ªØ H√†n (th∆∞·ªùng l√† ngh·ªá sƒ© H√†n Qu·ªëc)
        hangul_names = self.hangul_pattern.findall(text)
        for name in hangul_names:
            if len(name) >= 2 and len(name) <= 20:
                # Ki·ªÉm tra xem c√≥ ph·∫£i ngh·ªá sƒ© kh√¥ng
                context = self._get_context(text, name, 100)
                if any(kw in context.lower() for kw in self.artist_keywords):
                    entities.append({
                        'text': name,
                        'label': 'Artist',
                        'confidence': 0.7,
                        'context': context
                    })
        
        return entities
    
    def _extract_groups(self, text: str, text_lower: str) -> List[Dict]:
        """Tr√≠ch xu·∫•t nh√≥m nh·∫°c"""
        entities = []
        
        sentences = re.split(r'[.!?]\s+', text)
        for sentence in sentences:
            sentence_lower = sentence.lower()
            if any(kw in sentence_lower for kw in self.group_keywords):
                # T√¨m t√™n nh√≥m (th∆∞·ªùng l√† ch·ªØ in hoa ho·∫∑c c√≥ ch·ªØ H√†n)
                name_match = self._extract_name_near_keyword(sentence, sentence_lower, 'nh√≥m nh·∫°c')
                if name_match:
                    entities.append({
                        'text': name_match,
                        'label': 'Group',
                        'confidence': 0.75,
                        'context': sentence[:200]
                    })
        
        # T√¨m c√°c t√™n nh√≥m ph·ªï bi·∫øn (BTS, BLACKPINK, TWICE...)
        group_name_pattern = re.compile(r'\b([A-Z]{2,}(?:\s+[A-Z]+)?)\b')
        matches = group_name_pattern.findall(text)
        for match in matches:
            if len(match) >= 2 and len(match) <= 30:
                context = self._get_context(text, match, 100)
                if any(kw in context.lower() for kw in self.group_keywords):
                    entities.append({
                        'text': match,
                        'label': 'Group',
                        'confidence': 0.7,
                        'context': context
                    })
        
        return entities
    
    def _extract_albums(self, text: str, text_lower: str) -> List[Dict]:
        """Tr√≠ch xu·∫•t album"""
        entities = []
        
        sentences = re.split(r'[.!?]\s+', text)
        for sentence in sentences:
            sentence_lower = sentence.lower()
            if any(kw in sentence_lower for kw in self.album_keywords):
                # T√¨m t√™n album (th∆∞·ªùng trong d·∫•u ngo·∫∑c k√©p ho·∫∑c sau keyword)
                # Pattern: "T√™n Album" ho·∫∑c album "T√™n"
                quoted = re.findall(r'["\']([^"\']+)["\']', sentence)
                for name in quoted:
                    if len(name) >= 3:
                        entities.append({
                            'text': name,
                            'label': 'Album',
                            'confidence': 0.7,
                            'context': sentence[:200]
                        })
        
        return entities
    
    def _extract_songs(self, text: str, text_lower: str) -> List[Dict]:
        """Tr√≠ch xu·∫•t b√†i h√°t"""
        entities = []
        
        sentences = re.split(r'[.!?]\s+', text)
        for sentence in sentences:
            sentence_lower = sentence.lower()
            if any(kw in sentence_lower for kw in self.song_keywords):
                # T√¨m t√™n b√†i h√°t trong d·∫•u ngo·∫∑c k√©p
                quoted = re.findall(r'["\']([^"\']+)["\']', sentence)
                for name in quoted:
                    if len(name) >= 2:
                        entities.append({
                            'text': name,
                            'label': 'Song',
                            'confidence': 0.7,
                            'context': sentence[:200]
                        })
        
        return entities
    
    def _extract_companies(self, text: str, text_lower: str) -> List[Dict]:
        """Tr√≠ch xu·∫•t c√¥ng ty"""
        entities = []
        
        # T√¨m c√°c c√¥ng ty ph·ªï bi·∫øn
        company_names = ['SM Entertainment', 'YG Entertainment', 'JYP Entertainment', 
                        'HYBE', 'Big Hit', 'CUBE', 'Starship', 'Pledis', 'FNC']
        
        for company in company_names:
            if company.lower() in text_lower:
                context = self._get_context(text, company, 100)
                entities.append({
                    'text': company,
                    'label': 'Company',
                    'confidence': 0.8,
                    'context': context
                })
        
        # T√¨m pattern: "t√™n" + Entertainment/Company
        pattern = re.compile(r'([A-Z][A-Za-z\s]+)\s+(?:Entertainment|Company|Agency)')
        matches = pattern.findall(text)
        for match in matches:
            if len(match.strip()) >= 2:
                entities.append({
                    'text': match.strip(),
                    'label': 'Company',
                    'confidence': 0.7,
                    'context': self._get_context(text, match, 100)
                })
        
        return entities
    
    def _extract_genres(self, text: str, text_lower: str) -> List[Dict]:
        """Tr√≠ch xu·∫•t th·ªÉ lo·∫°i"""
        entities = []
        
        # Th·ªÉ lo·∫°i ph·ªï bi·∫øn
        common_genres = ['K-pop', 'Ballad', 'Hip hop', 'R&B', 'EDM', 'Rock', 'Jazz', 
                        'Trot', 'Indie', 'Rap', 'Dance', 'Electronic']
        
        for genre in common_genres:
            if genre.lower() in text_lower:
                context = self._get_context(text, genre, 100)
                entities.append({
                    'text': genre,
                    'label': 'Genre',
                    'confidence': 0.7,
                    'context': context
                })
        
        return entities
    
    def _extract_name_near_keyword(self, sentence: str, part: str, keyword: str) -> Optional[str]:
        """Tr√≠ch xu·∫•t t√™n g·∫ßn keyword"""
        # T√¨m t·ª´/c·ª•m t·ª´ c√≥ ch·ªØ H√†n ho·∫∑c ch·ªØ in hoa
        words = sentence.split()
        keyword_idx = -1
        for i, word in enumerate(words):
            if keyword.lower() in word.lower():
                keyword_idx = i
                break
        
        if keyword_idx >= 0:
            # L·∫•y 2-3 t·ª´ tr∆∞·ªõc v√† sau keyword
            start = max(0, keyword_idx - 3)
            end = min(len(words), keyword_idx + 4)
            candidate = ' '.join(words[start:end])
            
            # T√¨m t√™n trong candidate
            if self.hangul_pattern.search(candidate):
                return self.hangul_pattern.findall(candidate)[0]
            elif re.search(r'\b[A-Z][A-Z0-9\s&\.\']+\b', candidate):
                match = re.search(r'\b([A-Z][A-Z0-9\s&\.\']+)\b', candidate)
                if match:
                    return match.group(1).strip()
        
        return None
    
    def _get_context(self, text: str, entity: str, window: int = 100) -> str:
        """L·∫•y context xung quanh entity"""
        idx = text.lower().find(entity.lower())
        if idx == -1:
            return ''
        start = max(0, idx - window)
        end = min(len(text), idx + len(entity) + window)
        return text[start:end]
    
    def _merge_duplicates(self, entities: List[Dict]) -> List[Dict]:
        """Merge c√°c entities tr√πng l·∫∑p"""
        seen = {}
        merged = []
        
        for entity in entities:
            key = entity['text'].lower().strip()
            if key not in seen:
                seen[key] = entity
                merged.append(entity)
            else:
                # N·∫øu confidence cao h∆°n, thay th·∫ø
                if entity['confidence'] > seen[key]['confidence']:
                    seen[key] = entity
        
        return list(seen.values())


class RelationExtractor:
    """M√¥ h√¨nh nh·∫≠n d·∫°ng quan h·ªá gi·ªØa c√°c th·ª±c th·ªÉ"""
    
    def __init__(self):
        # Pattern cho c√°c quan h·ªá
        self.patterns = {
            'MEMBER_OF': [
                r'(.+?)\s+(?:l√†|th√†nh vi√™n|member|c·ªßa|of)\s+(.+?)(?:\.|,|$|v√†)',
                r'(.+?)\s+(?:gia nh·∫≠p|joined)\s+(.+?)(?:\.|,|$)',
            ],
            'SINGS': [
                r'(.+?)\s+(?:h√°t|sings|performs|tr√¨nh b√†y|ca kh√∫c)\s+(.+?)(?:\.|,|$)',
                r'(.+?)\s+(?:b√†i h√°t|song|single)\s+(.+?)(?:\.|,|$)',
            ],
            'RELEASED': [
                r'(.+?)\s+(?:ph√°t h√†nh|released|ra m·∫Øt|tung ra)\s+(.+?)(?:\.|,|$)',
            ],
            'CONTAINS': [
                r'(.+?)\s+(?:ch·ª©a|contains|bao g·ªìm|includes|c√≥ b√†i)\s+(.+?)(?:\.|,|$)',
            ],
            'IS_GENRE': [
                r'(.+?)\s+(?:thu·ªôc th·ªÉ lo·∫°i|genre|th·ªÉ lo·∫°i|d√≤ng nh·∫°c)\s+(.+?)(?:\.|,|$)',
            ],
            'MANAGED_BY': [
                r'(.+?)\s+(?:ƒë∆∞·ª£c qu·∫£n l√Ω|managed by|signed to|thu·ªôc|k√Ω h·ª£p ƒë·ªìng)\s+(.+?)(?:\.|,|$)',
            ],
            'COLLABORATED_WITH': [
                r'(.+?)\s+(?:h·ª£p t√°c|collaborates|collaborated|ft\.|feat\.|v·ªõi|v√†)\s+(.+?)(?:\.|,|$)',
            ],
            'PRODUCED_SONG': [
                r'(.+?)\s+(?:s·∫£n xu·∫•t|produced|producer)\s+(?:b√†i h√°t|song|ca kh√∫c)\s+(.+?)(?:\.|,|$)',
            ],
            'WROTE': [
                r'(.+?)\s+(?:s√°ng t√°c|wrote|composed|t√°c gi·∫£)\s+(.+?)(?:\.|,|$)',
            ],
        }
    
    def extract_relationships(self, text: str, entities: List[Dict]) -> List[Dict[str, Any]]:
        """Tr√≠ch xu·∫•t quan h·ªá gi·ªØa c√°c entities"""
        relationships = []
        text_lower = text.lower()
        
        # T√¨m c√°c c·∫∑p entity g·∫ßn nhau
        entity_pairs = self._find_entity_pairs(text, entities)
        
        for entity1, entity2, context in entity_pairs:
            rel_type = self._classify_relationship(entity1, entity2, context)
            if rel_type:
                relationships.append({
                    'source': entity1['text'],
                    'target': entity2['text'],
                    'source_label': entity1['label'],
                    'target_label': entity2['label'],
                    'type': rel_type,
                    'confidence': 0.75,
                    'context': context[:200]
                })
        
        return relationships
    
    def _find_entity_pairs(self, text: str, entities: List[Dict]) -> List[Tuple]:
        """T√¨m c√°c c·∫∑p entity xu·∫•t hi·ªán g·∫ßn nhau"""
        pairs = []
        
        # T√¨m v·ªã tr√≠ c·ªßa m·ªói entity trong text
        entity_positions = []
        for entity in entities:
            text_lower = text.lower()
            entity_lower = entity['text'].lower()
            idx = text_lower.find(entity_lower)
            if idx != -1:
                entity_positions.append({
                    'entity': entity,
                    'start': idx,
                    'end': idx + len(entity_lower)
                })
        
        # S·∫Øp x·∫øp theo v·ªã tr√≠
        entity_positions.sort(key=lambda x: x['start'])
        
        # T√¨m c√°c c·∫∑p g·∫ßn nhau
        window_size = 300
        seen_pairs = set()
        
        for i in range(len(entity_positions)):
            for j in range(i + 1, len(entity_positions)):
                e1 = entity_positions[i]
                e2 = entity_positions[j]
                
                if e2['start'] - e1['end'] > window_size:
                    break
                
                pair_key = (e1['entity']['text'].lower(), e2['entity']['text'].lower())
                if pair_key in seen_pairs:
                    continue
                seen_pairs.add(pair_key)
                
                # L·∫•y context
                context_start = max(0, e1['start'] - 100)
                context_end = min(len(text), e2['end'] + 100)
                context = text[context_start:context_end]
                
                pairs.append((e1['entity'], e2['entity'], context))
        
        return pairs
    
    def _classify_relationship(self, entity1: Dict, entity2: Dict, context: str) -> Optional[str]:
        """Ph√¢n lo·∫°i relationship"""
        context_lower = context.lower()
        label1 = entity1['label']
        label2 = entity2['label']
        
        # Ki·ªÉm tra patterns
        for rel_type, patterns in self.patterns.items():
            for pattern in patterns:
                match = re.search(pattern, context_lower, re.IGNORECASE)
                if match:
                    groups = match.groups()
                    if len(groups) >= 2:
                        e1_text = entity1['text'].lower()
                        e2_text = entity2['text'].lower()
                        
                        if (e1_text in groups[0].lower() and e2_text in groups[1].lower()) or \
                           (e1_text in groups[1].lower() and e2_text in groups[0].lower()):
                            if self._is_valid_relationship(label1, label2, rel_type):
                                return rel_type
        
        # Heuristic
        return self._heuristic_relationship(label1, label2, context_lower)
    
    def _is_valid_relationship(self, label1: str, label2: str, rel_type: str) -> bool:
        """Ki·ªÉm tra t√≠nh h·ª£p l·ªá"""
        valid = {
            'MEMBER_OF': [('Artist', 'Group')],
            'SINGS': [('Artist', 'Song'), ('Group', 'Song')],
            'RELEASED': [('Artist', 'Album'), ('Group', 'Album')],
            'CONTAINS': [('Album', 'Song')],
            'IS_GENRE': [('Artist', 'Genre'), ('Group', 'Genre'), ('Song', 'Genre'), ('Album', 'Genre')],
            'MANAGED_BY': [('Artist', 'Company'), ('Group', 'Company')],
            'COLLABORATED_WITH': [('Artist', 'Artist'), ('Group', 'Group'), ('Artist', 'Group')],
            'PRODUCED_SONG': [('Artist', 'Song'), ('Group', 'Song')],
            'WROTE': [('Artist', 'Song'), ('Group', 'Song')],
        }
        
        valid_pairs = valid.get(rel_type, [])
        return (label1, label2) in valid_pairs
    
    def _heuristic_relationship(self, label1: str, label2: str, context: str) -> Optional[str]:
        """Heuristic d·ª±a tr√™n label"""
        if label1 == 'Artist' and label2 == 'Group':
            if any(kw in context for kw in ['th√†nh vi√™n', 'member']):
                return 'MEMBER_OF'
        elif label1 == 'Group' and label2 == 'Album':
            if any(kw in context for kw in ['ph√°t h√†nh', 'released']):
                return 'RELEASED'
        elif label1 == 'Album' and label2 == 'Song':
            if any(kw in context for kw in ['ch·ª©a', 'contains']):
                return 'CONTAINS'
        return None


class CompleteGraphEnricher:
    """H·ªá th·ªëng l√†m gi√†u d·ªØ li·ªáu ƒë·∫ßy ƒë·ªß - t·∫°o c·∫£ nodes m·ªõi v√† relationships m·ªõi"""
    
    def __init__(self, uri: str, user: str, password: str, database: str = None):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))
        self.database = database
        self.text_collector = TextDataCollector()
        self.ner = KoreanMusicNER()
        self.relation_extractor = RelationExtractor()
        
        print(f"‚úì ƒê√£ k·∫øt n·ªëi v·ªõi Neo4j: {uri}")
    
    def close(self):
        self.driver.close()
    
    def get_existing_nodes(self) -> Dict[str, Dict]:
        """L·∫•y nodes hi·ªán c√≥ ƒë·ªÉ tr√°nh tr√πng l·∫∑p"""
        def get_nodes(tx):
            query = """
            MATCH (n)
            WHERE n.name IS NOT NULL AND n.id IS NOT NULL
            RETURN n.id as id, labels(n) as labels, n.name as name
            """
            result = tx.run(query)
            nodes = {}
            for record in result:
                nodes[record['id']] = {
                    'id': record['id'],
                    'label': record['labels'][0] if record['labels'] else 'Entity',
                    'name': record['name']
                }
            return nodes
        
        with self.driver.session(database=self.database) if self.database else self.driver.session() as session:
            return session.execute_read(get_nodes)
    
    def enrich_node(self, node_id: str, node_data: Dict, existing_nodes: Dict) -> Tuple[List[Dict], List[Dict]]:
        """L√†m gi√†u m·ªôt node"""
        print(f"\nüìù ƒêang l√†m gi√†u node: {node_data.get('name', node_id)}")
        
        # L·∫•y text t·ª´ Wikipedia
        url = node_data.get('url')
        if not url or 'wikipedia.org' not in url:
            return [], []
        
        title = url.split('/wiki/')[-1] if '/wiki/' in url else None
        if not title:
            return [], []
        
        title = unquote(title).replace('_', ' ')
        text_data = self.text_collector.fetch_wikipedia_full_text(title)
        
        # K·∫øt h·ª£p t·∫•t c·∫£ text
        full_text = ' '.join([text_data.get('intro', ''), 
                             text_data.get('career', ''),
                             text_data.get('full_text', '')])
        
        if len(full_text) < 100:
            return [], []
        
        print(f"  ‚úì ƒê√£ thu th·∫≠p {len(full_text)} k√Ω t·ª± text")
        
        # 1. NER - T√¨m entities M·ªöI
        new_entities = self.ner.extract_entities(full_text, existing_nodes)
        print(f"  ‚úì T√¨m th·∫•y {len(new_entities)} entities M·ªöI")
        
        # 2. Relation Extraction - T√¨m relationships
        # K·∫øt h·ª£p entities m·ªõi v√† entities c≈© ƒë·ªÉ t√¨m relationships
        all_entities = list(new_entities)
        for node in existing_nodes.values():
            all_entities.append({
                'text': node['name'],
                'label': node['label'],
                'confidence': 1.0
            })
        
        relationships = self.relation_extractor.extract_relationships(full_text, all_entities)
        print(f"  ‚úì T√¨m th·∫•y {len(relationships)} relationships")
        
        return new_entities, relationships
    
    def update_neo4j(self, new_entities: List[Dict], relationships: List[Dict], batch_size: int = 50):
        """C·∫≠p nh·∫≠t Neo4j v·ªõi nodes m·ªõi v√† relationships m·ªõi"""
        def run_write(tx, query, parameters=None):
            return tx.run(query, parameters or {})
        
        with self.driver.session(database=self.database) if self.database else self.driver.session() as session:
            # 1. T·∫°o nodes m·ªõi
            if new_entities:
                node_query = """
                UNWIND $batch AS e
                MERGE (n:Entity {id: e.id})
                SET n.name = e.name,
                    n.label = e.label,
                    n.enriched = true,
                    n.enrichment_confidence = e.confidence,
                    n.enrichment_source = 'ner_wikipedia'
                """
                
                batch = []
                for entity in new_entities:
                    entity_id = f"ENRICHED_{abs(hash(entity['text']))}"
                    batch.append({
                        'id': entity_id,
                        'name': entity['text'],
                        'label': entity['label'],
                        'confidence': entity['confidence']
                    })
                
                if batch:
                    for i in range(0, len(batch), batch_size):
                        session.execute_write(run_write, node_query, {'batch': batch[i:i+batch_size]})
                    print(f"  ‚úì ƒê√£ t·∫°o {len(batch)} nodes M·ªöI")
            
            # 2. T·∫°o relationships m·ªõi
            if relationships:
                # T·∫°o mapping t·ª´ t√™n sang ID
                name_to_id = {}
                for entity in new_entities:
                    entity_id = f"ENRICHED_{abs(hash(entity['text']))}"
                    name_to_id[entity['text'].lower()] = entity_id
                
                # L·∫•y IDs t·ª´ existing nodes
                existing_result = session.run("MATCH (n) RETURN n.id as id, n.name as name")
                for record in existing_result:
                    name_to_id[record['name'].lower() if record['name'] else ''] = record['id']
                
                # T·∫°o relationships
                rel_batch = []
                for rel in relationships:
                    source_id = name_to_id.get(rel['source'].lower())
                    target_id = name_to_id.get(rel['target'].lower())
                    
                    if source_id and target_id:
                        rel_batch.append({
                            'source': source_id,
                            'target': target_id,
                            'type': rel['type'],
                            'context': rel.get('context', '')[:300],
                            'confidence': rel.get('confidence', 0.75)
                        })
                
                if rel_batch:
                    # Group by relationship type
                    for rel_type in set(r['type'] for r in rel_batch):
                        type_batch = [r for r in rel_batch if r['type'] == rel_type]
                        
                        query = f"""
                        UNWIND $batch AS r
                        MATCH (s {{id: r.source}}), (t {{id: r.target}})
                        WHERE s IS NOT NULL AND t IS NOT NULL
                        MERGE (s)-[rel:`{rel_type}` {{enriched: true}}]->(t)
                        SET rel.context = r.context,
                            rel.enrichment_confidence = r.confidence,
                            rel.enrichment_source = 'relation_extraction'
                        """
                        
                        for i in range(0, len(type_batch), batch_size):
                            session.execute_write(run_write, query, {'batch': type_batch[i:i+batch_size]})
                    
                    print(f"  ‚úì ƒê√£ t·∫°o {len(rel_batch)} relationships M·ªöI")
    
    def enrich_all(self, limit: int = None, batch_size: int = 10):
        """L√†m gi√†u t·∫•t c·∫£ nodes"""
        print("=" * 80)
        print("B·∫ÆT ƒê·∫¶U L√ÄM GI√ÄU D·ªÆ LI·ªÜU ƒê·ªí TH·ªä (PHI√äN B·∫¢N ƒê·∫¶Y ƒê·ª¶)")
        print("=" * 80)
        print("üìå Ngu·ªìn: Wikipedia (intro, career, full text)")
        print("üìå NER: Ph√°t hi·ªán entities M·ªöI")
        print("üìå Relation Extraction: Ph√°t hi·ªán relationships M·ªöI")
        print("=" * 80)
        
        # L·∫•y nodes hi·ªán c√≥
        print("\nüìä ƒêang l·∫•y nodes hi·ªán c√≥...")
        existing_nodes = self.get_existing_nodes()
        print(f"‚úì T√¨m th·∫•y {len(existing_nodes)} nodes hi·ªán c√≥")
        
        # L·∫•y nodes c√≥ URL Wikipedia
        def get_nodes_with_url(tx):
            query = """
            MATCH (n)
            WHERE n.url IS NOT NULL AND n.url CONTAINS 'wikipedia.org'
            RETURN n.id as id, n.name as name, n.url as url, labels(n) as labels
            """
            if limit:
                query += f" LIMIT {limit}"
            result = tx.run(query)
            return [dict(record) for record in result]
        
        with self.driver.session(database=self.database) if self.database else self.driver.session() as session:
            nodes_to_process = session.execute_read(get_nodes_with_url)
        
        print(f"‚úì C√≥ {len(nodes_to_process)} nodes c√≥ URL Wikipedia ƒë·ªÉ x·ª≠ l√Ω")
        
        all_new_entities = []
        all_relationships = []
        processed = 0
        
        for node_data in nodes_to_process:
            try:
                node_id = node_data['id']
                entities, relationships = self.enrich_node(node_id, node_data, existing_nodes)
                all_new_entities.extend(entities)
                all_relationships.extend(relationships)
                
                # C·∫≠p nh·∫≠t existing_nodes v·ªõi entities m·ªõi
                for entity in entities:
                    entity_id = f"ENRICHED_{abs(hash(entity['text']))}"
                    existing_nodes[entity_id] = {
                        'id': entity_id,
                        'label': entity['label'],
                        'name': entity['text']
                    }
                
                processed += 1
                if processed % batch_size == 0:
                    print(f"\nüíæ ƒêang c·∫≠p nh·∫≠t Neo4j (ƒë√£ x·ª≠ l√Ω {processed}/{len(nodes_to_process)})...")
                    self.update_neo4j(all_new_entities, all_relationships)
                    all_new_entities = []
                    all_relationships = []
                    
            except Exception as e:
                print(f"  ‚ùå L·ªói: {e}")
                continue
        
        # C·∫≠p nh·∫≠t l·∫ßn cu·ªëi
        if all_new_entities or all_relationships:
            print(f"\nüíæ ƒêang c·∫≠p nh·∫≠t Neo4j l·∫ßn cu·ªëi...")
            self.update_neo4j(all_new_entities, all_relationships)
        
        print("\n" + "=" * 80)
        print("HO√ÄN T·∫§T L√ÄM GI√ÄU D·ªÆ LI·ªÜU")
        print("=" * 80)


def parse_args():
    parser = argparse.ArgumentParser(description='L√†m gi√†u d·ªØ li·ªáu ƒë·ªì th·ªã - T·∫°o nodes m·ªõi v√† relationships m·ªõi')
    parser.add_argument('--neo4j-uri', type=str, required=True)
    parser.add_argument('--neo4j-user', type=str, required=True)
    parser.add_argument('--neo4j-pass', type=str, required=True)
    parser.add_argument('--neo4j-db', type=str, default=None)
    parser.add_argument('--limit', type=int, default=None)
    parser.add_argument('--batch-size', type=int, default=10)
    return parser.parse_args()


def main():
    args = parse_args()
    enricher = CompleteGraphEnricher(args.neo4j_uri, args.neo4j_user, args.neo4j_pass, args.neo4j_db)
    try:
        enricher.enrich_all(limit=args.limit, batch_size=args.batch_size)
    finally:
        enricher.close()


if __name__ == '__main__':
    main()








