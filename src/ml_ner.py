# -*- coding: utf-8 -*-
"""
ML-BASED NER MODULE
S·ª≠ d·ª•ng pre-trained Vietnamese NER models ƒë·ªÉ b·ªï sung cho rule-based NER
"""
import sys
import io
import re
from typing import List, Dict, Optional

if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# Try to import transformers
TRANSFORMERS_AVAILABLE = False
try:
    from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError as e:
    TRANSFORMERS_AVAILABLE = False
    # Kh√¥ng in l·ªói ƒë·ªÉ tr√°nh spam, ch·ªâ set flag
except Exception as e:
    # X·ª≠ l√Ω c√°c l·ªói kh√°c (nh∆∞ GenerationMixin, version conflict, etc.)
    TRANSFORMERS_AVAILABLE = False

# Try to import spacy
try:
    import spacy
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False
    print("‚ö†Ô∏è  spacy kh√¥ng ƒë∆∞·ª£c c√†i ƒë·∫∑t. Ch·∫°y: pip install spacy")

# Mapping t·ª´ labels c·ªßa model sang labels c·ªßa ch√∫ng ta
LABEL_MAPPING = {
    # PERSON -> c√≥ th·ªÉ l√† Artist
    'PERSON': 'Artist',
    'PER': 'Artist',
    'B-PER': 'Artist',
    'I-PER': 'Artist',
    
    # ORG -> c√≥ th·ªÉ l√† Group ho·∫∑c Company
    'ORG': 'Group',  # M·∫∑c ƒë·ªãnh l√† Group, s·∫Ω ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh sau
    'ORGANIZATION': 'Group',
    'B-ORG': 'Group',
    'I-ORG': 'Group',
    
    # LOC -> c√≥ th·ªÉ l√† Company (n·∫øu c√≥ "Entertainment", "Music"...)
    'LOC': None,  # Kh√¥ng map tr·ª±c ti·∫øp
    'LOCATION': None,
    'B-LOC': None,
    'I-LOC': None,
    
    # MISC -> c√≥ th·ªÉ l√† Album/Song
    'MISC': None,
    'B-MISC': None,
    'I-MISC': None,
}

# Keywords ƒë·ªÉ ph√¢n bi·ªát Company vs Group
COMPANY_KEYWORDS = ['entertainment', 'music', 'media', 'label', 'agency', 'c√¥ng ty', 'h√£ng']
GROUP_KEYWORDS = ['nh√≥m', 'nh√≥m nh·∫°c', 'group', 'band', 'idol']

def clean_text(text: str) -> str:
    """Chu·∫©n h√≥a text entity"""
    if not text:
        return ""
    text = text.strip()
    # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát ·ªü ƒë·∫ßu/cu·ªëi
    text = re.sub(r'^[.,;:!?"\'()\[\]{}]+|[.,;:!?"\'()\[\]{}]+$', '', text)
    # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def classify_entity_type(text: str, original_label: str) -> Optional[str]:
    """
    Ph√¢n lo·∫°i entity type d·ª±a tr√™n text v√† label g·ªëc
    
    Args:
        text: Text c·ªßa entity
        original_label: Label t·ª´ model (PERSON, ORG, etc.)
    
    Returns:
        Entity type ph√π h·ª£p (Artist, Group, Company, Album, Song) ho·∫∑c None
    """
    text_lower = text.lower()
    
    # Normalize label ƒë·ªÉ so s√°nh (case-insensitive)
    label_upper = original_label.upper() if original_label else ''
    
    # PERSON -> Artist
    if label_upper in ['PERSON', 'PER', 'B-PER', 'I-PER'] or 'PERSON' in label_upper:
        return 'Artist'
    
    # ORG -> c·∫ßn ph√¢n bi·ªát Group vs Company
    if label_upper in ['ORG', 'ORGANIZATION', 'B-ORG', 'I-ORG'] or 'ORG' in label_upper:
        # N·∫øu c√≥ keywords c·ªßa Company -> Company
        if any(kw in text_lower for kw in COMPANY_KEYWORDS):
            return 'Company'
        # N·∫øu c√≥ keywords c·ªßa Group -> Group
        if any(kw in text_lower for kw in GROUP_KEYWORDS):
            return 'Group'
        # M·∫∑c ƒë·ªãnh l√† Group (v√¨ nh√≥m nh·∫°c ph·ªï bi·∫øn h∆°n)
        return 'Group'
    
    # MISC -> c√≥ th·ªÉ l√† Album/Song (c·∫ßn context ƒë·ªÉ ph√¢n bi·ªát)
    if label_upper in ['MISC', 'B-MISC', 'I-MISC'] or 'MISC' in label_upper:
        # Heuristic: n·∫øu c√≥ t·ª´ kh√≥a album/song -> ph√¢n lo·∫°i
        if any(kw in text_lower for kw in ['album', 'ep', 'mini-album']):
            return 'Album'
        if any(kw in text_lower for kw in ['song', 'b√†i h√°t', 'ca kh√∫c', 'single']):
            return 'Song'
        # Kh√¥ng ph√¢n lo·∫°i ƒë∆∞·ª£c -> None
        return None
    
    return None

class VietnameseNERModel:
    """Wrapper cho Vietnamese NER models"""
    
    def __init__(self, model_name: str = "NlpHUST/ner-vietnamese-electra-base"):
        """
        Kh·ªüi t·∫°o model
        
        Args:
            model_name: T√™n model tr√™n HuggingFace
        """
        self.model_name = model_name
        self.ner_pipeline = None
        self.available = False
        
        if not TRANSFORMERS_AVAILABLE:
            print("‚ö†Ô∏è  transformers kh√¥ng kh·∫£ d·ª•ng. B·ªè qua ML-based NER.")
            return
        
        try:
            print(f"üì• ƒêang t·∫£i model {model_name}...")
            self.ner_pipeline = pipeline(
                "ner",
                model=model_name,
                tokenizer=model_name,
                aggregation_strategy="simple"
            )
            self.available = True
            print(f"‚úì ƒê√£ t·∫£i model th√†nh c√¥ng")
        except Exception as e:
            # In l·ªói ƒë·ªÉ debug
            print(f"‚ö†Ô∏è  ML model kh√¥ng kh·∫£ d·ª•ng: {type(e).__name__}: {str(e)[:100]}")
            print(f"   Ch·ªâ s·ª≠ d·ª•ng rule-based NER")
            self.available = False
    
    def extract_entities(self, text: str, source_node: str = "") -> List[Dict]:
        """
        Tr√≠ch xu·∫•t entities t·ª´ text
        
        Args:
            text: Text c·∫ßn tr√≠ch xu·∫•t
            source_node: Node ID ngu·ªìn
        
        Returns:
            List c√°c entities ƒë∆∞·ª£c tr√≠ch xu·∫•t
        """
        if not self.available or not self.ner_pipeline:
            return []
        
        if not text or len(text.strip()) < 3:
            return []
        
        try:
            # Gi·ªõi h·∫°n ƒë·ªô d√†i text ƒë·ªÉ tr√°nh l·ªói tensor size mismatch
            # Model th∆∞·ªùng c√≥ max_length = 512 tokens
            # ∆Ø·ªõc t√≠nh: 1 token ‚âà 0.75 t·ª´, 512 tokens ‚âà 384 t·ª´ ‚âà 2000 k√Ω t·ª±
            MAX_TEXT_LENGTH = 2000  # An to√†n h∆°n
            
            if len(text) > MAX_TEXT_LENGTH:
                # Chia text th√†nh c√°c chunks nh·ªè h∆°n
                chunks = []
                chunk_size = MAX_TEXT_LENGTH
                i = 0
                while i < len(text):
                    chunk = text[i:i + chunk_size]
                    # C·ªë g·∫Øng c·∫Øt ·ªü kho·∫£ng tr·∫Øng ƒë·ªÉ tr√°nh c·∫Øt gi·ªØa t·ª´
                    if i + chunk_size < len(text):
                        last_space = chunk.rfind(' ')
                        if last_space > chunk_size * 0.8:  # N·∫øu c√≥ kho·∫£ng tr·∫Øng g·∫ßn cu·ªëi
                            chunk = chunk[:last_space]
                            i = i + last_space + 1
                        else:
                            i = i + chunk_size
                    else:
                        i = len(text)  # Chunk cu·ªëi c√πng
                    chunks.append(chunk)
            else:
                chunks = [text]
            
            # Ch·∫°y NER tr√™n t·ª´ng chunk
            all_results = []
            for chunk in chunks:
                try:
                    # Pipeline kh√¥ng nh·∫≠n truncation/max_length nh∆∞ parameter
                    # N√≥ t·ª± ƒë·ªông x·ª≠ l√Ω trong tokenizer
                    chunk_results = self.ner_pipeline(chunk)
                    if isinstance(chunk_results, list):
                        all_results.extend(chunk_results)
                    elif isinstance(chunk_results, dict):
                        all_results.append(chunk_results)
                except Exception as chunk_error:
                    # B·ªè qua chunk c√≥ l·ªói, ti·∫øp t·ª•c v·ªõi chunk kh√°c
                    # Kh√¥ng in l·ªói ƒë·ªÉ tr√°nh spam
                    continue
            
            results = all_results
            
            # Debug: ƒë·∫øm s·ªë results tr∆∞·ªõc khi filter
            total_results = len(results)
            
            entities = []
            filtered_count = 0
            for result in results:
                # X·ª≠ l√Ω format kh√°c nhau c·ªßa k·∫øt qu·∫£
                if isinstance(result, dict):
                    entity_text = result.get('word', '') or result.get('entity', '')
                    label = result.get('entity_group', '') or result.get('label', '')
                    score = result.get('score', 0.7)
                else:
                    continue
                
                entity_text = str(entity_text).strip()
                if not entity_text:
                    continue
                
                # Chu·∫©n h√≥a text
                entity_text = clean_text(entity_text)
                if not entity_text or len(entity_text) < 2:
                    continue
                
                # Ph√¢n lo·∫°i entity type
                entity_type = classify_entity_type(entity_text, label)
                if not entity_type:
                    # B·ªè qua n·∫øu kh√¥ng ph√¢n lo·∫°i ƒë∆∞·ª£c
                    filtered_count += 1
                    continue
                
                # Lo·∫°i b·ªè c√°c entity qu√° ng·∫Øn ho·∫∑c kh√¥ng h·ª£p l·ªá
                if len(entity_text) < 2 or len(entity_text) > 50:
                    continue
                
                entities.append({
                    'text': entity_text,
                    'type': entity_type,
                    'method': 'ml-based',
                    'confidence': min(0.95, score * 0.9),  # Gi·∫£m confidence m·ªôt ch√∫t so v·ªõi rule-based
                    'source_node': source_node,
                    'ml_label': label,  # L∆∞u label g·ªëc t·ª´ model
                })
            
            # Debug info (ch·ªâ in cho v√†i l·∫ßn ƒë·∫ßu)
            if len(entities) == 0 and total_results > 0:
                # C√≥ results nh∆∞ng kh√¥ng c√≥ entities n√†o pass filter
                # C√≥ th·ªÉ do classify_entity_type qu√° strict
                pass
            
            return entities
            
        except Exception as e:
            # Kh√¥ng in l·ªói ƒë·ªÉ tr√°nh spam (ƒë√£ x·ª≠ l√Ω ·ªü tr√™n)
            return []


# Global model instance
_ner_model = None

def get_ner_model(model_name: str = "NlpHUST/ner-vietnamese-electra-base") -> Optional[VietnameseNERModel]:
    """
    L·∫•y instance c·ªßa NER model (singleton pattern)
    
    Args:
        model_name: T√™n model tr√™n HuggingFace
    
    Returns:
        VietnameseNERModel instance ho·∫∑c None
    """
    global _ner_model
    if _ner_model is None:
        _ner_model = VietnameseNERModel(model_name)
    return _ner_model if _ner_model.available else None

def extract_ml_entities(text: str, source_node: str = "") -> List[Dict]:
    """
    Tr√≠ch xu·∫•t entities b·∫±ng ML model
    
    Args:
        text: Text c·∫ßn tr√≠ch xu·∫•t
        source_node: Node ID ngu·ªìn
    
    Returns:
        List c√°c entities ƒë∆∞·ª£c tr√≠ch xu·∫•t
    """
    model = get_ner_model()
    if not model:
        return []
    return model.extract_entities(text, source_node)

