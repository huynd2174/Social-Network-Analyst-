# -*- coding: utf-8 -*-
"""
THU THáº¬P VÃ€ Lá»°A CHá»ŒN Táº¬P Dá»® LIá»†U LÃ€M GIÃ€U
=============================================
Nguá»“n dá»¯ liá»‡u: Wikipedia tiáº¿ng Viá»‡t
Má»¥c tiÃªu: Thu tháº­p vÄƒn báº£n Ä‘á»ƒ lÃ m Ä‘áº§u vÃ o cho NER vÃ  Relation Extraction

Cáº¥u trÃºc dá»¯ liá»‡u Ä‘áº§u ra (JSON):
{
    "node_id": "...",           # ID cá»§a node trong Neo4j
    "node_name": "...",         # TÃªn node
    "node_label": "...",        # Label (Artist, Group, Album, Song, Genre, Company)
    "wikipedia_url": "...",     # URL Wikipedia
    "text": "...",              # VÄƒn báº£n Ä‘Ã£ lÃ m sáº¡ch (dÃ¹ng cho NER vÃ  RE)
    "sections": {               # CÃ¡c pháº§n chi tiáº¿t
        "intro": "...",         # Äoáº¡n giá»›i thiá»‡u
        "career": "...",        # Sá»± nghiá»‡p
        "discography": "...",   # Danh sÃ¡ch album/Ä‘Ä©a nháº¡c
        "awards": "..."         # Giáº£i thÆ°á»Ÿng
    }
}
"""
import sys
import io
import json
import time
import re
from typing import Dict, List, Optional
from urllib.parse import unquote, quote
from datetime import datetime

import requests
from bs4 import BeautifulSoup
from neo4j import GraphDatabase

# UTF-8 output on Windows
if sys.platform == 'win32':
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    except Exception:
        pass


class WikipediaCollector:
    """Thu tháº­p vÄƒn báº£n tá»« Wikipedia"""
    
    def __init__(self, delay: float = 0.3):
        self.base_url = "https://vi.wikipedia.org/wiki/"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Python-requests'
        })
        self.delay = delay
    
    def collect(self, title: str) -> Dict:
        """Thu tháº­p dá»¯ liá»‡u tá»« má»™t trang Wikipedia"""
        try:
            url = self.base_url + quote(title.replace(' ', '_'))
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            result = {
                'url': url,
                'title': title,
                'text': '',
                'sections': {
                    'intro': '',
                    'career': '',
                    'discography': '',
                    'awards': ''
                }
            }
            
            # 1. Láº¥y intro (Ä‘oáº¡n Ä‘áº§u tiÃªn)
            content = soup.find('div', class_='mw-parser-output')
            if content:
                # Láº¥y táº¥t cáº£ Ä‘oáº¡n p trÆ°á»›c heading Ä‘áº§u tiÃªn
                intro_parts = []
                for elem in content.children:
                    if elem.name in ['h2', 'h3']:
                        break
                    if elem.name == 'p':
                        text = elem.get_text(strip=True)
                        if text:
                            intro_parts.append(text)
                result['sections']['intro'] = ' '.join(intro_parts)
            
            # 2. Láº¥y cÃ¡c section quan trá»ng
            headings = soup.find_all(['h2', 'h3'])
            for heading in headings:
                heading_text = heading.get_text(strip=True).lower()
                section_content = []
                
                # Láº¥y ná»™i dung sau heading
                elem = heading.find_next_sibling()
                while elem and elem.name not in ['h2', 'h3']:
                    if elem.name == 'p':
                        text = elem.get_text(strip=True)
                        if text and len(text) > 20:
                            section_content.append(text)
                    elif elem.name == 'ul':
                        for li in elem.find_all('li'):
                            text = li.get_text(strip=True)
                            if text:
                                section_content.append(text)
                    elem = elem.find_next_sibling()
                
                section_text = ' '.join(section_content)
                
                # PhÃ¢n loáº¡i vÃ o cÃ¡c section
                if any(kw in heading_text for kw in ['sá»± nghiá»‡p', 'career', 'hoáº¡t Ä‘á»™ng']):
                    result['sections']['career'] = section_text
                elif any(kw in heading_text for kw in ['album', 'discography', 'Ä‘Ä©a nháº¡c', 'tÃ¡c pháº©m']):
                    result['sections']['discography'] = section_text
                elif any(kw in heading_text for kw in ['giáº£i thÆ°á»Ÿng', 'award', 'thÃ nh tÃ­ch']):
                    result['sections']['awards'] = section_text
            
            # 3. Láº¥y full text (Ä‘Ã£ lÃ m sáº¡ch)
            content_div = soup.find('div', id='mw-content-text')
            if content_div:
                # Loáº¡i bá» cÃ¡c pháº§n khÃ´ng cáº§n
                for tag in content_div.find_all(['table', 'div', 'span'], 
                                                class_=['navbox', 'reference', 'mw-references-wrap', 
                                                       'mw-editsection', 'toc', 'infobox']):
                    tag.decompose()
                
                full_text = content_div.get_text(separator=' ', strip=True)
                full_text = re.sub(r'\[\d+\]', '', full_text)  # Loáº¡i bá» [1], [2]...
                full_text = re.sub(r'\s+', ' ', full_text)
                result['text'] = full_text
            
            time.sleep(self.delay)
            return result
            
        except Exception as e:
            return {
                'url': '',
                'title': title,
                'text': '',
                'sections': {'intro': '', 'career': '', 'discography': '', 'awards': ''},
                'error': str(e)
            }


class DataCollector:
    """Thu tháº­p dá»¯ liá»‡u tá»« cÃ¡c nodes trong Neo4j"""
    
    def __init__(self, uri: str, user: str, password: str, database: str = None):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))
        self.database = database
        self.wiki = WikipediaCollector()
        print(f"âœ“ ÄÃ£ káº¿t ná»‘i Neo4j: {uri}")
    
    def close(self):
        self.driver.close()
    
    def collect_and_save(self, output_file: str = 'enrichment_text_data.json', limit: int = None, 
                         labels: List[str] = None):
        """Thu tháº­p dá»¯ liá»‡u vÃ  lÆ°u vÃ o file JSON"""
        print("=" * 70)
        print("THU THáº¬P Táº¬P Dá»® LIá»†U LÃ€M GIÃ€U Tá»ª WIKIPEDIA")
        print("=" * 70)
        
        # Labels máº·c Ä‘á»‹nh náº¿u khÃ´ng chá»‰ Ä‘á»‹nh
        if labels is None:
            labels = ['Artist', 'Group', 'Song', 'Album', 'Company']
        
        print(f"ğŸ“Œ Labels: {', '.join(labels)}")
        
        # Láº¥y nodes cÃ³ URL Wikipedia vÃ  thuá»™c cÃ¡c labels chá»‰ Ä‘á»‹nh
        def get_nodes(tx):
            # Táº¡o Ä‘iá»u kiá»‡n cho labels
            label_conditions = ' OR '.join([f'n:{label}' for label in labels])
            query = f"""
            MATCH (n)
            WHERE ({label_conditions})
            AND n.url IS NOT NULL AND n.url CONTAINS 'wikipedia.org'
            RETURN n.id as id, n.name as name, n.url as url, labels(n) as labels
            ORDER BY n.name
            """
            if limit:
                query += f" LIMIT {limit}"
            return [dict(r) for r in tx.run(query)]
        
        with self.driver.session(database=self.database) if self.database else self.driver.session() as session:
            nodes = session.execute_read(get_nodes)
        
        print(f"âœ“ TÃ¬m tháº¥y {len(nodes)} nodes cÃ³ URL Wikipedia\n")
        
        # Thu tháº­p dá»¯ liá»‡u
        collected = []
        
        for i, node in enumerate(nodes, 1):
            name = node['name']
            url = node['url']
            label = node['labels'][0] if node['labels'] else 'Entity'
            
            print(f"[{i}/{len(nodes)}] {name} ({label})")
            
            # Láº¥y title tá»« URL
            title = url.split('/wiki/')[-1] if '/wiki/' in url else name
            title = unquote(title).replace('_', ' ')
            
            # Thu tháº­p tá»« Wikipedia
            data = self.wiki.collect(title)
            
            # Táº¡o record
            record = {
                'node_id': node['id'],
                'node_name': name,
                'node_label': label,
                'wikipedia_url': data.get('url', url),
                'text': data.get('text', ''),
                'sections': data.get('sections', {}),
                'text_length': len(data.get('text', ''))
            }
            
            if 'error' in data:
                record['error'] = data['error']
                print(f"  âš  Lá»—i: {data['error']}")
            else:
                print(f"  âœ“ {record['text_length']:,} kÃ½ tá»±")
            
            collected.append(record)
        
        # Lá»c bá» cÃ¡c record lá»—i hoáº·c quÃ¡ ngáº¯n
        valid_data = [r for r in collected if r['text_length'] >= 500 and 'error' not in r]
        
        # LÆ°u file
        output = {
            'metadata': {
                'description': 'Táº­p dá»¯ liá»‡u vÄƒn báº£n tá»« Wikipedia Ä‘á»ƒ lÃ m giÃ u Ä‘á»“ thá»‹ tri thá»©c',
                'source': 'Wikipedia tiáº¿ng Viá»‡t',
                'collected_at': datetime.now().isoformat(),
                'total_nodes': len(nodes),
                'valid_records': len(valid_data),
                'total_characters': sum(r['text_length'] for r in valid_data)
            },
            'data': valid_data
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        print(f"\n{'=' * 70}")
        print(f"Káº¾T QUáº¢ THU THáº¬P")
        print(f"{'=' * 70}")
        print(f"ğŸ“Š Tá»•ng nodes xá»­ lÃ½: {len(nodes)}")
        print(f"ğŸ“Š Records há»£p lá»‡: {len(valid_data)}")
        print(f"ğŸ“Š Tá»•ng kÃ½ tá»±: {output['metadata']['total_characters']:,}")
        print(f"ğŸ’¾ ÄÃ£ lÆ°u: {output_file}")
        
        return output


def main():
    import argparse
    parser = argparse.ArgumentParser(description='Thu tháº­p dá»¯ liá»‡u lÃ m giÃ u tá»« Wikipedia')
    parser.add_argument('--neo4j-uri', default='bolt://127.0.0.1:7687')
    parser.add_argument('--neo4j-user', default='neo4j')
    parser.add_argument('--neo4j-pass', default='12345678')
    parser.add_argument('--neo4j-db', default='network')
    parser.add_argument('--output', default='enrichment_text_data.json')
    parser.add_argument('--limit', type=int, default=None)
    parser.add_argument('--labels', nargs='+', default=['Artist', 'Group', 'Song', 'Album', 'Company'],
                        help='CÃ¡c labels cáº§n thu tháº­p (máº·c Ä‘á»‹nh: Artist, Group, Song, Album, Company)')
    args = parser.parse_args()
    
    collector = DataCollector(args.neo4j_uri, args.neo4j_user, args.neo4j_pass, args.neo4j_db)
    try:
        collector.collect_and_save(output_file=args.output, limit=args.limit, labels=args.labels)
    finally:
        collector.close()


if __name__ == '__main__':
    main()

