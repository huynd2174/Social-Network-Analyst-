"""
Main Chatbot Module for K-pop Knowledge Graph

This module integrates all components:
- Knowledge Graph
- GraphRAG
- Multi-hop Reasoning
- Small LLM

Provides a unified interface for the K-pop chatbot.
"""

import json
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from datetime import datetime

# Support running both as a package (streamlit) and as a script (python .../run_chatbot.py)
try:
    from .knowledge_graph import KpopKnowledgeGraph
    from .knowledge_graph_neo4j import KpopKnowledgeGraphNeo4j
    from .graph_rag import GraphRAG
    from .multi_hop_reasoning import MultiHopReasoner, ReasoningResult
    from .small_llm import SmallLLM, get_llm, TRANSFORMERS_AVAILABLE
except ImportError:  # Fallback for no-package context
    from knowledge_graph import KpopKnowledgeGraph
    from knowledge_graph_neo4j import KpopKnowledgeGraphNeo4j
    from graph_rag import GraphRAG
    from multi_hop_reasoning import MultiHopReasoner, ReasoningResult
    from small_llm import SmallLLM, get_llm, TRANSFORMERS_AVAILABLE


@dataclass
class ChatMessage:
    """A single chat message."""
    role: str  # 'user' or 'assistant'
    content: str
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    metadata: Dict = field(default_factory=dict)


@dataclass
class ChatSession:
    """A chat session with history."""
    session_id: str
    messages: List[ChatMessage] = field(default_factory=list)
    created_at: str = field(default_factory=lambda: datetime.now().isoformat())
    
    def add_message(self, role: str, content: str, metadata: Dict = None):
        """Add a message to the session."""
        self.messages.append(ChatMessage(
            role=role,
            content=content,
            metadata=metadata or {}
        ))
        
    def get_history(self, max_turns: int = 5) -> List[Dict]:
        """Get conversation history for context."""
        history = []
        for msg in self.messages[-max_turns * 2:]:
            history.append({
                "role": msg.role,
                "content": msg.content
            })
        return history


class KpopChatbot:
    """
    K-pop Knowledge Graph Chatbot.
    
    Combines GraphRAG retrieval with multi-hop reasoning
    and small LLM generation for answering K-pop questions.
    """
    
    def __init__(
        self,
        data_path: str = "data/merged_kpop_data.json",
        llm_model: str = "qwen2-0.5b",
        use_embeddings: bool = True,
        verbose: bool = True
    ):
        """
        Initialize the chatbot.
        
        Args:
            data_path: Path to merged K-pop data
            llm_model: Model key for small LLM
            use_embeddings: Whether to use semantic embeddings
            verbose: Print initialization progress
        """
        self.verbose = verbose
        self.sessions: Dict[str, ChatSession] = {}
        
        # Initialize components
        if verbose:
            print("üîÑ Initializing K-pop Chatbot...")
            
        # 1. Knowledge Graph
        if verbose:
            print("  üìä Loading Knowledge Graph...")
        self.kg = KpopKnowledgeGraph(data_path)
        
        # 2. GraphRAG
        if verbose:
            print("  üîç Initializing GraphRAG...")
        # Pass LLM to GraphRAG ƒë·ªÉ d√πng cho understanding (n·∫øu c√≥)
        # LLM s·∫Ω ƒë∆∞·ª£c load sau, n√™n pass None l√∫c ƒë·∫ßu, s·∫Ω set sau
        self.rag = GraphRAG(
            knowledge_graph=self.kg,
            use_cache=True,
            llm_for_understanding=None  # S·∫Ω set sau khi LLM load xong
        )
        
        # 3. Multi-hop Reasoner
        if verbose:
            print("  üß† Initializing Multi-hop Reasoner...")
        # Pass GraphRAG ƒë·ªÉ reasoner c√≥ th·ªÉ d√πng LLM extract entities khi thi·∫øu
        self.reasoner = MultiHopReasoner(self.kg, graph_rag=self.rag)
        
        # 4. Small LLM (optional)
        self.llm = None
        if llm_model:
            if verbose:
                print(f"  ü§ñ Loading LLM: {llm_model}...")
            try:
                self.llm = get_llm(llm_model)
                # Set LLM cho GraphRAG ƒë·ªÉ d√πng cho understanding
                self.rag.llm_for_understanding = self.llm
            except Exception as e:
                if verbose:
                    print(f"  ‚ö†Ô∏è LLM loading failed: {e}")
                    print("  üí° Using fallback mode (context-based responses)")
                self.llm = None
        else:
            if verbose:
                print("  ü§ñ LLM skipped (graph-only mode)")
            
        if verbose:
            print("‚úÖ Chatbot initialized successfully!")
            
    def create_session(self, session_id: str = None) -> str:
        """Create a new chat session."""
        if session_id is None:
            session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
        self.sessions[session_id] = ChatSession(session_id=session_id)
        return session_id
        
    def get_session(self, session_id: str) -> Optional[ChatSession]:
        """Get an existing session."""
        return self.sessions.get(session_id)
        
    def chat(
        self,
        query: str,
        session_id: str = None,
        use_multi_hop: bool = True,
        max_hops: int = 3,
        return_details: bool = False,
        use_llm: bool = True
    ) -> Dict:
        """
        Process a chat query and return response.
        
        Args:
            query: User's question
            session_id: Session ID for conversation history
            use_multi_hop: Enable multi-hop reasoning
            max_hops: Maximum reasoning hops
            return_details: Include detailed reasoning info
            
        Returns:
            Response dictionary with answer and metadata
        """
        # Get or create session
        if session_id and session_id in self.sessions:
            session = self.sessions[session_id]
        else:
            session_id = self.create_session(session_id)
            session = self.sessions[session_id]
            
        # Add user message
        session.add_message("user", query)
        
        # ============================================
        # B∆Ø·ªöC 1: GRAPHRAG - L·∫§Y CONTEXT T·ª™ ƒê·ªí TH·ªä TRI TH·ª®C
        # ============================================
        # ‚úÖ GraphRAG LU√îN ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ l·∫•y context t·ª´ Knowledge Graph
        # GraphRAG th·ª±c hi·ªán 3 b∆∞·ªõc:
        # 1. Semantic Search: T√¨m node g·∫ßn nh·∫•t b·∫±ng vector search (FAISS + embeddings)
        # 2. Expand Subgraph: T·ª´ node t√¨m ƒë∆∞·ª£c ‚Üí m·ªü r·ªông h√†ng x√≥m 1-2 hop ‚Üí l·∫•y subgraph
        # 3. Build Context: Chuy·ªÉn subgraph ‚Üí text/triples ƒë·ªÉ feed v√†o LLM
        # 
        # T·∫§T C·∫¢ th√¥ng tin ƒë·ªÅu l·∫•y t·ª´ ƒê·ªí TH·ªä TRI TH·ª®C (Knowledge Graph), kh√¥ng ph·∫£i t·ª´ LLM memory
        
        # ============================================
        # B∆Ø·ªöC 1: GRAPHRAG - RETRIEVE CONTEXT (rule-based tr∆∞·ªõc, LLM fallback)
        # ============================================
        # ‚úÖ Rule-based ch·∫°y TR∆Ø·ªöC trong retrieve_context() ‚Üí extract_entities()
        # LLM ch·ªâ ƒë∆∞·ª£c g·ªçi khi rule-based kh√¥ng ƒë·ªß ho·∫∑c kh√¥ng hi·ªÉu
        context = self.rag.retrieve_context(
            query,
            max_entities=5,
            max_hops=max_hops
        )
        
        # 2.5. Check if this is a membership Yes/No question - use reasoning directly
        import re
        query_clean = re.sub(r"[^\w\s\-]", " ", query.lower())
        query_lower = " ".join(query_clean.split())
        
        # ‚úÖ Rule-based intent detection TR∆Ø·ªöC
        is_membership_question = (
            any(kw in query_lower for kw in ['c√≥ ph·∫£i', 'ph·∫£i', 'l√† th√†nh vi√™n', 'is a member', 'belongs to', 'c√≥ th√†nh vi√™n']) and
            any(kw in query_lower for kw in ['th√†nh vi√™n', 'member'])
        )
        
        # Check if this is a "list members" question: "Ai l√† th√†nh vi√™n", "Who are members"
        is_list_members_question = any(kw in query_lower for kw in [
            'ai l√† th√†nh vi√™n', 'who are', 'th√†nh vi√™n c·ªßa', 'members of',
            'th√†nh vi√™n nh√≥m', 'th√†nh vi√™n ban nh·∫°c', 'c√≥ nh·ªØng th√†nh vi√™n'
        ]) and 'c√≥ ph·∫£i' not in query_lower and 'kh√¥ng' not in query_lower
        
        # Check if this is an "artist group" question: "Lisa thu·ªôc nh√≥m nh·∫°c n√†o"
        is_artist_group_question = any(kw in query_lower for kw in [
            'thu·ªôc nh√≥m', 'thu·ªôc nh√≥m nh·∫°c', 'nh√≥m n√†o', 'nh√≥m nh·∫°c n√†o',
            'belongs to group', 'group of', 'nh√≥m c·ªßa'
        ]) and 'c√πng' not in query_lower  # Tr√°nh nh·∫ßm v·ªõi "c√πng nh√≥m"
        
        # Check if this is a "same group" question - use reasoning directly
        is_same_group_question = any(kw in query_lower for kw in [
            'c√πng nh√≥m', 'c√πng nh√≥m nh·∫°c', 'c√πng m·ªôt nh√≥m', 'c√πng m·ªôt nh√≥m nh·∫°c',
            'same group', 'c√πng ban nh·∫°c', 'chung nh√≥m', 'chung nh√≥m nh·∫°c'
        ])
        
        # ‚úÖ LLM FALLBACK: Ch·ªâ g·ªçi LLM khi rule-based kh√¥ng detect ƒë∆∞·ª£c intent
        # V√≠ d·ª•: "c√πng m·ªôt nh√≥m nh·∫°c" c√≥ th·ªÉ kh√¥ng match pattern n·∫øu rule-based miss t·ª´ "m·ªôt"
        llm_intent = None
        if self.rag.llm_for_understanding and not (is_same_group_question or is_artist_group_question or is_membership_question or is_list_members_question):
            # Rule-based kh√¥ng detect ƒë∆∞·ª£c ‚Üí d√πng LLM ƒë·ªÉ hi·ªÉu bi·∫øn th·ªÉ ng√¥n ng·ªØ
            try:
                llm_result = self.rag._extract_entities_with_llm(query)
                if llm_result and len(llm_result) > 0:
                    llm_intent = llm_result[0].get('intent', '')
                    # Update intent flags d·ª±a tr√™n LLM
                    if llm_intent == 'same_group':
                        is_same_group_question = True
                    elif llm_intent == 'membership':
                        if 'nh√≥m' in query_lower:
                            is_artist_group_question = True
                        else:
                            is_membership_question = True
            except Exception as e:
                # N·∫øu LLM fail, gi·ªØ nguy√™n rule-based
                pass
        
        # Check if this is a "same company" question - use reasoning directly
        # M·ªü r·ªông patterns ƒë·ªÉ detect nhi·ªÅu c√°ch h·ªèi h∆°n
        is_same_company_question = any(kw in query_lower for kw in [
            'c√πng c√¥ng ty', 'same company', 'c√πng h√£ng', 'c√πng label', 'c√πng h√£ng ƒëƒ©a',
            'c√πng c√¥ng ty hay', 'c√πng h√£ng hay', 'c√πng c√¥ng ty kh√¥ng', 'c√πng h√£ng kh√¥ng',
            'c√≥ c√πng c√¥ng ty', 'c√≥ c√πng h√£ng', 'c√≥ c√πng label'
        ])
        
        # B·ªï sung nh·∫≠n d·∫°ng cho c√°c c√¢u h·ªèi ƒëa d·∫°ng trong dataset ƒë√°nh gi√°
        is_genre_question = 'th·ªÉ lo·∫°i' in query_lower or 'genre' in query_lower
        is_song_in_album_question = (
            ('b√†i h√°t' in query_lower and 'album' in query_lower)
            or ('contains' in query_lower and 'released' in query_lower)
        )
        is_company_via_group_question = (
            'c√¥ng ty n√†o qu·∫£n l√Ω' in query_lower
            or ('ƒë∆∞·ª£c qu·∫£n l√Ω b·ªüi' in query_lower and 'nh√≥m' in query_lower)
            or ('qu·∫£n l√Ω' in query_lower and 'nh√≥m' in query_lower)
        )
        is_occupation_question = 'ngh·ªÅ nghi·ªáp' in query_lower or 'occupation' in query_lower
        is_artist_song_question = ('b√†i h√°t' in query_lower and ('tr√¨nh b√†y' in query_lower or 'h√°t' in query_lower))
        is_artist_album_question = ('album' in query_lower and ('ph√°t h√†nh' in query_lower or 'ra m·∫Øt' in query_lower))
        is_artist_genre_question = is_genre_question and ('ngh·ªá sƒ©' in query_lower or 'artist' in query_lower or 'ca sƒ©' in query_lower)
        is_same_occupation_question = is_occupation_question and any(kw in query_lower for kw in ['ai', 'ngh·ªá sƒ©', 'artist'])
        is_album_song_group_question = ('album' in query_lower and 'b√†i h√°t' in query_lower and 'nh√≥m' in query_lower)
        is_three_hop_hint = ('qua' in query_lower and 'r·ªìi' in query_lower) or ('th√¥ng qua' in query_lower and 'sau ƒë√≥' in query_lower)
        # 3-hop ki·ªÉu Song -> Artist -> Group -> Company (t·ª´ b·ªô ƒë√°nh gi√°)
        is_song_company_chain_question = (
            ('b√†i h√°t' in query_lower and ('c√¥ng ty' in query_lower or 'label' in query_lower))
            or '(3-hop)' in query_lower
            or ('qua' in query_lower and 'nh√≥m' in query_lower and 'c√¥ng ty' in query_lower)
        )
        
        # X√°c ƒë·ªãnh label k·ª≥ v·ªçng t·ª´ c√¢u h·ªèi ƒë·ªÉ l·ªçc th·ª±c th·ªÉ ƒë√∫ng lo·∫°i
        expected_labels = set()
        if is_same_group_question or is_list_members_question or 'nh√≥m' in query_lower or 'ban nh·∫°c' in query_lower:
            expected_labels.add('Group')
        if is_membership_question or 'ngh·ªá sƒ©' in query_lower or 'ca sƒ©' in query_lower or 'artist' in query_lower:
            expected_labels.add('Artist')
        if is_same_company_question or is_company_via_group_question or 'c√¥ng ty' in query_lower or 'label' in query_lower or 'h√£ng' in query_lower:
            expected_labels.add('Company')
        if 'b√†i h√°t' in query_lower or 'song' in query_lower:
            expected_labels.add('Song')
        if 'album' in query_lower:
            expected_labels.add('Album')
        if is_genre_question or 'th·ªÉ lo·∫°i' in query_lower or 'genre' in query_lower:
            expected_labels.add('Genre')
        if is_occupation_question or 'ngh·ªÅ' in query_lower:
            expected_labels.add('Occupation')
        if is_song_company_chain_question:
            expected_labels.update({'Song', 'Artist', 'Group', 'Company'})
        
        # Check if this is a "list members" question: "Ai l√† th√†nh vi√™n", "Who are members"
        is_list_members_question = any(kw in query_lower for kw in [
            'ai l√† th√†nh vi√™n', 'who are', 'th√†nh vi√™n c·ªßa', 'members of',
            'th√†nh vi√™n nh√≥m', 'th√†nh vi√™n ban nh·∫°c', 'c√≥ nh·ªØng th√†nh vi√™n'
        ]) and 'c√≥ ph·∫£i' not in query_lower and 'kh√¥ng' not in query_lower
        
        # ============================================
        # B∆Ø·ªöC 2: MULTI-HOP REASONING - SUY LU·∫¨N TR√äN ƒê·ªí TH·ªä
        # ============================================
        # ‚úÖ ƒê·∫£m b·∫£o multi-hop reasoning LU√îN ƒë∆∞·ª£c s·ª≠ d·ª•ng khi enabled
        # Multi-hop reasoning s·ª≠ d·ª•ng ƒê·ªí TH·ªä TRI TH·ª®C ƒë·ªÉ:
        # - T√¨m paths gi·ªØa entities (BFS/DFS tr√™n graph)
        # - Traverse relationships (MEMBER_OF, MANAGED_BY, etc.)
        # - So s√°nh entities qua nhi·ªÅu hops
        # 
        # T·∫§T C·∫¢ suy lu·∫≠n ƒë·ªÅu d·ª±a tr√™n ƒê·ªí TH·ªä TRI TH·ª®C, kh√¥ng ph·∫£i LLM reasoning
        reasoning_result = None
        if use_multi_hop:
            # ‚úÖ CHI·∫æN L∆Ø·ª¢C AN TO√ÄN: Rule-based extraction + KG validation tr∆∞·ªõc khi reasoning
            # 
            # ∆Øu ti√™n extract entities cho same_group/same_company/list_members questions b·∫±ng rule-based
            # V√¨ ƒë√¢y l√† c√¢u h·ªèi factual, c·∫ßn entities ch√≠nh x√°c ƒë·ªÉ reasoning ƒë√∫ng
            eval_pattern_question = (
                is_same_group_question or
                is_same_company_question or
                is_list_members_question or
                is_genre_question or
                is_song_in_album_question or
                is_company_via_group_question or
                is_occupation_question or
                is_artist_song_question or
                is_artist_album_question or
                is_artist_genre_question or
                is_same_occupation_question or
                is_album_song_group_question or
            is_three_hop_hint or
            is_song_company_chain_question
            )
            
            if is_same_group_question or is_same_company_question or is_list_members_question or is_artist_group_question:
                # ‚úÖ CHI·∫æN L∆Ø·ª¢C HYBRID: Rule-based + LLM understanding
                # 1. Th·ª≠ rule-based tr∆∞·ªõc (nhanh, ch√≠nh x√°c cho t√™n chu·∫©n)
                extracted = self._extract_entities_for_membership(query, expected_labels=expected_labels)
                
                # V·ªõi list_members_question v√† artist_group_question, ch·ªâ c·∫ßn 1 entity
                min_entities = 1 if (is_list_members_question or is_artist_group_question) else 2
                
                # 2. N·∫øu rule-based kh√¥ng ƒë·ªß ‚Üí d√πng LLM understanding (fallback)
                if len(extracted) < min_entities and self.rag.llm_for_understanding:
                    try:
                        # G·ªçi LLM ƒë·ªÉ extract entities
                        llm_entities = self.rag._extract_entities_with_llm(query)
                        # Validate v√† th√™m v√†o extracted
                        for llm_e in llm_entities:
                            entity_id = llm_e.get('text', '')
                            if entity_id and entity_id not in extracted:
                                # Validate v·ªõi KG
                                entity_data = self.kg.get_entity(entity_id)
                                if entity_data:
                                    extracted.append(entity_id)
                                    # Update context
                                    if not any(existing['id'].lower() == entity_id.lower() for existing in context['entities']):
                                        context['entities'].append({
                                            'id': entity_id,
                                            'type': entity_data.get('label', 'Unknown'),
                                            'score': llm_e.get('score', 0.8)
                                        })
                    except Exception as e:
                        # N·∫øu LLM fail, ti·∫øp t·ª•c v·ªõi rule-based
                        pass
                
                if len(extracted) >= min_entities:
                    # ‚úÖ VALIDATE: Verify t·∫•t c·∫£ entities v·ªõi KG tr∆∞·ªõc khi reasoning
                    validated_entities = []
                    for e in extracted:
                        entity_data = self.kg.get_entity(e)
                        if entity_data:  # Ch·ªâ d√πng n·∫øu validate th√†nh c√¥ng
                            validated_entities.append(e)
                    
                    if len(validated_entities) >= min_entities:
                        # C√≥ ƒë·ªß entities ƒë√£ validate ‚Üí d√πng ngay ƒë·ªÉ reasoning (nhanh v√† ch√≠nh x√°c)
                        # ‚ö†Ô∏è QUAN TR·ªåNG: Multi-hop reasoning do Reasoner th·ª±c hi·ªán (graph algorithm)
                        # KH√îNG giao cho LLM nh·ªè
                        reasoning_result = self.reasoner.reason(
                            query,
                            start_entities=validated_entities,
                            max_hops=max_hops
                        )
                        # Update context v·ªõi entities ƒë√£ validate
                        for e in validated_entities:
                            if not any(existing['id'].lower() == e.lower() for existing in context['entities']):
                                entity_data = self.kg.get_entity(e)
                                if entity_data:
                                    context['entities'].append({
                                        'id': e,
                                        'type': entity_data.get('label', 'Unknown'),
                                        'score': 0.9  # High score v√¨ ƒë√£ verify v·ªõi KG
                                    })
                elif len(extracted) == 1 and (is_artist_group_question or is_list_members_question):
                    # Ch·ªâ c√≥ 1 entity v√† ƒë√¢y l√† c√¢u h·ªèi ch·ªâ c·∫ßn 1 entity ‚Üí OK
                    reasoning_result = self.reasoner.reason(
                        query,
                        start_entities=extracted,
                        max_hops=max_hops
                    )
                elif len(extracted) == 1:
                    # Ch·ªâ c√≥ 1 entity ‚Üí v·ªõi same_company/same_group questions, c·∫ßn ƒë·ªß 2
                    if is_same_company_question or is_same_group_question:
                        # Th·ª≠ extract l·∫°i v·ªõi logic m·∫°nh h∆°n
                        # Ho·∫∑c ƒë·ªÉ reasoner t·ª± extract t·ª´ query
                        reasoning_result = self.reasoner.reason(
                            query,
                            start_entities=extracted,  # C√≥ 1 entity, reasoner s·∫Ω extract th√™m
                            max_hops=max_hops
                        )
                        # N·∫øu reasoner v·∫´n kh√¥ng extract ƒë∆∞·ª£c ƒë·ªß 2, s·∫Ω tr·∫£ v·ªÅ l·ªói r√µ r√†ng
                    else:
                        # V·ªõi c√°c c√¢u h·ªèi kh√°c, 1 entity c√≥ th·ªÉ ƒë·ªß
                        reasoning_result = self.reasoner.reason(
                            query,
                            start_entities=extracted,
                            max_hops=max_hops
                        )
                else:
                    # Kh√¥ng t√¨m ƒë∆∞·ª£c entities ‚Üí reasoner s·∫Ω t·ª± extract
                    reasoning_result = self.reasoner.reason(
                        query,
                        start_entities=[],
                        max_hops=max_hops
                    )
            elif (eval_pattern_question or is_artist_group_question) and len(context['entities']) < 2:
                # Membership question: try to extract entities n·∫øu GraphRAG kh√¥ng t√¨m ƒë·ªß
                extracted = self._extract_entities_for_membership(query, expected_labels=expected_labels)
                if extracted:
                    # Add to context for consistency
                    for e in extracted:
                        if not any(existing['id'].lower() == e.lower() for existing in context['entities']):
                            entity_data = self.kg.get_entity(e)
                            if entity_data:
                                context['entities'].append({
                                    'id': e,
                                    'type': entity_data.get('label', 'Unknown'),
                                    'score': 0.8
                                })
            
            # ‚úÖ LU√îN ch·∫°y multi-hop reasoning n·∫øu ch∆∞a c√≥ result
            # QUAN TR·ªåNG: Reasoning v·∫´n do ƒê·ªí TH·ªä th·ª±c hi·ªán (graph traversal, path search)
            # LLM ch·ªâ d√πng ƒë·ªÉ hi·ªÉu ƒë·∫ßu v√†o (intent, entities, relations) ‚Üí kh√¥ng l√†m reasoning
            if reasoning_result is None:
                if context['entities']:
                    entities = [e['id'] for e in context['entities']]
                    
                    # S·ª≠ d·ª•ng multi_hop_depth t·ª´ LLM understanding n·∫øu c√≥
                    # (LLM ƒë√£ detect depth ‚Üí d√πng ƒë·ªÉ optimize graph traversal)
                    detected_depth = max_hops
                    for e in context['entities']:
                        if e.get('multi_hop_depth'):
                            detected_depth = max(detected_depth, e.get('multi_hop_depth', max_hops))
                            break
                    
                    reasoning_result = self.reasoner.reason(
                        query,
                        start_entities=entities,
                        max_hops=detected_depth  # S·ª≠ d·ª•ng depth t·ª´ LLM understanding
                    )
                else:
                    # Kh√¥ng c√≥ entities ‚Üí reasoner s·∫Ω t·ª± extract
                    reasoning_result = self.reasoner.reason(
                        query,
                        start_entities=[],
                        max_hops=max_hops
                    )
        
        # ============================================
        # B∆Ø·ªöC 3: FORMAT CONTEXT CHO LLM (T·ª´ GraphRAG - Knowledge Graph)
        # ============================================
        # ‚úÖ LLM LU√îN nh·∫≠n context t·ª´ GraphRAG (y√™u c·∫ßu b√†i t·∫≠p)
        # Context bao g·ªìm:
        # - Entities t·ª´ ƒë·ªì th·ªã (nodes)
        # - Relationships t·ª´ ƒë·ªì th·ªã (edges)
        # - Facts t·ª´ ƒë·ªì th·ªã (triples)
        # - Paths t·ª´ ƒë·ªì th·ªã (multi-hop paths)
        # 
        # T·∫§T C·∫¢ context ƒë·ªÅu t·ª´ ƒê·ªí TH·ªä TRI TH·ª®C, LLM ch·ªâ nh·∫≠n v√† format th√†nh c√¢u tr·∫£ l·ªùi
        # Gi·ªõi h·∫°n context ƒë·ªÉ tr√°nh v∆∞·ª£t qu√° max_length c·ªßa model
        # QUAN TR·ªåNG: Gi·∫£m context size ƒë·ªÉ tr√°nh LLM b·ªã nhi·ªÖu (1969 entities ‚Üí qu√° nhi·ªÅu!)
        if reasoning_result and reasoning_result.confidence >= 0.6:
            # C√≥ reasoning result t·ªët ‚Üí gi·∫£m context size (ch·ªâ l·∫•y essentials)
            formatted_context = self.rag.format_context_for_llm(context, max_tokens=5000)
        else:
            # Kh√¥ng c√≥ reasoning result ho·∫∑c confidence th·∫•p ‚Üí c·∫ßn nhi·ªÅu context h∆°n
            formatted_context = self.rag.format_context_for_llm(context, max_tokens=10000)
        
        # Add reasoning info to context (Multi-hop reasoning results t·ª´ ƒë·ªì th·ªã)
        # Reasoning results c≈©ng ƒë∆∞·ª£c t·∫°o t·ª´ ƒê·ªí TH·ªä TRI TH·ª®C (graph traversal)
        if reasoning_result:
            formatted_context += f"\n\n=== K·∫æT QU·∫¢ SUY LU·∫¨N MULTI-HOP (T·ª´ ƒê·ªì Th·ªã Tri Th·ª©c) ===\n{reasoning_result.explanation}"
            if reasoning_result.steps:
                formatted_context += f"\n\nS·ªë b∆∞·ªõc suy lu·∫≠n: {len(reasoning_result.steps)}-hop"
                for i, step in enumerate(reasoning_result.steps[:3], 1):
                    formatted_context += f"\n  B∆∞·ªõc {i}: {step.explanation[:100]}"
        
        # ============================================
        # B∆Ø·ªöC 4: GENERATE RESPONSE - LLM T·∫†O C√ÇU TR·∫¢ L·ªúI T·ª™ CONTEXT
        # ============================================
        # ‚úÖ Y√äU C·∫¶U B√ÄI T·∫¨P: "L·ª±a ch·ªçn m·ªôt m√¥ h√¨nh ng√¥n ng·ªØ nh·ªè" ‚Üí PH·∫¢I d√πng LLM
        # LLM NH·∫¨N context t·ª´ Knowledge Graph (GraphRAG) v√† t·∫°o c√¢u tr·∫£ l·ªùi t·ª± nhi√™n
        # 
        # LLM KH√îNG t·ª± nghƒ© ra th√¥ng tin - CH·ªà format context t·ª´ ƒë·ªì th·ªã th√†nh c√¢u tr·∫£ l·ªùi
        # T·∫•t c·∫£ facts ƒë·ªÅu t·ª´ ƒê·ªí TH·ªä TRI TH·ª®C:
        # - Entities: t·ª´ nodes trong graph
        # - Relationships: t·ª´ edges trong graph  
        # - Facts: t·ª´ triples (source, relationship, target) trong graph
        # - Reasoning: t·ª´ graph traversal (paths, hops)
        
        # If reasoning found a direct answer for membership, same group, or same company, use it (more accurate than LLM)
        # QUAN TR·ªåNG: ∆Øu ti√™n reasoning result cho c√°c c√¢u h·ªèi factual (tr√°nh LLM hallucinate)
        if (is_membership_question or is_same_group_question or is_same_company_question) and reasoning_result and reasoning_result.answer_text:
            # For membership/same group/same company questions, ALWAYS prioritize reasoning result if available
            # Reasoning is more accurate than LLM for factual checks
            # ‚úÖ QUAN TR·ªåNG: LU√îN d√πng reasoning result tr·ª±c ti·∫øp, KH√îNG qua LLM ƒë·ªÉ tr√°nh hallucination
            response = reasoning_result.answer_text
            if reasoning_result.answer_entities:
                entities_str = ", ".join(reasoning_result.answer_entities[:10])
                if entities_str and entities_str not in response:
                    response += f"\n\nDanh s√°ch: {entities_str}"
            # ‚úÖ B·ªè qua LLM generation cho same_group/same_company questions ƒë·ªÉ tr√°nh tr·∫£ l·ªùi sai
        elif self.llm and use_llm:
            # ‚úÖ S·ª¨ D·ª§NG Small LLM v·ªõi context t·ª´ Knowledge Graph (ƒë√∫ng y√™u c·∫ßu)
            history = session.get_history(max_turns=3)
            response = self.llm.generate(
                query,
                context=formatted_context,  # Context t·ª´ GraphRAG (Knowledge Graph)
                history=history
            )
        elif reasoning_result and reasoning_result.answer_text:
            # Fallback: N·∫øu LLM kh√¥ng available, d√πng reasoning result
            # (Nh∆∞ng ∆∞u ti√™n d√πng LLM ƒë·ªÉ ƒë√°p ·ª©ng y√™u c·∫ßu b√†i t·∫≠p)
            response = reasoning_result.answer_text
            if reasoning_result.answer_entities:
                entities_str = ", ".join(reasoning_result.answer_entities[:10])
                if len(reasoning_result.answer_entities) > 10:
                    entities_str += f" v√† {len(reasoning_result.answer_entities) - 10} kh√°c"
                if entities_str and entities_str not in response:
                    response += f"\n\nDanh s√°ch: {entities_str}"
        elif context['facts']:
            # Fallback: D√πng facts t·ª´ Knowledge Graph
            response = "D·ª±a tr√™n ƒë·ªì th·ªã tri th·ª©c:\n" + "\n".join(f"‚Ä¢ {f}" for f in context['facts'][:5])
        else:
            response = "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong ƒë·ªì th·ªã tri th·ª©c."
                
        # Add assistant message
        session.add_message("assistant", response, {
            "entities": [e['id'] for e in context['entities']],
            "reasoning_type": reasoning_result.reasoning_type.value if reasoning_result else None
        })
        
        # Build response
        result = {
            "session_id": session_id,
            "query": query,
            "response": response,
            "entities_found": len(context['entities']),
            "reasoning_hops": len(reasoning_result.steps) if reasoning_result else 0
        }
        
        if return_details:
            result["context"] = context
            result["reasoning"] = {
                "type": reasoning_result.reasoning_type.value if reasoning_result else None,
                "steps": [
                    {
                        "hop": s.hop_number,
                        "operation": s.operation,
                        "explanation": s.explanation
                    }
                    for s in reasoning_result.steps
                ] if reasoning_result else [],
                "confidence": reasoning_result.confidence if reasoning_result else 0
            }
            result["formatted_context"] = formatted_context
            
        return result
        
    def answer_yes_no(
        self,
        query: str,
        return_details: bool = False
    ) -> Dict:
        """
        Answer a Yes/No question.
        
        Args:
            query: Yes/No question
            return_details: Include detailed info
            
        Returns:
            Answer dictionary
        """
        query_lower = query.lower()
        
        # Get context
        context = self.rag.retrieve_context(query, max_entities=5, max_hops=2)
        formatted_context = self.rag.format_context_for_llm(context)
        
        # Perform reasoning
        entities = [e['id'] for e in context['entities']]
        reasoning_result = self.reasoner.reason(query, entities, max_hops=2)
        
        # Check if reasoning result already has a Yes/No answer
        if reasoning_result and reasoning_result.answer_text:
            answer_text_lower = reasoning_result.answer_text.lower()
            if answer_text_lower.startswith('c√≥') or 'l√† th√†nh vi√™n' in answer_text_lower:
                return {
                    "query": query,
                    "answer": "C√≥",
                    "confidence": reasoning_result.confidence,
                    "explanation": reasoning_result.explanation
                }
            elif answer_text_lower.startswith('kh√¥ng') or 'kh√¥ng ph·∫£i' in answer_text_lower:
                return {
                    "query": query,
                    "answer": "Kh√¥ng",
                    "confidence": reasoning_result.confidence,
                    "explanation": reasoning_result.explanation
                }
        
        # Rule-based answer FIRST (more accurate for knowledge graph queries)
        answer = None
        confidence = 0.0
        
        # Pattern 1: "X c√≥ ph·∫£i l√† th√†nh vi√™n c·ªßa Y kh√¥ng?" 
        if 'th√†nh vi√™n' in query_lower or 'member' in query_lower:
            # Find artist and group in context
            artist_entity = None
            group_entity = None
            
            for entity in context['entities']:
                if entity['type'] == 'Artist':
                    artist_entity = entity
                elif entity['type'] == 'Group':
                    group_entity = entity
            
            # If we have both artist and group, check membership directly
            if artist_entity and group_entity:
                artist_name = artist_entity['id']
                group_name = group_entity['id']
                groups = self.kg.get_artist_groups(artist_name)
                
                if group_name in groups:
                    answer = "C√≥"
                    confidence = 1.0
                else:
                    answer = "Kh√¥ng"
                    confidence = 1.0
            elif artist_entity:
                # Only have artist, check all groups
                artist_name = artist_entity['id']
                groups = self.kg.get_artist_groups(artist_name)
                # Check if any group is mentioned in query or context
                query_groups = [e['id'] for e in context['entities'] if e['type'] == 'Group']
                if query_groups:
                    # Check if artist is member of any mentioned group
                    if any(g in groups for g in query_groups):
                        answer = "C√≥"
                        confidence = 1.0
                    else:
                        answer = "Kh√¥ng"
                        confidence = 0.9
                else:
                    # No group found, check if group name is in query text
                    for group in groups:
                        if group.lower() in query_lower:
                            answer = "C√≥"
                            confidence = 1.0
                            break
                    if answer is None:
                        answer = "Kh√¥ng"
                        confidence = 0.8
            else:
                # No artist found
                answer = "Kh√¥ng"
                confidence = 0.7
                
        # Pattern 2: "X thu·ªôc c√¥ng ty Y" (True/False check)
        elif 'thu·ªôc c√¥ng ty' in query_lower or 'thu·ªôc company' in query_lower:
            # Extract company name from query (after "thu·ªôc c√¥ng ty")
            for entity in context['entities']:
                if entity['type'] == 'Group':
                    company = self.kg.get_group_company(entity['id'])
                    if company and company.lower() in query_lower:
                        answer = "ƒê√∫ng"
                        confidence = 1.0
                        break
                    elif company:
                        answer = "Sai"
                        confidence = 0.9
                        break
            if answer is None:
                answer = "Sai"
                confidence = 0.7
                
        # Pattern 3: "X v√† Y c√≥ c√πng c√¥ng ty kh√¥ng?"
        elif 'c√πng c√¥ng ty' in query_lower or 'same company' in query_lower:
            if len(context['entities']) >= 2:
                result = self.reasoner.check_same_company(
                    context['entities'][0]['id'],
                    context['entities'][1]['id']
                )
                if result.answer_entities:
                    answer = "C√≥"
                    confidence = 1.0
                else:
                    answer = "Kh√¥ng"
                    confidence = 0.9
                    
        # Fallback: Use reasoning result
        if answer is None:
            answer_text = reasoning_result.answer_text.lower() if reasoning_result else ""
            if any(word in answer_text for word in ['c√≥', 'ƒë√∫ng', 'yes', 'thu·ªôc', 'l√†', 'c√πng']):
                answer = "C√≥"
                confidence = reasoning_result.confidence if reasoning_result else 0.6
            elif any(word in answer_text for word in ['kh√¥ng', 'sai', 'no', 'kh√°c', 'kh√¥ng r√µ']):
                answer = "Kh√¥ng"
                confidence = reasoning_result.confidence if reasoning_result else 0.6
            else:
                # Try LLM as last resort
                if self.llm:
                    try:
                        llm_result = self.llm.evaluate_yes_no(query, formatted_context)
                        answer = llm_result['answer']
                        confidence = llm_result['confidence']
                    except:
                        answer = "Kh√¥ng"
                        confidence = 0.5
                else:
                    answer = "Kh√¥ng"
                    confidence = 0.5
                
        result = {
            "query": query,
            "answer": answer,
            "confidence": confidence,
            "explanation": reasoning_result.explanation if reasoning_result else ""
        }
        
        if return_details:
            result["context"] = context
            result["reasoning"] = reasoning_result
            
        return result
        
    def answer_multiple_choice(
        self,
        query: str,
        choices: List[str],
        return_details: bool = False
    ) -> Dict:
        """
        Answer a multiple choice question.
        
        Args:
            query: Question
            choices: List of choices
            return_details: Include detailed info
            
        Returns:
            Answer dictionary
        """
        query_lower = query.lower()
        
        # Get context
        context = self.rag.retrieve_context(query, max_entities=5, max_hops=2)
        formatted_context = self.rag.format_context_for_llm(context)
        
        # Perform reasoning
        entities = [e['id'] for e in context['entities']]
        reasoning_result = self.reasoner.reason(query, entities, max_hops=2)
        
        selected_index = None
        selected_choice = None
        confidence = 0.0
        
        # ============================================
        # SMART ANSWER SELECTION BASED ON QUERY TYPE
        # ============================================
        
        # Pattern 1: "C√¥ng ty n√†o qu·∫£n l√Ω X?" - find company in choices
        if 'c√¥ng ty' in query_lower or 'company' in query_lower:
            for entity in context['entities']:
                if entity['type'] == 'Group':
                    company = self.kg.get_group_company(entity['id'])
                    if company:
                        # Find matching choice
                        for i, choice in enumerate(choices):
                            if company.lower() in choice.lower() or choice.lower() in company.lower():
                                selected_index = i
                                selected_choice = choices[i]
                                confidence = 1.0
                                break
                    break
                    
        # Pattern 2: "X thu·ªôc nh√≥m n√†o?" - find group in choices
        elif 'nh√≥m n√†o' in query_lower or 'thu·ªôc nh√≥m' in query_lower:
            for entity in context['entities']:
                if entity['type'] == 'Artist':
                    groups = self.kg.get_artist_groups(entity['id'])
                    for group in groups:
                        for i, choice in enumerate(choices):
                            if group.lower() in choice.lower() or choice.lower() in group.lower():
                                selected_index = i
                                selected_choice = choices[i]
                                confidence = 1.0
                                break
                        if selected_index is not None:
                            break
                    break
                    
        # Pattern 3: "Nh√≥m n√†o c√πng c√¥ng ty v·ªõi X?" - find labelmates in choices
        elif 'c√πng c√¥ng ty' in query_lower or 'labelmate' in query_lower:
            for entity in context['entities']:
                if entity['type'] == 'Group':
                    labelmates = self.reasoner.get_labelmates(entity['id'])
                    for labelmate in labelmates.answer_entities:
                        for i, choice in enumerate(choices):
                            if labelmate.lower() in choice.lower() or choice.lower() in labelmate.lower():
                                selected_index = i
                                selected_choice = choices[i]
                                confidence = 0.9
                                break
                        if selected_index is not None:
                            break
                    break
        
        # Fallback: Score-based selection using context and reasoning result
        if selected_index is None:
            # Combine context and reasoning for better matching
            search_text = formatted_context.lower()
            if reasoning_result:
                search_text += " " + reasoning_result.answer_text.lower()
                search_text += " " + " ".join(reasoning_result.answer_entities).lower()
            
            scores = []
            for i, choice in enumerate(choices):
                score = 0
                choice_clean = choice.lower().strip()
                
                # Exact match - highest score
                if choice_clean in search_text:
                    score += 10
                    
                # Word matching
                choice_words = [w for w in choice_clean.split() if len(w) > 2]
                for word in choice_words:
                    if word in search_text:
                        score += 2
                        
                # Entity matching
                for entity in context['entities']:
                    if entity['id'].lower() in choice_clean:
                        score += 3
                        
                scores.append(score)
                
            max_score = max(scores) if scores else 0
            if max_score > 0:
                selected_index = scores.index(max_score)
                selected_choice = choices[selected_index]
                confidence = min(max_score / 10, 1.0)
            else:
                # Last resort: try LLM
                if self.llm:
                    try:
                        llm_result = self.llm.evaluate_multiple_choice(query, choices, formatted_context)
                        selected_index = llm_result['selected_index']
                        selected_choice = llm_result['selected_choice']
                        confidence = llm_result['confidence']
                    except:
                        selected_index = 0  # Default to first choice
                        selected_choice = choices[0]
                        confidence = 0.25
                else:
                    selected_index = 0
                    selected_choice = choices[0]
                    confidence = 0.25
                
        result = {
            "query": query,
            "choices": choices,
            "selected_choice": selected_choice,
            "selected_index": selected_index,
            "selected_letter": chr(65 + selected_index) if selected_index is not None else None,
            "confidence": confidence
        }
        
        if return_details:
            result["context"] = context
            result["formatted_context"] = formatted_context
            
        return result
    
    def _extract_entities_for_membership(self, query: str, expected_labels: Optional[set] = None) -> List[str]:
        """
        Extract entities from query for membership questions.
        Tries to find artist and group names even if GraphRAG didn't find them.
        
        expected_labels: t·∫≠p label ∆∞u ti√™n (Artist, Group, Company, Song, Album, Genre, Occupation)
        N·∫øu provided, ch·ªâ gi·ªØ th·ª±c th·ªÉ c√≥ label trong t·∫≠p n√†y (ƒë·ªÉ gi·∫£m nhi·ªÖu).
        """
        # ƒê·∫£m b·∫£o c√≥ s·∫µn map bi·∫øn th·ªÉ t·ª´ graph
        self._ensure_entity_variant_map()
        variant_map = self._entity_variant_map
        expected_labels = expected_labels or set()
        
        entities = []
        query_lower = query.lower()
        
        # Try to find group/artist/others (case-insensitive, filtered by expected_labels n·∫øu c√≥)
        all_groups = [node for node, data in self.kg.graph.nodes(data=True) 
                     if data.get('label') == 'Group' and (not expected_labels or 'Group' in expected_labels)]
        
        all_artists = [node for node, data in self.kg.graph.nodes(data=True) 
                      if data.get('label') == 'Artist' and (not expected_labels or 'Artist' in expected_labels)]
        
        # Th√™m c√°c lo·∫°i kh√°c n·∫øu c·∫ßn cho intent (song/album/company/genre/occupation)
        all_companies = [node for node, data in self.kg.graph.nodes(data=True)
                        if data.get('label') == 'Company' and (not expected_labels or 'Company' in expected_labels)]
        all_songs = [node for node, data in self.kg.graph.nodes(data=True)
                    if data.get('label') == 'Song' and (not expected_labels or 'Song' in expected_labels)]
        all_albums = [node for node, data in self.kg.graph.nodes(data=True)
                     if data.get('label') == 'Album' and (not expected_labels or 'Album' in expected_labels)]
        all_genres = [node for node, data in self.kg.graph.nodes(data=True)
                     if data.get('label') == 'Genre' and (not expected_labels or 'Genre' in expected_labels)]
        all_occupations = [node for node, data in self.kg.graph.nodes(data=True)
                          if data.get('label') == 'Occupation' and (not expected_labels or 'Occupation' in expected_labels)]

        # Helper: normalize v√† sinh variants cho m·ªôt t√™n node
        def _variants(name: str) -> List[str]:
            base = self._normalize_entity_name(name).lower()
            variants = {
                base,  # Original
                base.replace('-', ' '),  # "go-won" ‚Üí "go won"
                base.replace('-', ''),   # "go-won" ‚Üí "gowon"
                base.replace(' ', ''),   # "go won" ‚Üí "gowon"
                base.replace(' ', '-'),  # "go won" ‚Üí "go-won"
            }
            return list(variants)  # Lo·∫°i b·ªè tr√πng l·∫∑p

        # ===== Graph -> Query: qu√©t n-gram (1-4 words) ƒë·ªÉ b·∫Øt c·∫∑p t√™n li·ªÅn nhau =====
        # QUAN TR·ªåNG: X·ª≠ l√Ω tokens c√≥ dash trong ƒë√≥ (nh∆∞ "won-young")
        # T√°ch tokens, nh∆∞ng c≈©ng t√°ch c√°c token c√≥ dash th√†nh nhi·ªÅu parts
        tokens = query_lower.split()
        expanded_tokens = []
        for token in tokens:
            expanded_tokens.append(token)  # Gi·ªØ nguy√™n token g·ªëc
            # N·∫øu token c√≥ dash, th√™m c√°c parts
            if '-' in token:
                parts = token.split('-')
                expanded_tokens.extend(parts)  # "won-young" ‚Üí ["won-young", "won", "young"]
        
        ngrams = []
        for n in [1, 2, 3, 4]:
            # T·∫°o n-grams t·ª´ c·∫£ tokens g·ªëc v√† expanded_tokens
            for token_list in [tokens, expanded_tokens]:
                for i in range(len(token_list) - n + 1):
                    ngram = " ".join(token_list[i:i+n])
                    ngrams.append(ngram)  # Original: "go won", "jang won-young", "jang won young"
                    # th√™m phi√™n b·∫£n kh√¥ng d·∫•u c√°ch ƒë·ªÉ b·∫Øt "go won" vs "gowon"
                    ngrams.append(ngram.replace(" ", ""))
                    # th√™m phi√™n b·∫£n thay space b·∫±ng g·∫°ch ƒë·ªÉ b·∫Øt "jang won young" vs "jang-won-young"
                    ngrams.append(ngram.replace(" ", "-"))
                    # QUAN TR·ªåNG: X·ª≠ l√Ω t√™n c√≥ d·∫•u g·∫°ch ngang trong query
                    # N·∫øu ngram c√≥ d·∫•u g·∫°ch ngang, t·∫°o th√™m variant v·ªõi space
                    if '-' in ngram:
                        ngrams.append(ngram.replace("-", " "))  # "won-young" ‚Üí "won young", "jang won-young" ‚Üí "jang won young"
                        ngrams.append(ngram.replace("-", ""))   # "won-young" ‚Üí "wonyoung"
        
        # Lo·∫°i b·ªè tr√πng l·∫∑p
        ngrams = list(dict.fromkeys(ngrams))

        matched_from_graph = []
        candidate_scores = []  # list of (name, score, label)
        token_set = set(tokens)

        # Track normalized names ƒë·ªÉ tr√°nh duplicate (v√≠ d·ª•: "Ros√©" v√† "Ros√© (ca sƒ©)" ‚Üí ch·ªâ gi·ªØ 1)
        normalized_seen = set()
        # Track c√°c t·ª´ ƒë√£ ƒë∆∞·ª£c match trong t√™n ƒë·∫ßy ƒë·ªß ƒë·ªÉ tr√°nh match single word khi ƒë√£ c√≥ match ƒë·∫ßy ƒë·ªß
        # V√≠ d·ª•: n·∫øu ƒë√£ match "Yoo Jeong-yeon", th√¨ kh√¥ng match "Yoo" n·ªØa
        words_in_matched_full_names = set()
        
        # QUAN TR·ªåNG: ƒê·ªãnh nghƒ©a query_words_list TR∆Ø·ªöC function _match_list ƒë·ªÉ c√≥ th·ªÉ s·ª≠ d·ª•ng
        query_words_list = query_lower.split()  # List ƒë·ªÉ gi·ªØ th·ª© t·ª±

        # Thu th·∫≠p ·ª©ng vi√™n t·ª´ variant_map b·∫±ng n-gram (graph -> query)
        # QUAN TR·ªåNG: Normalize v√† lookup v·ªõi nhi·ªÅu variants ƒë·ªÉ cover m·ªçi tr∆∞·ªùng h·ª£p
        seen_entities = set()  # Tr√°nh tr√πng l·∫∑p
        for ng in ngrams:
            if len(ng) < 2:
                continue
            # Normalize n-gram (lo·∫°i b·ªè spaces th·ª´a)
            ng_normalized = " ".join(ng.split())
            # T·∫°o c√°c lookup keys: original, normalized, lowercase
            lookup_keys = [ng, ng_normalized, ng.lower(), ng_normalized.lower()]
            # Lo·∫°i b·ªè tr√πng l·∫∑p
            lookup_keys = list(dict.fromkeys(lookup_keys))
            
            for lookup_key in lookup_keys:
                if lookup_key in variant_map:
                    for ent in variant_map[lookup_key]:
                        if ent["label"] in ['Artist', 'Group']:
                            entity_name = ent["name"]
                            normalized = self._normalize_entity_name(entity_name).lower()
                            # Tr√°nh tr√πng l·∫∑p b·∫±ng normalized name
                            if normalized not in normalized_seen:
                                normalized_seen.add(normalized)
                                seen_entities.add(entity_name)
                                candidate_scores.append((entity_name, ent.get("score", 1.5)))
                                matched_from_graph.append({"name": entity_name, "score": ent.get("score", 1.5)})

        # Search for group/company/song/album/genre/occupation in query (case-insensitive) - ∆∞u ti√™n match exact/variant
        # QUAN TR·ªåNG: ∆Øu ti√™n match ƒë·∫ßy ƒë·ªß t√™n (n-gram) tr∆∞·ªõc single word
        def _match_list(nodes: List[str], score_val: float, label: str):
            # T·∫°o n-grams t·ª´ query (2-4 words) ƒë·ªÉ ∆∞u ti√™n match ƒë·∫ßy ƒë·ªß t√™n
            query_ngrams_for_match = []
            for n in [2, 3, 4]:
                for i in range(len(query_words_list) - n + 1):
                    ngram = " ".join(query_words_list[i:i+n])
                    query_ngrams_for_match.append(ngram)
                    query_ngrams_for_match.append(ngram.replace(" ", ""))
                    query_ngrams_for_match.append(ngram.replace(" ", "-"))
                    if '-' in ngram:
                        query_ngrams_for_match.append(ngram.replace("-", " "))
                        query_ngrams_for_match.append(ngram.replace("-", ""))
            query_ngrams_for_match = list(dict.fromkeys(query_ngrams_for_match))
            
            for node in nodes:
                normalized = self._normalize_entity_name(node).lower()
                # Check duplicate b·∫±ng normalized name
                if normalized in normalized_seen:
                    continue
                
                variants = _variants(node)
                hit = False
                base_name_word_count = len(normalized.split())
                
                # Method 1: Check n-gram matching (∆∞u ti√™n match ƒë·∫ßy ƒë·ªß t√™n tr∆∞·ªõc)
                # Ch·ªâ check n·∫øu base_name c√≥ nhi·ªÅu t·ª´ (‚â•2) ƒë·ªÉ ∆∞u ti√™n match ƒë·∫ßy ƒë·ªß
                if base_name_word_count >= 2:
                    for ngram in query_ngrams_for_match:
                        if len(ngram) < 3:
                            continue
                        for variant in variants:
                            if len(variant) < 3:
                                continue
                            # Exact match ho·∫∑c substring match
                            if variant == ngram or variant in ngram or ngram in variant:
                                base_score = score_val + 0.5  # Bonus cho n-gram match
                                if variant in token_set:
                                    base_score += 0.4  # ∆∞u ti√™n match ƒë√∫ng token
                                candidate_scores.append((node, base_score, label))
                                hit = True
                                break
                        if hit:
                            break
                
                # Method 2: Check single word matching (ch·ªâ cho single word names ho·∫∑c fallback)
                if not hit:
                    for variant in variants:
                        if len(variant) < 3:
                            continue
                        # Ch·ªâ match single word n·∫øu base_name ch·ªâ c√≥ 1 t·ª´
                        if base_name_word_count == 1:
                            if variant in query_lower:
                                base_score = score_val
                                if variant in token_set:
                                    base_score += 0.4  # ∆∞u ti√™n match ƒë√∫ng token
                                candidate_scores.append((node, base_score, label))
                                hit = True
                                break
                        # N·∫øu base_name c√≥ nhi·ªÅu t·ª´, ch·ªâ match n·∫øu t·∫•t c·∫£ c√°c t·ª´ ƒë·ªÅu c√≥ trong query
                        elif base_name_word_count > 1:
                            variant_words = set(variant.split())
                            query_words_set = set(query_words_list)
                            if variant_words.issubset(query_words_set):
                                base_score = score_val
                                if variant in token_set:
                                    base_score += 0.4
                                candidate_scores.append((node, base_score, label))
                                hit = True
                                break
                
                if hit:
                    entities.append(node)
                    normalized_seen.add(normalized)
                    # kh√¥ng break ƒë·ªÉ c√≥ th·ªÉ th√™m nhi·ªÅu th·ª±c th·ªÉ, nh∆∞ng tr√°nh tr√πng l·∫∑p
        
        _match_list(all_groups, 1.6, 'Group')
        _match_list(all_companies, 1.3, 'Company')
        _match_list(all_songs, 1.2, 'Song')
        _match_list(all_albums, 1.2, 'Album')
        _match_list(all_genres, 1.1, 'Genre')
        _match_list(all_occupations, 1.0, 'Occupation')
        
        # Search for artist names trong c√¢u h·ªèi (query -> graph) - b·∫Øt exact/variant, tr√°nh substring l·ªèng
        found_artists = []
        # query_words_list ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a ·ªü tr√™n (tr∆∞·ªõc function _match_list)
        query_text = query_lower  # Full query text ƒë·ªÉ check substring
        
        # Helper: normalize unicode ƒë·ªÉ match t·ªët h∆°n (Ros√© vs ros√©)
        import unicodedata
        def normalize_unicode(text: str) -> str:
            """Normalize unicode ƒë·ªÉ match t·ªët h∆°n (√© ‚Üí e, nh∆∞ng gi·ªØ nguy√™n n·∫øu c·∫ßn)"""
            # Gi·ªØ nguy√™n ƒë·ªÉ match ch√≠nh x√°c h∆°n v·ªõi t√™n c√≥ d·∫•u
            return text.lower()
        
        # QUAN TR·ªåNG: S·∫Øp x·∫øp artists theo ƒë·ªô d√†i t√™n (d√†i tr∆∞·ªõc) ƒë·ªÉ ∆∞u ti√™n match t√™n ƒë·∫ßy ƒë·ªß tr∆∞·ªõc
        # V√≠ d·ª•: "Yoo Jeong-yeon" s·∫Ω ƒë∆∞·ª£c duy·ªát tr∆∞·ªõc "Yoo" ƒë·ªÉ match ƒë√∫ng
        all_artists_sorted = sorted(all_artists, key=lambda x: len(self._normalize_entity_name(x)), reverse=True)
        
        for artist in all_artists_sorted:
            artist_lower = artist.lower()
            # Extract base name (kh√¥ng c√≥ ƒëu√¥i)
            base_name = self._normalize_entity_name(artist)
            base_name_lower = base_name.lower()
            
            # Check duplicate b·∫±ng normalized name TR∆Ø·ªöC khi match
            if base_name_lower in normalized_seen:
                continue
            
            # QUAN TR·ªåNG: ƒê·ªãnh nghƒ©a base_name_word_count TR∆Ø·ªöC khi d√πng
            base_name_word_count = len(base_name_lower.split())
            
            # T·∫°o variants ƒë·ªÉ match v·ªõi nhi·ªÅu format: "g-dragon", "g dragon", "gdragon", "go won", "go-won", "gowon"
            base_name_variants = [
                base_name_lower,  # Original
                base_name_lower.replace('-', ' '),  # "g-dragon" ‚Üí "g dragon", "go-won" ‚Üí "go won"
                base_name_lower.replace('-', ''),    # "g-dragon" ‚Üí "gdragon", "go-won" ‚Üí "gowon"
                base_name_lower.replace(' ', ''),    # "black pink" ‚Üí "blackpink", "go won" ‚Üí "gowon"
                base_name_lower.replace(' ', '-'),   # "go won" ‚Üí "go-won", "jang won young" ‚Üí "jang-won-young"
            ]
            # Lo·∫°i b·ªè tr√πng l·∫∑p
            base_name_variants = list(dict.fromkeys(base_name_variants))
            
            # QUAN TR·ªåNG: ∆Øu ti√™n match ƒë·∫ßy ƒë·ªß t√™n (n-gram) TR∆Ø·ªöC khi match single word
            # ƒê·∫£o th·ª© t·ª±: Method 2 (n-gram) tr∆∞·ªõc, Method 1 (single word) sau
            
            # Method 2: Check n-gram matching (2-3 words) ƒë·ªÉ b·∫Øt t√™n ph·ª©c t·∫°p nh∆∞ "Cho Seung-youn", "Yoo Jeong-yeon"
            # T·∫°o n-grams t·ª´ query (2-3 words) - t∆∞∆°ng t·ª± nh∆∞ multi_hop_reasoning.py
            expanded_words = []
            for word in query_words_list:
                expanded_words.append(word)  # Gi·ªØ nguy√™n: "jeong-yeon"
                if '-' in word:
                    # T√°ch token c√≥ dash th√†nh parts
                    parts = word.split('-')
                    expanded_words.extend(parts)  # "jeong-yeon" ‚Üí ["jeong-yeon", "jeong", "yeon"]
                    # Th√™m variant v·ªõi space: "jeong yeon"
                    expanded_words.append(" ".join(parts))
            
            query_ngrams = []
            for n in [2, 3, 4]:  # TƒÉng l√™n 4 ƒë·ªÉ b·∫Øt t√™n d√†i
                # T·∫°o n-grams t·ª´ c·∫£ query_words_list v√† expanded_words
                for word_list in [query_words_list, expanded_words]:
                    for i in range(len(word_list) - n + 1):
                        ngram = " ".join(word_list[i:i+n])
                        query_ngrams.append(ngram)  # Original: "yoo jeong-yeon", "yoo jeong yeon"
                        # Th√™m variant kh√¥ng c√≥ space: "yoojeong-yeon"
                        query_ngrams.append(ngram.replace(" ", ""))
                        # Th√™m variant v·ªõi dash: "yoo-jeong-yeon"
                        query_ngrams.append(ngram.replace(" ", "-"))
                        # QUAN TR·ªåNG: N·∫øu ngram c√≥ d·∫•u g·∫°ch ngang, t·∫°o th√™m variant v·ªõi space
                        if '-' in ngram:
                            query_ngrams.append(ngram.replace("-", " "))  # "yoo jeong-yeon" ‚Üí "yoo jeong yeon"
                            query_ngrams.append(ngram.replace("-", ""))   # "jeong-yeon" ‚Üí "jeongyeon"
            
            # Lo·∫°i b·ªè tr√πng l·∫∑p
            query_ngrams = list(dict.fromkeys(query_ngrams))
            
            # QUAN TR·ªåNG: Duy·ªát t·∫•t c·∫£ n-grams tr∆∞·ªõc ƒë·ªÉ match t√™n ƒë·∫ßy ƒë·ªß, sau ƒë√≥ m·ªõi check single word
            matched_in_ngram = False
            for ngram in query_ngrams:
                if len(ngram) < 3:
                    continue
                for variant in base_name_variants:
                    # Exact match (∆∞u ti√™n cao nh·∫•t)
                    if variant == ngram:
                        if base_name_lower not in normalized_seen:
                            found_artists.append(artist)
                            normalized_seen.add(base_name_lower)
                            # Track c√°c t·ª´ trong t√™n ƒë·∫ßy ƒë·ªß ƒë√£ match ƒë·ªÉ tr√°nh match single word sau
                            # QUAN TR·ªåNG: Normalize (thay dash b·∫±ng space) tr∆∞·ªõc khi split ƒë·ªÉ t√°ch ƒë√∫ng c√°c t·ª´
                            if base_name_word_count >= 2:
                                normalized_name = base_name_lower.replace('-', ' ').replace('  ', ' ').strip()
                                words_in_matched_full_names.update(normalized_name.split())
                            matched_in_ngram = True
                            break
                    # QUAN TR·ªåNG: X·ª≠ l√Ω t√™n c√≥ dash tr∆∞·ªõc khi check substring
                    # Normalize c·∫£ 2 v·ªÅ c√πng format ƒë·ªÉ so s√°nh ch√≠nh x√°c h∆°n
                    elif '-' in variant or '-' in ngram:
                        # Normalize c·∫£ 2 v·ªÅ c√πng format (space) ƒë·ªÉ so s√°nh
                        variant_normalized = variant.replace('-', ' ').replace('  ', ' ').strip()
                        ngram_normalized = ngram.replace('-', ' ').replace('  ', ' ').strip()
                        # Exact match sau khi normalize
                        if variant_normalized == ngram_normalized:
                            if base_name_lower not in normalized_seen:
                                found_artists.append(artist)
                                normalized_seen.add(base_name_lower)
                                # Track c√°c t·ª´ trong t√™n ƒë·∫ßy ƒë·ªß ƒë√£ match
                                if base_name_word_count >= 2:
                                    words_in_matched_full_names.update(variant_normalized.split())
                                matched_in_ngram = True
                                break
                        # So s√°nh parts: n·∫øu c√≥ √≠t nh·∫•t 2 parts gi·ªëng nhau ‚Üí match
                        variant_parts = set(variant_normalized.split())
                        ngram_parts = set(ngram_normalized.split())
                        if len(variant_parts.intersection(ngram_parts)) >= 2:
                            if base_name_lower not in normalized_seen:
                                found_artists.append(artist)
                                normalized_seen.add(base_name_lower)
                                # Track c√°c t·ª´ trong t√™n ƒë·∫ßy ƒë·ªß ƒë√£ match
                                if base_name_word_count >= 2:
                                    words_in_matched_full_names.update(variant_normalized.split())
                                matched_in_ngram = True
                                break
                    # Substring match (variant trong ngram ho·∫∑c ng∆∞·ª£c l·∫°i)
                    elif variant in ngram or ngram in variant:
                        # Verify: n·∫øu c·∫£ 2 ƒë·ªÅu c√≥ nhi·ªÅu t·ª´, ph·∫£i c√≥ √≠t nh·∫•t 2 t·ª´ tr√πng
                        variant_words = variant.split()
                        ngram_words = ngram.split()
                        if len(variant_words) >= 2 and len(ngram_words) >= 2:
                            # Check xem c√≥ √≠t nh·∫•t 2 t·ª´ tr√πng nhau kh√¥ng
                            variant_set = set(variant_words)
                            ngram_set = set(ngram_words)
                            if len(variant_set.intersection(ngram_set)) >= 2:
                                if base_name_lower not in normalized_seen:
                                    found_artists.append(artist)
                                    normalized_seen.add(base_name_lower)
                                    # Track c√°c t·ª´ trong t√™n ƒë·∫ßy ƒë·ªß ƒë√£ match
                                    # QUAN TR·ªåNG: Normalize (thay dash b·∫±ng space) tr∆∞·ªõc khi split ƒë·ªÉ t√°ch ƒë√∫ng c√°c t·ª´
                                    if base_name_word_count >= 2:
                                        normalized_name = base_name_lower.replace('-', ' ').replace('  ', ' ').strip()
                                        words_in_matched_full_names.update(normalized_name.split())
                                    matched_in_ngram = True
                                    break
                        else:
                            # N·∫øu m·ªôt trong 2 ch·ªâ c√≥ 1 t·ª´, ch·ªâ c·∫ßn exact match ho·∫∑c substring match
                            if base_name_lower not in normalized_seen:
                                found_artists.append(artist)
                                normalized_seen.add(base_name_lower)
                                # Track c√°c t·ª´ trong t√™n ƒë·∫ßy ƒë·ªß ƒë√£ match
                                # QUAN TR·ªåNG: Normalize (thay dash b·∫±ng space) tr∆∞·ªõc khi split ƒë·ªÉ t√°ch ƒë√∫ng c√°c t·ª´
                                if base_name_word_count >= 2:
                                    normalized_name = base_name_lower.replace('-', ' ').replace('  ', ' ').strip()
                                    words_in_matched_full_names.update(normalized_name.split())
                                matched_in_ngram = True
                                break
                if matched_in_ngram:
                    break
            # QUAN TR·ªåNG: N·∫øu ƒë√£ match trong n-gram, skip t·∫•t c·∫£ c√°c method kh√°c
            if matched_in_ngram:
                continue
            
            if base_name_lower in normalized_seen:
                continue
            
            # Method 1: Check n·∫øu base name ho·∫∑c variants l√† m·ªôt t·ª´ trong query (exact match)
            # QUAN TR·ªåNG: Ch·ªâ ch·∫°y n·∫øu base_name ch·ªâ c√≥ 1 t·ª´ (tr√°nh match "Yoo" v·ªõi "Yoo Jeong-yeon")
            # V√Ä t·ª´ ƒë√≥ ch∆∞a ƒë∆∞·ª£c match trong t√™n ƒë·∫ßy ƒë·ªß n√†o (tr√°nh match "Yoo" khi ƒë√£ match "Yoo Jeong-yeon")
            # V√≠ d·ª•: query "lisa c√≥ c√πng nh√≥m" ‚Üí word "lisa" match v·ªõi base_name "lisa"
            base_name_word_count = len(base_name_lower.split())
            if base_name_word_count == 1:
                # Check xem t·ª´ n√†y ƒë√£ ƒë∆∞·ª£c match trong t√™n ƒë·∫ßy ƒë·ªß n√†o ch∆∞a
                if base_name_lower in words_in_matched_full_names:
                    continue  # ƒê√£ ƒë∆∞·ª£c match trong t√™n ƒë·∫ßy ƒë·ªß, kh√¥ng match single word n·ªØa
                
                if any(variant in query_words_list for variant in base_name_variants):
                    found_artists.append(artist)
                    normalized_seen.add(base_name_lower)
                    continue
            
            if base_name_lower in normalized_seen:
                continue
            
            # Method 3: Check substring match (cho t√™n ph·ª©c t·∫°p nh∆∞ "Cho Seung-youn")
            # Ch·ªâ check n·∫øu base name c√≥ ƒë·ªô d√†i h·ª£p l√Ω (‚â•4 chars) ƒë·ªÉ tr√°nh match sai
            if len(base_name_lower) >= 4:
                for variant in base_name_variants:
                    if len(variant) >= 4 and variant in query_lower:
                        # Verify: ph·∫£i c√≥ √≠t nh·∫•t 2 t·ª´ trong variant xu·∫•t hi·ªán trong query
                        variant_words = variant.split()
                        if len(variant_words) >= 2:
                            matched_words = sum(1 for w in variant_words if len(w) >= 3 and w in query_lower)
                            if matched_words >= 2:
                                if base_name_lower not in normalized_seen:
                                    found_artists.append(artist)
                                    normalized_seen.add(base_name_lower)
                                    break
                        elif len(variant_words) == 1 and variant in query_lower:
                            if variant in query_words_list or any(variant in w for w in query_words_list if len(w) >= len(variant)):
                                if base_name_lower not in normalized_seen:
                                    found_artists.append(artist)
                                    normalized_seen.add(base_name_lower)
                                    break
                if base_name_lower in normalized_seen:
                    continue
            
            # Method 4: Check t·ª´ng word trong query v·ªõi base name v√† variants (strict, tr√°nh match nh·∫ßm)
            for word in query_words_list:
                if len(word) < 3:  # Skip short words
                    continue
                
                # QUAN TR·ªåNG: Check xem t·ª´ n√†y ƒë√£ ƒë∆∞·ª£c match trong t√™n ƒë·∫ßy ƒë·ªß n√†o ch∆∞a
                if word in words_in_matched_full_names:
                    continue  # ƒê√£ ƒë∆∞·ª£c match trong t√™n ƒë·∫ßy ƒë·ªß, kh√¥ng match single word n·ªØa
                
                # Exact match v·ªõi base name ho·∫∑c variants
                if word in base_name_variants or word == base_name_lower:
                    if base_name_lower not in normalized_seen:
                        found_artists.append(artist)
                        normalized_seen.add(base_name_lower)
                        break
                # X·ª≠ l√Ω t√™n c√≥ d·∫•u g·∫°ch ngang: "g-dragon" match v·ªõi "g" v√† "dragon"
                elif '-' in base_name_lower:
                    base_parts = base_name_lower.split('-')
                    if word in base_parts and len(word) >= 3:
                        other_parts = [p for p in base_parts if p != word and len(p) >= 2]
                        if any(p in query_words_list for p in other_parts) or any(p in query_lower for p in other_parts):
                            if base_name_lower not in normalized_seen:
                                found_artists.append(artist)
                                normalized_seen.add(base_name_lower)
                                break
                        # Normalize c·∫£ 2 v·ªÅ c√πng format (space) ƒë·ªÉ so s√°nh exact match
                        variant_normalized = variant.replace(' ', ' ').strip()
                        ngram_normalized = ngram.replace('-', ' ').replace('  ', ' ').strip()
                        if variant_normalized == ngram_normalized:
                            if base_name_lower not in normalized_seen:
                                found_artists.append(artist)
                                normalized_seen.add(base_name_lower)
                                # Track c√°c t·ª´ trong t√™n ƒë·∫ßy ƒë·ªß ƒë√£ match
                                if base_name_word_count >= 2:
                                    words_in_matched_full_names.update(variant_normalized.split())
                                break
                        # So s√°nh parts
                        variant_parts = set(variant.split(' '))
                        ngram_parts = set(ngram.split('-'))
                        if len(variant_parts.intersection(ngram_parts)) >= 2:
                            if base_name_lower not in normalized_seen:
                                found_artists.append(artist)
                                normalized_seen.add(base_name_lower)
                                # Track c√°c t·ª´ trong t√™n ƒë·∫ßy ƒë·ªß ƒë√£ match
                                if base_name_word_count >= 2:
                                    words_in_matched_full_names.update(variant_normalized.split())
                                break
                    elif '-' in variant and ' ' in ngram:
                        # Normalize c·∫£ 2 v·ªÅ c√πng format (space) ƒë·ªÉ so s√°nh exact match
                        variant_normalized = variant.replace('-', ' ').replace('  ', ' ').strip()
                        ngram_normalized = ngram.replace(' ', ' ').strip()
                        if variant_normalized == ngram_normalized:
                            if base_name_lower not in normalized_seen:
                                found_artists.append(artist)
                                normalized_seen.add(base_name_lower)
                                # Track c√°c t·ª´ trong t√™n ƒë·∫ßy ƒë·ªß ƒë√£ match
                                if base_name_word_count >= 2:
                                    words_in_matched_full_names.update(variant_normalized.split())
                                break
                        # So s√°nh parts
                        variant_parts = set(variant.split('-'))
                        ngram_parts = set(ngram.split(' '))
                        if len(variant_parts.intersection(ngram_parts)) >= 2:
                            if base_name_lower not in normalized_seen:
                                found_artists.append(artist)
                                normalized_seen.add(base_name_lower)
                                # Track c√°c t·ª´ trong t√™n ƒë·∫ßy ƒë·ªß ƒë√£ match
                                if base_name_word_count >= 2:
                                    words_in_matched_full_names.update(variant_normalized.split())
                                break
                if base_name_lower in normalized_seen:
                    break
            
            if base_name_lower in normalized_seen:
                continue
            
            # Method 1: Check n·∫øu base name ho·∫∑c variants l√† m·ªôt t·ª´ trong query (exact match)
            # QUAN TR·ªåNG: Ch·ªâ ch·∫°y n·∫øu base_name ch·ªâ c√≥ 1 t·ª´ (tr√°nh match "Yoo" v·ªõi "Yoo Jeong-yeon")
            # V√Ä t·ª´ ƒë√≥ ch∆∞a ƒë∆∞·ª£c match trong t√™n ƒë·∫ßy ƒë·ªß n√†o (tr√°nh match "Yoo" khi ƒë√£ match "Yoo Jeong-yeon")
            # V√≠ d·ª•: query "lisa c√≥ c√πng nh√≥m" ‚Üí word "lisa" match v·ªõi base_name "lisa"
            base_name_word_count = len(base_name_lower.split())
            if base_name_word_count == 1:
                # Check xem t·ª´ n√†y ƒë√£ ƒë∆∞·ª£c match trong t√™n ƒë·∫ßy ƒë·ªß n√†o ch∆∞a
                if base_name_lower in words_in_matched_full_names:
                    continue  # ƒê√£ ƒë∆∞·ª£c match trong t√™n ƒë·∫ßy ƒë·ªß, kh√¥ng match single word n·ªØa
                
                if any(variant in query_words_list for variant in base_name_variants):
                    found_artists.append(artist)
                    normalized_seen.add(base_name_lower)
                    continue
            
            if base_name_lower in normalized_seen:
                continue
            
            # Method 3: Check substring match (cho t√™n ph·ª©c t·∫°p nh∆∞ "Cho Seung-youn")
            # Ch·ªâ check n·∫øu base name c√≥ ƒë·ªô d√†i h·ª£p l√Ω (‚â•4 chars) ƒë·ªÉ tr√°nh match sai
            if len(base_name_lower) >= 4:
                for variant in base_name_variants:
                    if len(variant) >= 4 and variant in query_lower:
                        # Verify: ph·∫£i c√≥ √≠t nh·∫•t 2 t·ª´ trong variant xu·∫•t hi·ªán trong query
                        variant_words = variant.split()
                        if len(variant_words) >= 2:
                            # Check xem c√≥ √≠t nh·∫•t 2 t·ª´ trong variant xu·∫•t hi·ªán trong query kh√¥ng
                            matched_words = sum(1 for w in variant_words if len(w) >= 3 and w in query_lower)
                            if matched_words >= 2:
                                if base_name_lower not in normalized_seen:
                                    found_artists.append(artist)
                                    normalized_seen.add(base_name_lower)
                                    break
                        elif len(variant_words) == 1 and variant in query_lower:
                            # Single word variant: check exact match ho·∫∑c trong t·ª´ ƒë·∫ßy ƒë·ªß
                            if variant in query_words_list or any(variant in w for w in query_words_list if len(w) >= len(variant)):
                                if base_name_lower not in normalized_seen:
                                    found_artists.append(artist)
                                    normalized_seen.add(base_name_lower)
                                    break
                if base_name_lower in normalized_seen:
                    continue
            
            # Method 4: Check t·ª´ng word trong query v·ªõi base name v√† variants (strict, tr√°nh match nh·∫ßm)
            for word in query_words_list:
                if len(word) < 3:  # Skip short words
                    continue
                # Exact match v·ªõi base name ho·∫∑c variants
                if word in base_name_variants or word == base_name_lower:
                    if base_name_lower not in normalized_seen:
                        found_artists.append(artist)
                        normalized_seen.add(base_name_lower)
                        break
                # X·ª≠ l√Ω t√™n c√≥ d·∫•u g·∫°ch ngang: y√™u c·∫ßu c√≥ ƒë·ªß ‚â•2 ph·∫ßn trong query
                elif '-' in base_name_lower:
                    base_parts = base_name_lower.split('-')
                    if word in base_parts and len(word) >= 3:
                        other_parts = [p for p in base_parts if p != word and len(p) >= 2]
                        if any(p in query_lower.split() for p in other_parts) or any(p in query_lower for p in other_parts):
                            if base_name_lower not in normalized_seen:
                                found_artists.append(artist)
                                normalized_seen.add(base_name_lower)
                                break
        
        # Th√™m t·∫•t c·∫£ artists t√¨m ƒë∆∞·ª£c (kh√¥ng ch·ªâ 1)
        entities.extend(found_artists)
        candidate_scores.extend([(a, 1.4) for a in found_artists])
        # Th√™m c√°c match t·ª´ b∆∞·ªõc graph->query n-gram (∆∞u ti√™n score cao tr∆∞·ªõc)
        if matched_from_graph:
            matched_from_graph_sorted = sorted(matched_from_graph, key=lambda x: x['score'], reverse=True)
            for m in matched_from_graph_sorted:
                if m['name'] not in entities:
                    entities.append(m['name'])
        # N·∫øu ƒë√£ c√≥ ƒë·ªß 2 th·ª±c th·ªÉ t·ª´ candidate_scores ‚Üí ∆∞u ti√™n top 2 ƒë·ªÉ tr√°nh nhi·ªÖu
        if candidate_scores:
            ordered = []
            seen = set()
            for name, score in sorted(candidate_scores, key=lambda x: x[1], reverse=True):
                if name not in seen:
                    ordered.append(name)
                    seen.add(name)
            if len(ordered) >= 2:
                return ordered[:10]
        
        # N·∫øu ch∆∞a t√¨m ƒë·ªß, try fuzzy matching v·ªõi t·ª´ng word (nh∆∞ng ƒë√£ c√≥ match n-gram ∆∞u ti√™n)
        # QUAN TR·ªåNG: Ch·ªâ match v·ªõi artists/groups, kh√¥ng match v·ªõi albums/songs (tr√°nh sai)
        if len(entities) < 2:
            words = query_lower.split()
            # Filter out common Vietnamese words
            stop_words = {'c√≥', 'v√†', 'c√πng', 'nh√≥m', 'nh·∫°c', 'kh√¥ng', 'l√†', 'thu·ªôc', 'c·ªßa', 'v·ªõi', 'hay', 'ho·∫∑c'}
            words = [w for w in words if w not in stop_words and len(w) >= 3]
            
            for word in words:
                # Try exact match (case-insensitive) v·ªõi artists only (tr√°nh match albums/songs)
                for artist in all_artists:
                    base_name = self._normalize_entity_name(artist)
                    base_name_lower = base_name.lower()
                    
                    # Check duplicate b·∫±ng normalized name
                    if base_name_lower in normalized_seen:
                        continue
                    
                    # Exact match v·ªõi base name ho·∫∑c variants (x·ª≠ l√Ω d·∫•u g·∫°ch ngang)
                    base_name_variants = [
                        base_name_lower,
                        base_name_lower.replace('-', ' '),
                        base_name_lower.replace('-', ''),
                        base_name_lower.replace(' ', ''),
                    ]
                    if word in base_name_variants or base_name_lower == word:
                        entities.append(artist)
                        candidate_scores.append((artist, 1.0))
                        normalized_seen.add(base_name_lower)
                        break
                    # X·ª≠ l√Ω t√™n c√≥ d·∫•u g·∫°ch ngang: "g-dragon" match v·ªõi "g" v√† "dragon"
                    if '-' in base_name_lower:
                        base_parts = base_name_lower.split('-')
                        if word in base_parts and len(word) >= 3:
                            # Check xem c√≥ part kh√°c c≈©ng trong query kh√¥ng
                            other_parts = [p for p in base_parts if p != word]
                            if any(p in query_lower for p in other_parts):
                                entities.append(artist)
                                candidate_scores.append((artist, 1.0))
                                normalized_seen.add(base_name_lower)
                                break
                
                # Try exact match v·ªõi groups (c≈©ng x·ª≠ l√Ω variants)
                for group in all_groups:
                    group_lower = group.lower()
                    group_normalized = self._normalize_entity_name(group).lower()
                    
                    # Check duplicate b·∫±ng normalized name
                    if group_normalized in normalized_seen:
                        continue
                    
                    group_variants = [
                        group_lower,
                        group_lower.replace('-', ' '),
                        group_lower.replace('-', ''),
                        group_lower.replace(' ', ''),
                    ]
                    if word in group_variants or group_lower == word:
                        entities.append(group)
                        candidate_scores.append((group, 1.0))
                        normalized_seen.add(group_normalized)
                        break
        
        # ∆Øu ti√™n c√°c entity c√≥ ƒëi·ªÉm cao nh·∫•t (t·ª´ n-gram/alias/exact)
        if candidate_scores:
            # ∆Øu ti√™n theo label: Group > Artist > Company > Song > Album > Genre > Occupation
            label_priority = {'Group': 7, 'Artist': 6, 'Company': 5, 'Song': 4, 'Album': 3, 'Genre': 2, 'Occupation': 1}
            ordered = []
            seen = set()
            for name, score, label in sorted(
                candidate_scores,
                key=lambda x: (label_priority.get(x[2], 0), x[1], len(x[0])),
                reverse=True
            ):
                if name not in seen:
                    ordered.append(name)
                    seen.add(name)
            return ordered[:10]
        
        # Return fallback
        return entities[:10]  # Return max 10 entities ƒë·ªÉ ƒë·∫£m b·∫£o t√¨m ƒë·ªß
    
    def _normalize_entity_name(self, entity_name: str) -> str:
        """
        Normalize entity name b·∫±ng c√°ch remove suffixes trong parentheses.
        
        V√≠ d·ª•:
        - "Lisa (ca sƒ©)" ‚Üí "Lisa"
        - "BLACKPINK (nh√≥m nh·∫°c)" ‚Üí "BLACKPINK"
        - "BTS (rapper)" ‚Üí "BTS"
        
        Args:
            entity_name: Entity name c√≥ th·ªÉ c√≥ ƒëu√¥i
            
        Returns:
            Base name (kh√¥ng c√≥ ƒëu√¥i)
        """
        import re
        # Remove suffixes trong parentheses: (ca sƒ©), (nh√≥m nh·∫°c), (rapper), etc.
        normalized = re.sub(r'\s*\([^)]+\)\s*$', '', entity_name)
        return normalized.strip()
    
    def _generate_variants(self, name: str) -> List[str]:
        """Sinh c√°c bi·∫øn th·ªÉ ƒë∆°n gi·∫£n c·ªßa m·ªôt t√™n entity."""
        base = self._normalize_entity_name(name).lower()
        variants = {
            base,  # Original: "jang won-young"
            base.replace('-', ' '),  # "jang won-young" ‚Üí "jang won young", "go-won" ‚Üí "go won"
            base.replace('-', ''),   # "jang won-young" ‚Üí "jangwonyoung", "go-won" ‚Üí "gowon"
            base.replace(' ', ''),   # "jang won-young" ‚Üí "jangwon-young", "go won" ‚Üí "gowon"
            base.replace(' ', '-'),  # "jang won-young" ‚Üí "jang-won-young", "go won" ‚Üí "go-won"
        }
        
        # QUAN TR·ªåNG: X·ª≠ l√Ω t√™n c√≥ C·∫¢ dash V√Ä space nh∆∞ "jang won-young"
        # T√°ch th√†nh c√°c parts (c·∫£ dash v√† space ƒë·ªÅu l√† separator)
        # "jang won-young" ‚Üí ["jang", "won", "young"]
        all_parts = []
        # T√°ch theo dash tr∆∞·ªõc
        for part in base.split('-'):
            # M·ªói part c√≥ th·ªÉ c√≥ space, t√°ch ti·∫øp
            all_parts.extend(part.split())
        # Lo·∫°i b·ªè empty parts
        all_parts = [p for p in all_parts if p]
        
        if len(all_parts) >= 2:
            # T·∫°o variants v·ªõi t·∫•t c·∫£ c√°c combinations
            # "jang won-young" ‚Üí ["jang", "won", "young"]
            variants.add(" ".join(all_parts))  # "jang won young"
            variants.add("-".join(all_parts))  # "jang-won-young"
            variants.add("".join(all_parts))   # "jangwonyoung"
            
            # T·∫°o c√°c combinations: m·ªôt s·ªë parts c√≥ dash, m·ªôt s·ªë c√≥ space
            # "jang won-young" ‚Üí "jang won-young", "jang-won young", etc.
            if len(all_parts) == 3:
                # 3 parts: c√≥ th·ªÉ c√≥ 2 dash positions
                variants.add(f"{all_parts[0]} {all_parts[1]}-{all_parts[2]}")  # "jang won-young"
                variants.add(f"{all_parts[0]}-{all_parts[1]} {all_parts[2]}")  # "jang-won young"
                variants.add(f"{all_parts[0]}-{all_parts[1]}-{all_parts[2]}")  # "jang-won-young"
                variants.add(f"{all_parts[0]} {all_parts[1]} {all_parts[2]}")  # "jang won young"
        
        # N·∫øu c√≥ g·∫°ch, th√™m b·∫£n t√°ch g·∫°ch v·ªõi nhi·ªÅu space v√† combinations
        if '-' in base:
            parts = base.split('-')
            # "yoo-jeong-yeon" ‚Üí ["yoo", "jeong", "yeon"]
            variants.add(" ".join(parts))  # "yoo jeong yeon"
            variants.add("".join(parts))   # "yoojeongyeon"
            # Th√™m c√°c combinations: "yoo jeong-yeon", "yoo-jeong yeon", etc.
            for i in range(len(parts)):
                # T·∫°o variant v·ªõi m·ªôt s·ªë ph·∫ßn c√≥ g·∫°ch, m·ªôt s·ªë c√≥ space
                if i < len(parts) - 1:
                    variant_parts = parts.copy()
                    variant_parts[i] = variant_parts[i] + "-" + variant_parts[i+1]
                    variant_parts.pop(i+1)
                    variants.add(" ".join(variant_parts))
        # N·∫øu c√≥ space, th√™m variant v·ªõi g·∫°ch v√† c√°c combinations
        if ' ' in base:
            parts = base.split(' ')
            # "go won" ‚Üí ["go", "won"]
            variants.add("-".join(parts))  # "go-won"
            variants.add("".join(parts))   # "gowon"
            # V·ªõi t√™n d√†i h∆°n: "jang won young" ‚Üí "jang-won-young", "jangwonyoung"
            if len(parts) > 2:
                variants.add("-".join(parts))  # "jang-won-young"
                variants.add("".join(parts))   # "jangwonyoung"
        return list(variants)
    
    def _ensure_entity_variant_map(self):
        """
        Build m·ªôt map variant -> [entity] ƒë·ªÉ tra c·ª©u nhanh (graph -> query).
        Ch·ªâ gi·ªØ label Artist/Group; th√™m alias th·ªß c√¥ng cho m·ªôt s·ªë case d·ªÖ nh·∫ßm.
        """
        if hasattr(self, "_entity_variant_map") and self._entity_variant_map is not None:
            return
        
        alias_map = {
            # LOONA / LOOŒ†Œî
            "loona": ["loona", "looœÄŒ¥", "loonŒ±", "loona-loona"],
            "vi vi": ["vivi", "vi-vi", "vi vi"],
            "vivi": ["vivi", "vi-vi", "vi vi"],
            "go won": ["go won", "gowon", "go-won"],
            "gowon": ["go won", "gowon", "go-won"],
            # BLACKPINK
            "blackpink": ["blackpink", "black pink", "black-pink", "bp"],
        }
        
        variant_map: Dict[str, List[Dict[str, Any]]] = {}
        for node, data in self.kg.graph.nodes(data=True):
            label = data.get('label')
            if label not in ['Artist', 'Group']:
                continue
            
            base_name = self._normalize_entity_name(node)
            base_variants = self._generate_variants(node)
            
            # Th√™m alias th·ªß c√¥ng n·∫øu kh·ªõp base name
            extra_alias = []
            base_lower = base_name.lower()
            if base_lower in alias_map:
                extra_alias = alias_map[base_lower]
            
            # QUAN TR·ªåNG: T·∫°o th√™m variants t·ª´ base_name (kh√¥ng ch·ªâ t·ª´ node)
            # ƒê·∫£m b·∫£o cover ƒë∆∞·ª£c c·∫£ base_name variants
            base_name_variants = self._generate_variants(base_name)
            
            all_variants = set(base_variants + base_name_variants + extra_alias)
            
            # Th√™m c·∫£ full node name (c√≥ th·ªÉ c√≥ ƒëu√¥i) v√† base name
            all_variants.add(node.lower())
            all_variants.add(base_name.lower())
            
            for v in all_variants:
                if len(v) < 2:
                    continue
                # Normalize: lo·∫°i b·ªè spaces th·ª´a
                v = " ".join(v.split())
                if len(v) < 2:
                    continue
                    
                if v not in variant_map:
                    variant_map[v] = []
                # Score: alias cao h∆°n m·ªôt ch√∫t, base name variants cao h∆°n node variants
                if v in extra_alias:
                    score = 2.0
                elif v in base_name_variants:
                    score = 1.6
                else:
                    score = 1.5
                variant_map[v].append({
                    "name": node,
                    "label": label,
                    "score": score
                })
        
        self._entity_variant_map = variant_map
        
    # =========== Specialized Query Methods ===========
    
    def get_group_members(self, group_name: str) -> Dict:
        """Get members of a K-pop group."""
        result = self.reasoner.get_group_members(group_name)
        return {
            "group": group_name,
            "members": result.answer_entities,
            "member_count": len(result.answer_entities),
            "answer": result.answer_text
        }
        
    def get_group_company(self, group_name: str) -> Dict:
        """Get company managing a group."""
        result = self.reasoner.get_company_of_group(group_name)
        return {
            "group": group_name,
            "company": result.answer_entities[0] if result.answer_entities else None,
            "answer": result.answer_text
        }
        
    def check_same_company(self, entity1: str, entity2: str) -> Dict:
        """Check if two entities are under the same company."""
        result = self.reasoner.check_same_company(entity1, entity2)
        return {
            "entity1": entity1,
            "entity2": entity2,
            "same_company": len(result.answer_entities) > 0,
            "common_company": result.answer_entities[0] if result.answer_entities else None,
            "answer": result.answer_text
        }
        
    def get_labelmates(self, entity: str) -> Dict:
        """Get all groups/artists under same company."""
        result = self.reasoner.get_labelmates(entity)
        return {
            "entity": entity,
            "labelmates": result.answer_entities,
            "count": len(result.answer_entities),
            "answer": result.answer_text
        }
        
    def find_path(self, source: str, target: str) -> Dict:
        """Find relationship path between two entities."""
        path = self.kg.find_path(source, target)
        
        if path:
            details = self.kg.get_path_details(path)
            path_str = " ‚Üí ".join([
                f"{d['entity']}({d['type']})" for d in details
            ])
            return {
                "source": source,
                "target": target,
                "path_found": True,
                "path": path,
                "hops": len(path) - 1,
                "description": path_str
            }
        else:
            return {
                "source": source,
                "target": target,
                "path_found": False,
                "path": [],
                "hops": -1,
                "description": f"Kh√¥ng t√¨m th·∫•y ƒë∆∞·ªùng ƒëi t·ª´ {source} ƒë·∫øn {target}"
            }
            
    def get_statistics(self) -> Dict:
        """Get chatbot and knowledge graph statistics."""
        kg_stats = self.kg.get_statistics()
        return {
            "knowledge_graph": kg_stats,
            "active_sessions": len(self.sessions),
            "llm_available": self.llm is not None,
            "embeddings_available": self.rag.embedder is not None
        }


def main():
    """Test the chatbot."""
    print("="*60)
    print("üé§ K-pop Knowledge Graph Chatbot Demo")
    print("="*60)
    
    # Initialize chatbot
    chatbot = KpopChatbot(
        llm_model="qwen2-0.5b",
        verbose=True
    )
    
    # Print statistics
    print("\nüìä Statistics:")
    stats = chatbot.get_statistics()
    print(f"  Nodes: {stats['knowledge_graph']['total_nodes']}")
    print(f"  Edges: {stats['knowledge_graph']['total_edges']}")
    print(f"  LLM: {'‚úÖ' if stats['llm_available'] else '‚ùå'}")
    print(f"  Embeddings: {'‚úÖ' if stats['embeddings_available'] else '‚ùå'}")
    
    # Test queries
    test_queries = [
        "BTS c√≥ bao nhi√™u th√†nh vi√™n?",
        "C√¥ng ty n√†o qu·∫£n l√Ω BLACKPINK?",
        "BTS v√† SEVENTEEN c√≥ c√πng c√¥ng ty kh√¥ng?",
    ]
    
    print("\n" + "="*60)
    print("üß™ Running Test Queries")
    print("="*60)
    
    session_id = chatbot.create_session()
    
    for query in test_queries:
        print(f"\n‚ùì Query: {query}")
        result = chatbot.chat(query, session_id, return_details=True)
        print(f"ü§ñ Response: {result['response']}")
        print(f"üìç Entities: {result['entities_found']}, Hops: {result['reasoning_hops']}")
        
    # Test specialized queries
    print("\n" + "="*60)
    print("üß™ Specialized Queries")
    print("="*60)
    
    print("\nüë• BTS Members:")
    result = chatbot.get_group_members("BTS")
    print(f"  {result['answer']}")
    
    print("\nüè¢ BTS Company:")
    result = chatbot.get_group_company("BTS")
    print(f"  {result['answer']}")
    
    print("\nüîç Same Company Check (BTS vs SEVENTEEN):")
    result = chatbot.check_same_company("BTS", "SEVENTEEN")
    print(f"  {result['answer']}")


if __name__ == "__main__":
    main()

