"""
GraphRAG Module for K-pop Knowledge Graph

This module implements Graph-based Retrieval Augmented Generation (GraphRAG)
for the K-pop knowledge graph. It combines graph traversal with semantic
similarity search to retrieve relevant context for answering questions.

Key Features:
- Entity extraction from queries
- Graph-based context retrieval
- Semantic similarity matching
- Multi-hop relationship traversal
- Context ranking and filtering
"""

import json
import numpy as np
from typing import Dict, List, Tuple, Optional, Set, Any
from collections import defaultdict
import os
import re

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("‚ö†Ô∏è sentence-transformers not installed. Using keyword-based retrieval.")

try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    print("‚ö†Ô∏è faiss not installed. Using numpy-based similarity search.")

from .knowledge_graph import KpopKnowledgeGraph


class GraphRAG:
    """
    Graph-based Retrieval Augmented Generation for K-pop Knowledge Graph.
    
    ‚úÖ GraphRAG = Retrieval layer tr√™n ƒë·ªì th·ªã tri th·ª©c (Knowledge Graph)
    
    üéØ NHI·ªÜM V·ª§ DUY NH·∫§T: T√åM t·ª´ ƒë·ªì th·ªã nh·ªØng th√¥ng tin li√™n quan nh·∫•t t·ªõi c√¢u h·ªèi
    
    GraphRAG L√ÄM:
    ‚úÖ 1. T√¨m th·ª±c th·ªÉ ch√≠nh trong c√¢u h·ªèi (Entity extraction)
    ‚úÖ 2. T√¨m neighbors / h√†ng x√≥m g·∫ßn nh·∫•t (Graph traversal)
    ‚úÖ 3. T√¨m ƒë∆∞·ªùng ƒëi (paths) gi·ªØa c√°c entity (Path finding)
    ‚úÖ 4. Chuy·ªÉn th√†nh "context" cho LLM (Format triples/text)
    
    GraphRAG KH√îNG L√ÄM:
    ‚ùå Kh√¥ng di·ªÖn gi·∫£i
    ‚ùå Kh√¥ng t√≥m t·∫Øt
    ‚ùå Kh√¥ng b·ªãa th√¥ng tin
    ‚ùå Kh√¥ng suy lu·∫≠n multi-hop (do MultiHopReasoner l√†m)
    ‚ùå Kh√¥ng t·∫°o c√¢u tr·∫£ l·ªùi (do LLM l√†m)
    
    üìå GraphRAG ch·ªâ l√† "Retrieval layer" c·ªßa chatbot.
    Reasoning v√† answer generation do c√°c component kh√°c th·ª±c hi·ªán.
    
    Combines:
    1. Entity extraction from natural language queries
    2. Graph traversal for structured context
    3. Semantic embedding for similarity matching
    4. Multi-hop path finding
    """
    
    def __init__(
        self,
        knowledge_graph: Optional[KpopKnowledgeGraph] = None,
        embedding_model: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        use_cache: bool = True,
        llm_for_understanding: Optional[Any] = None
    ):
        """
        Initialize GraphRAG.
        
        Args:
            knowledge_graph: Pre-built knowledge graph (will create if None)
            embedding_model: Sentence transformer model for embeddings
            use_cache: Whether to cache embeddings
            llm_for_understanding: Optional LLM for understanding queries (entity extraction + intent detection)
        """
        self.kg = knowledge_graph or KpopKnowledgeGraph()
        self.embedding_model_name = embedding_model
        self.use_cache = use_cache
        self.llm_for_understanding = llm_for_understanding  # LLM ƒë·ªÉ hi·ªÉu c√¢u h·ªèi
        
        # Initialize embedding model
        self.embedder = None
        self.entity_embeddings = None
        self.entity_ids = []
        self.faiss_index = None
        
        if SENTENCE_TRANSFORMERS_AVAILABLE:
            self._init_embeddings()
        else:
            print("‚ö†Ô∏è Running in keyword-only mode (no semantic embeddings)")
            
        # Entity patterns for extraction
        self._init_entity_patterns()
    
    def _normalize_entity_name(self, entity_name: str) -> str:
        """
        Normalize entity name b·∫±ng c√°ch remove suffixes trong parentheses.
        
        V√≠ d·ª•:
        - "Lisa (ca sƒ©)" ‚Üí "Lisa"
        - "BLACKPINK (nh√≥m nh·∫°c)" ‚Üí "BLACKPINK"
        - "BTS (rapper)" ‚Üí "BTS"
        
        Args:
            entity_name: Entity name c√≥ th·ªÉ c√≥ ƒëu√¥i
            
        Returns:
            Base name (kh√¥ng c√≥ ƒëu√¥i)
        """
        # Remove suffixes trong parentheses: (ca sƒ©), (nh√≥m nh·∫°c), (rapper), etc.
        # Pattern: (.*) ·ªü cu·ªëi string
        import re
        # Match pattern: space + (anything) ·ªü cu·ªëi
        normalized = re.sub(r'\s*\([^)]+\)\s*$', '', entity_name)
        return normalized.strip()
        
    def _init_embeddings(self):
        """Initialize sentence transformer and build entity embeddings."""
        print(f"üîÑ Loading embedding model: {self.embedding_model_name}")
        self.embedder = SentenceTransformer(self.embedding_model_name)
        
        # Check for cached embeddings
        cache_path = "data/entity_embeddings.npz"
        if self.use_cache and os.path.exists(cache_path):
            print("üìÇ Loading cached embeddings...")
            data = np.load(cache_path, allow_pickle=True)
            self.entity_embeddings = data['embeddings']
            self.entity_ids = data['entity_ids'].tolist()
        else:
            print("üîÑ Building entity embeddings...")
            self._build_entity_embeddings()
            if self.use_cache:
                np.savez(
                    cache_path,
                    embeddings=self.entity_embeddings,
                    entity_ids=self.entity_ids
                )
                
        # Build FAISS index
        self._build_faiss_index()
        
    def _build_entity_embeddings(self):
        """Build embeddings for all entities."""
        texts = []
        self.entity_ids = []
        
        for node_id, data in self.kg.graph.nodes(data=True):
            # Create text representation of entity
            text = self._entity_to_text(node_id, data)
            texts.append(text)
            self.entity_ids.append(node_id)
            
        # Batch encode
        self.entity_embeddings = self.embedder.encode(
            texts,
            show_progress_bar=True,
            batch_size=64
        )
        
        print(f"‚úÖ Built embeddings for {len(self.entity_ids)} entities")
        
    def _entity_to_text(self, entity_id: str, data: Dict) -> str:
        """Convert entity to text representation for embedding."""
        parts = [entity_id]
        
        # Add type
        if 'label' in data:
            parts.append(f"lo·∫°i: {data['label']}")
            
        # Add title if different
        title = data.get('title', '')
        if title and title != entity_id:
            parts.append(title)
            
        # Add infobox info
        infobox = data.get('infobox', {})
        if infobox:
            # Key fields
            for key in ['Th·ªÉ lo·∫°i', 'NƒÉm ho·∫°t ƒë·ªông', 'H√£ng ƒëƒ©a', 'Th√†nh vi√™n']:
                if key in infobox and infobox[key]:
                    parts.append(f"{key}: {infobox[key]}")
                    
        return " | ".join(parts)
        
    def _build_faiss_index(self):
        """Build FAISS index for fast similarity search."""
        if not FAISS_AVAILABLE or self.entity_embeddings is None:
            return
            
        dim = self.entity_embeddings.shape[1]
        self.faiss_index = faiss.IndexFlatIP(dim)  # Inner product for cosine similarity
        
        # Normalize for cosine similarity
        normalized = self.entity_embeddings / np.linalg.norm(
            self.entity_embeddings, axis=1, keepdims=True
        )
        self.faiss_index.add(normalized.astype('float32'))
        
        print(f"‚úÖ Built FAISS index with {self.faiss_index.ntotal} vectors")
        
    def _init_entity_patterns(self):
        """Initialize regex patterns for entity extraction."""
        # Common K-pop group and artist name patterns
        self.entity_patterns = {
            'group': [
                r'\b(BTS|EXO|BLACKPINK|TWICE|NCT|SEVENTEEN|Stray Kids|ITZY|aespa|NewJeans)\b',
                r'\b(Red Velvet|Girls\' Generation|Super Junior|Big Bang|2NE1|f\(x\))\b',
                r'\b(ENHYPEN|TXT|LE SSERAFIM|IVE|NMIXX|(G)I-dle|Kep1er)\b',
                r'\b(GOT7|Monsta X|iKON|WINNER|MAMAMOO|GFRIEND|LOONA)\b',
                r'\b(SHINee|2PM|B.A.P|Block B|VIXX|BTOB|BEAST|Highlight)\b',
            ],
            'company': [
                r'\b(SM Entertainment|JYP Entertainment|YG Entertainment|HYBE|Big Hit)\b',
                r'\b(Cube Entertainment|Starship Entertainment|Pledis Entertainment)\b',
                r'\b(FNC Entertainment|Woollim Entertainment|RBW Entertainment)\b',
            ]
        }
        
    def extract_entities(self, query: str) -> List[Dict]:
        """
        Extract potential entities from a natural language query.
        
        S·ª≠ d·ª•ng 2 ph∆∞∆°ng ph√°p:
        1. Pattern matching + Semantic search (nhanh, ch√≠nh x√°c cho entity names)
        2. LLM understanding (n·∫øu c√≥) - hi·ªÉu ng·ªØ c·∫£nh t·ªët h∆°n, x·ª≠ l√Ω c√¢u h·ªèi ph·ª©c t·∫°p
        
        Args:
            query: User's question
            
        Returns:
            List of extracted entities with types
        """
        entities = []
        query_lower = query.lower()
        
        # ============================================
        # PH∆Ø∆†NG PH√ÅP 1: Rule + KG + Semantic Search (∆ØU TI√äN - Fast, An to√†n)
        # ============================================
        # ‚úÖ CHI·∫æN L∆Ø·ª¢C: ∆Øu ti√™n rule-based v√† KG lookup tr∆∞·ªõc
        # - Pattern matching: Regex patterns cho groups, companies
        # - KG lookup: T√¨m entities trong Knowledge Graph (quoted, capitalized, context patterns)
        # - Semantic search: FAISS + embeddings (n·∫øu available)
        # T·∫•t c·∫£ ƒë·ªÅu c√≥ threshold/validation ƒë·ªÉ ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng
        
        # 1a. Pattern-based extraction
        for entity_type, patterns in self.entity_patterns.items():
            for pattern in patterns:
                matches = re.findall(pattern, query, re.IGNORECASE)
                for match in matches:
                    entities.append({
                        'text': match,
                        'type': entity_type,
                        'method': 'pattern'
                    })
                    
        # 1b. Knowledge graph lookup - T√¨m t·∫•t c·∫£ entities c√≥ th·ªÉ c√≥ trong query
        # Extract potential entity names t·ª´ nhi·ªÅu ng·ªØ c·∫£nh kh√°c nhau:
        # - Quoted strings: "BTS", 'BLACKPINK'
        # - Capitalized words: BTS, BLACKPINK, Lisa, Jennie
        # - Words after keywords: "nh√≥m BTS", "ca sƒ© Lisa", "c√¥ng ty YG"
        # - Words before keywords: "BTS l√† nh√≥m", "Lisa thu·ªôc nh√≥m"
        
        # Method 1: Quoted strings
        quoted_names = re.findall(r'"([^"]+)"|\'([^\']+)\'', query)
        for match in quoted_names:
            name = match[0] or match[1]
            if name:
                results = self.kg.search_entities(name, limit=1)
                if results and results[0]['score'] > 0.7:
                    entities.append({
                        'text': results[0]['id'],
                        'type': results[0]['type'],
                        'method': 'kg_lookup_quoted',
                        'score': results[0]['score']
                    })
        
        # Method 2: Capitalized words (t√™n ri√™ng)
        capitalized_words = re.findall(r'\b([A-Z][a-zA-Z]+(?:\s+[A-Z][a-zA-Z]+)*)\b', query)
        for name in capitalized_words:
            # Skip common words
            if name.lower() not in ['c√≥', 'kh√¥ng', 'v√†', 'v·ªõi', 'c·ªßa', 'l√†', 'thu·ªôc', 'trong', 't·ª´']:
                results = self.kg.search_entities(name, limit=1)
                if results and results[0]['score'] > 0.7:
                    entities.append({
                        'text': results[0]['id'],
                        'type': results[0]['type'],
                        'method': 'kg_lookup_capitalized',
                        'score': results[0]['score']
                    })
        
        # Method 3: T√¨m entities sau keywords (ng·ªØ c·∫£nh: "nh√≥m X", "ca sƒ© Y", "c√¥ng ty Z")
        context_patterns = [
            (r'(nh√≥m|group|ban nh·∫°c)\s+([A-Z][a-zA-Z]+(?:\s+[A-Z][a-zA-Z]+)*)', 'Group'),
            (r'(ca sƒ©|ngh·ªá sƒ©|artist|singer|idol)\s+([A-Z][a-zA-Z]+(?:\s+[A-Z][a-zA-Z]+)*)', 'Artist'),
            (r'(c√¥ng ty|company|label|h√£ng ƒëƒ©a)\s+([A-Z][a-zA-Z]+(?:\s+[A-Z][a-zA-Z]+)*)', 'Company'),
            (r'(b√†i h√°t|song|ca kh√∫c|track)\s+([A-Z][a-zA-Z]+(?:\s+[A-Z][a-zA-Z]+)*)', 'Song'),
        ]
        for pattern, entity_type in context_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            for match in matches:
                name = match[1] if isinstance(match, tuple) else match
                if name:
                    results = self.kg.search_entities(name, limit=1)
                    if results and results[0]['score'] > 0.6:
                        entities.append({
                            'text': results[0]['id'],
                            'type': results[0]['type'],
                            'method': f'kg_lookup_context_{entity_type.lower()}',
                            'score': results[0]['score']
                        })
        
        # Method 4: T√¨m entities tr∆∞·ªõc keywords (ng·ªØ c·∫£nh: "X l√† nh√≥m", "Y thu·ªôc c√¥ng ty")
        before_keyword_patterns = [
            (r'([A-Z][a-zA-Z]+(?:\s+[A-Z][a-zA-Z]+)*)\s+(l√†|thu·ªôc|belongs to|is)\s+(nh√≥m|group|ban nh·∫°c)', 'Group'),
            (r'([A-Z][a-zA-Z]+(?:\s+[A-Z][a-zA-Z]+)*)\s+(l√†|thu·ªôc|belongs to|is)\s+(ca sƒ©|ngh·ªá sƒ©|artist)', 'Artist'),
            (r'([A-Z][a-zA-Z]+(?:\s+[A-Z][a-zA-Z]+)*)\s+(thu·ªôc|belongs to|is)\s+(c√¥ng ty|company)', 'Company'),
        ]
        for pattern, entity_type in before_keyword_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            for match in matches:
                name = match[0] if isinstance(match, tuple) else match
                if name:
                    results = self.kg.search_entities(name, limit=1)
                    if results and results[0]['score'] > 0.6:
                        entities.append({
                            'text': results[0]['id'],
                            'type': results[0]['type'],
                            'method': f'kg_lookup_before_keyword_{entity_type.lower()}',
                            'score': results[0]['score']
                        })
        
        # Method 5: T√¨m t·∫•t c·∫£ nodes trong KG v√† check xem c√≥ trong query kh√¥ng (fuzzy match)
        # QUAN TR·ªåNG: X·ª≠ l√Ω lowercase names nh∆∞ "jennie", "jisoo", "lisa"
        # L·∫•y t·∫•t c·∫£ entity names t·ª´ KG (cached ƒë·ªÉ tr√°nh ch·∫≠m)
        if not hasattr(self, '_all_entity_names'):
            self._all_entity_names = list(self.kg.graph.nodes())
        
        # Cache lowercase mapping ƒë·ªÉ t√¨m nhanh h∆°n
        # QUAN TR·ªåNG: X·ª≠ l√Ω node c√≥ ƒëu√¥i nh∆∞ "Lisa (ca sƒ©)", "BLACKPINK (nh√≥m nh·∫°c)"
        if not hasattr(self, '_entity_lowercase_map'):
            self._entity_lowercase_map = {}
            self._entity_base_name_map = {}  # Map base name (kh√¥ng c√≥ ƒëu√¥i) ‚Üí full name
            
            for name in self._all_entity_names:
                # Map full name lowercase
                self._entity_lowercase_map[name.lower()] = name
                
                # Extract base name (remove suffixes nh∆∞ "(ca sƒ©)", "(nh√≥m nh·∫°c)")
                base_name = self._normalize_entity_name(name)
                if base_name != name:
                    # Map base name ‚Üí full name
                    if base_name.lower() not in self._entity_base_name_map:
                        self._entity_base_name_map[base_name.lower()] = []
                    self._entity_base_name_map[base_name.lower()].append(name)
        
        query_words = query_lower.split()
        # T√¨m t·ª´ng word trong query (case-insensitive)
        for word in query_words:
            if len(word) < 3:  # Skip short words
                continue
            
            # Method 5a: Exact match (case-insensitive) - v·ªõi full name
            if word in self._entity_lowercase_map:
                entity_name = self._entity_lowercase_map[word]
                # Check xem ƒë√£ c√≥ ch∆∞a
                if not any(e['text'].lower() == entity_name.lower() for e in entities):
                    entity_data = self.kg.get_entity(entity_name)
                    if entity_data:
                        entities.append({
                            'text': entity_name,
                            'type': entity_data.get('label', 'Unknown'),
                            'method': 'kg_lookup_fuzzy_exact',
                            'score': 0.9
                        })
                        if len(entities) >= 5:  # ƒê·ªß r·ªìi
                            break
                    continue
            
            # Method 5a2: Match v·ªõi base name (kh√¥ng c√≥ ƒëu√¥i)
            # V√≠ d·ª•: query "lisa" ‚Üí match v·ªõi "Lisa (ca sƒ©)"
            if word in self._entity_base_name_map:
                for entity_name in self._entity_base_name_map[word]:
                    if not any(e['text'].lower() == entity_name.lower() for e in entities):
                        entity_data = self.kg.get_entity(entity_name)
                        if entity_data:
                            entities.append({
                                'text': entity_name,
                                'type': entity_data.get('label', 'Unknown'),
                                'method': 'kg_lookup_base_name',
                                'score': 0.95  # High score v√¨ match ch√≠nh x√°c base name
                            })
                            if len(entities) >= 5:  # ƒê·ªß r·ªìi
                                break
            
            # Method 5b: Partial match - word l√† substring c·ªßa entity name (ho·∫∑c base name)
            for entity_name in self._all_entity_names[:1000]:  # Limit ƒë·ªÉ tr√°nh ch·∫≠m
                entity_lower = entity_name.lower()
                base_name = self._normalize_entity_name(entity_name).lower()
                
                # Check n·∫øu word match v·ªõi full name ho·∫∑c base name
                if (word in entity_lower and len(word) >= 3) or (word in base_name and len(word) >= 3):
                    # Check xem ƒë√£ c√≥ ch∆∞a
                    if not any(e['text'].lower() == entity_name.lower() for e in entities):
                        entity_data = self.kg.get_entity(entity_name)
                        if entity_data:
                            # Ch·ªâ th√™m n·∫øu l√† Artist ho·∫∑c Group (tr√°nh false positives)
                            entity_type = entity_data.get('label', '')
                            if entity_type in ['Artist', 'Group', 'Company']:
                                entities.append({
                                    'text': entity_name,
                                    'type': entity_type,
                                    'method': 'kg_lookup_fuzzy_partial',
                                    'score': 0.7
                                })
                                if len(entities) >= 5:  # ƒê·ªß r·ªìi
                                    break
                    
        # 1c. Semantic similarity search (if available)
        if self.embedder:
            similar_entities = self.semantic_search(query, top_k=3)
            for entity, score in similar_entities:
                if score > 0.5:  # Threshold
                    entities.append({
                        'text': entity,
                        'type': self.kg.get_entity_type(entity),
                        'method': 'semantic',
                        'score': score
                    })
        
        # ============================================
        # PH∆Ø∆†NG PH√ÅP 2: LLM Understanding (FALLBACK/AUGMENTATION + INTENT DETECTION)
        # ============================================
        # ‚úÖ CHI·∫æN L∆Ø·ª¢C: LLM d√πng ƒë·ªÉ:
        # 1. FALLBACK: Khi rule/semantic kh√¥ng t√¨m ƒë·ªß entities (< 2)
        # 2. AUGMENTATION: Khi confidence th·∫•p ho·∫∑c c·∫ßn normalize (lowercase names)
        # 3. INTENT DETECTION: Detect intent ch√≠nh x√°c h∆°n rule-based (x·ª≠ l√Ω bi·∫øn th·ªÉ ng√¥n ng·ªØ)
        #    - "c√πng m·ªôt nh√≥m nh·∫°c" ‚Üí same_group (rule c√≥ th·ªÉ miss t·ª´ "m·ªôt")
        #    - "thu·ªôc nh√≥m nh·∫°c n√†o" ‚Üí membership (rule c√≥ th·ªÉ miss bi·∫øn th·ªÉ)
        # - Parse: Extract entities, detect intent, detect hop depth
        # 
        # ‚ö†Ô∏è QUAN TR·ªåNG: 
        # - LLM CH·ªà parse c√¢u h·ªèi ‚Üí KH√îNG l√†m reasoning
        # - T·∫•t c·∫£ k·∫øt qu·∫£ t·ª´ LLM PH·∫¢I ƒë∆∞·ª£c validate v·ªõi KG + threshold
        llm_intent = None
        llm_metadata = {}
        if self.llm_for_understanding:
            # ‚úÖ LU√îN g·ªçi LLM ƒë·ªÉ detect intent (quan tr·ªçng cho bi·∫øn th·ªÉ ng√¥n ng·ªØ)
            # G·ªçi LLM trong c√°c tr∆∞·ªùng h·ª£p:
            # 1. Kh√¥ng t√¨m ƒë·ªß entities (< 2) - rule/semantic kh√¥ng ƒë·ªß
            # 2. Query c√≥ lowercase names (jungkook, lisa) - pattern matching c√≥ th·ªÉ miss
            # 3. Query c√≥ comparison keywords - c·∫ßn detect intent ch√≠nh x√°c
            # 4. Query c√≥ t·ª´ "m·ªôt", "c√°c", "n√†o" - bi·∫øn th·ªÉ ng√¥n ng·ªØ t·ª± nhi√™n
            should_use_llm = (
                not entities or 
                len(entities) < 2 or
                any(word.islower() and len(word) >= 4 for word in query_lower.split()) or  # C√≥ lowercase words d√†i
                any(kw in query_lower for kw in ['v√†', 'and', 'c√πng', 'same', 'c√≥ ph·∫£i', 'ph·∫£i', 'm·ªôt', 'c√°c', 'n√†o'])  # C√¢u h·ªèi so s√°nh ho·∫∑c bi·∫øn th·ªÉ
            )
            
            if should_use_llm:
                try:
                    llm_entities = self._extract_entities_with_llm(query)
                    # ‚úÖ ALWAYS VALIDATE: K·∫øt qu·∫£ t·ª´ LLM ph·∫£i ƒë∆∞·ª£c validate v·ªõi KG + threshold
                    # Chi·∫øn l∆∞·ª£c an to√†n: LLM l√†m fallback, nh∆∞ng ph·∫£i validate tr∆∞·ªõc khi d√πng
                    for llm_entity in llm_entities:
                        # Ch·ªâ th√™m n·∫øu ch∆∞a c√≥ v√† ƒë√£ ƒë∆∞·ª£c validate v·ªõi KG
                        if not any(e['text'].lower() == llm_entity['text'].lower() for e in entities):
                            entity_id = llm_entity.get('text', '')
                            if entity_id:
                                # Validate 1: Check entity t·ªìn t·∫°i trong KG
                                entity_data = self.kg.get_entity(entity_id)
                                if entity_data:
                                    # Validate 2: Check confidence threshold (n·∫øu c√≥)
                                    llm_score = llm_entity.get('score', 0.5)
                                    # N·∫øu LLM tr·∫£ v·ªÅ score th·∫•p, verify th√™m b·∫±ng KG search
                                    if llm_score < 0.6:
                                        kg_results = self.kg.search_entities(entity_id, limit=1)
                                        if kg_results and kg_results[0]['score'] > 0.6:
                                            # KG search confirm ‚Üí d√πng v·ªõi score t·ª´ KG
                                            llm_entity['score'] = kg_results[0]['score']
                                            entities.append(llm_entity)
                                    else:
                                        # LLM score ƒë·ªß cao ‚Üí d√πng lu√¥n (ƒë√£ validate v·ªõi KG)
                                        entities.append(llm_entity)
                except Exception as e:
                    # N·∫øu LLM fail, fallback v·ªÅ pattern matching (an to√†n)
                    pass
                    
        # Deduplicate
        seen = set()
        unique_entities = []
        for entity in entities:
            if entity['text'] not in seen:
                seen.add(entity['text'])
                unique_entities.append(entity)
                
        return unique_entities
    
    def _extract_entities_with_llm(self, query: str) -> List[Dict]:
        """
        S·ª≠ d·ª•ng LLM nh·ªè ƒë·ªÉ hi·ªÉu ƒë·∫ßu v√†o (intent + entity extraction).
        
        ‚úÖ LLM nh·ªè (‚â§1B params) ph√π h·ª£p cho nhi·ªám v·ª• n√†y v√¨:
        - Hi·ªÉu c√¢u ti·∫øng Vi·ªát t·ª± nhi√™n t·ªët h∆°n rule
        - Normalize c√¢u h·ªèi ‚Üí mapping v·ªÅ template
        - X·ª≠ l√Ω ƒëa d·∫°ng ng√¥n ng·ªØ t·ª± nhi√™n
        
        LLM s·∫Ω:
        - Detect intent (lo·∫°i c√¢u h·ªèi: membership, company, same group, comparison, etc.)
        - Extract entities (ngh·ªá sƒ©, nh√≥m, c√¥ng ty) trong c√¢u h·ªèi
        - Extract relations (MEMBER_OF, MANAGED_BY, FRIENDS_WITH, etc.)
        - Detect multi-hop depth (1-hop, 2-hop, 3-hop)
        - Hi·ªÉu ng·ªØ c·∫£nh ph·ª©c t·∫°p (lowercase names, nhi·ªÅu entities, so s√°nh)
        
        ‚ö†Ô∏è QUAN TR·ªåNG: LLM CH·ªà d√πng ƒë·ªÉ parse c√¢u ‚Üí KH√îNG l√†m reasoning
        Reasoning v·∫´n do ƒë·ªì th·ªã th·ª±c hi·ªán (graph traversal, path search)
        
        Args:
            query: User's question
            
        Returns:
            List of extracted entities with types
        """
        if not self.llm_for_understanding:
            return []
        
        # Prompt cho LLM ƒë·ªÉ hi·ªÉu ƒë·∫ßu v√†o - C·∫¢I THI·ªÜN ƒë·ªÉ detect intent, relations, multi-hop depth
        prompt = f"""B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ K-pop. Nhi·ªám v·ª• c·ªßa b·∫°n l√† HI·ªÇU C√ÇU H·ªéI (parse input), kh√¥ng ph·∫£i tr·∫£ l·ªùi.

C√¢u h·ªèi: "{query}"

NHI·ªÜM V·ª§ C·ª¶A B·∫†N:
1. Detect Intent (lo·∫°i c√¢u h·ªèi):
   - membership: "X c√≥ ph·∫£i th√†nh vi√™n Y kh√¥ng?", "X thu·ªôc nh√≥m n√†o?"
   - same_group: "X v√† Y c√≥ c√πng nh√≥m kh√¥ng?", "X v√† Y c√≥ c√πng ban nh·∫°c kh√¥ng?"
   - same_company: "X v√† Y c√≥ c√πng c√¥ng ty kh√¥ng?", "X v√† Y c√≥ c√πng h√£ng ƒëƒ©a kh√¥ng?"
   - company: "X thu·ªôc c√¥ng ty n√†o?", "C√¥ng ty n√†o qu·∫£n l√Ω X?"
   - song: "X h√°t b√†i n√†o?", "B√†i h√°t n√†o c·ªßa X?"
   - album: "X ph√°t h√†nh album n√†o?"
   - comparison: "X v√† Y c√≥ li√™n quan g√¨?", "So s√°nh X v√† Y"
   - multi_hop: "B·∫°n c·ªßa X l√† ai?", "Nh·ªØng ng∆∞·ªùi c√πng c√¥ng ty v·ªõi ng∆∞·ªùi h·ª£p t√°c v·ªõi X?"

2. Extract Entities (t√¨m T·∫§T C·∫¢ entities):
   - X·ª≠ l√Ω lowercase names: "jungkook" ‚Üí "Jungkook", "lisa" ‚Üí "Lisa"
   - X·ª≠ l√Ω t√™n c√≥ ƒëu√¥i: "Lisa (ca sƒ©)" ‚Üí "Lisa"
   - Hi·ªÉu ng·ªØ c·∫£nh: "jungkook v√† lisa" ‚Üí c·∫£ 2 ƒë·ªÅu l√† entities
   - T√¨m t·∫•t c·∫£: ngh·ªá sƒ©, nh√≥m, c√¥ng ty, b√†i h√°t, album

3. Extract Relations (lo·∫°i quan h·ªá):
   - MEMBER_OF: "th√†nh vi√™n", "thu·ªôc nh√≥m", "member"
   - MANAGED_BY: "c√¥ng ty", "h√£ng ƒëƒ©a", "qu·∫£n l√Ω", "company"
   - FRIENDS_WITH: "b·∫°n", "quen", "ch∆°i chung"
   - SINGS: "h√°t", "tr√¨nh b√†y", "ca kh√∫c"
   - RELEASED: "ph√°t h√†nh", "album"

4. Detect Multi-hop Depth:
   - 1-hop: "X thu·ªôc nh√≥m n√†o?" (X ‚Üí Group)
   - 2-hop: "X thu·ªôc c√¥ng ty n√†o?" (X ‚Üí Group ‚Üí Company)
   - 3-hop: "B·∫°n c·ªßa X l√† ai?" (X ‚Üí Friend ‚Üí Friend's Friend)

Tr·∫£ l·ªùi theo format JSON:
{{
    "intent": "membership|same_group|same_company|company|song|album|comparison|multi_hop",
    "entities": [
        {{"name": "t√™n th·ª±c th·ªÉ", "type": "Artist|Group|Company|Song|Album"}}
    ],
    "relations": ["MEMBER_OF|MANAGED_BY|FRIENDS_WITH|SINGS|RELEASED"],
    "multi_hop_depth": 1 ho·∫∑c 2 ho·∫∑c 3,
    "question_type": "yes_no|true_false|fact|comparison"
}}

Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng th√™m text kh√°c."""

        try:
            response = self.llm_for_understanding.generate(
                prompt,
                context="",
                max_new_tokens=300,  # TƒÉng ƒë·ªÉ ƒë·ªß cho intent, relations, multi_hop_depth
                temperature=0.1  # Low temperature ƒë·ªÉ output nh·∫•t qu√°n
            )
            
            # Parse JSON response
            import json
            # Extract JSON from response (c√≥ th·ªÉ c√≥ text th√™m)
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group())
                
                # L∆∞u intent, relations, multi_hop_depth ƒë·ªÉ d√πng sau
                # (c√≥ th·ªÉ l∆∞u v√†o context ho·∫∑c return c√πng v·ªõi entities)
                intent = data.get('intent', '')
                relations = data.get('relations', [])
                multi_hop_depth = data.get('multi_hop_depth', 2)
                question_type = data.get('question_type', 'fact')
                
                # Extract entities
                entities = []
                for item in data.get('entities', []):
                    name = item.get('name', '').strip()
                    entity_type = item.get('type', '').strip()
                    if name:
                        # T√¨m entity trong knowledge graph
                        results = self.kg.search_entities(name, limit=1)
                        if results and results[0]['score'] > 0.6:
                            entity_dict = {
                                'text': results[0]['id'],
                                'type': results[0]['type'],
                                'method': 'llm_understanding',
                                'score': results[0]['score']
                            }
                            # Th√™m metadata t·ª´ LLM understanding
                            entity_dict['intent'] = intent
                            entity_dict['relations'] = relations
                            entity_dict['multi_hop_depth'] = multi_hop_depth
                            entity_dict['question_type'] = question_type
                            entities.append(entity_dict)
                        else:
                            # N·∫øu kh√¥ng t√¨m th·∫•y trong KG, v·∫´n th√™m v·ªõi type t·ª´ LLM
                            entity_dict = {
                                'text': name,
                                'type': entity_type,
                                'method': 'llm_understanding',
                                'score': 0.5
                            }
                            # Th√™m metadata
                            entity_dict['intent'] = intent
                            entity_dict['relations'] = relations
                            entity_dict['multi_hop_depth'] = multi_hop_depth
                            entity_dict['question_type'] = question_type
                            entities.append(entity_dict)
                
                return entities
        except Exception as e:
            # N·∫øu LLM fail, return empty list
            return []
        
        return []
        
    def semantic_search(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """
        Search entities by semantic similarity.
        
        Args:
            query: Search query
            top_k: Number of results
            
        Returns:
            List of (entity_id, score) tuples
        """
        if not self.embedder:
            return []
            
        # Encode query
        query_embedding = self.embedder.encode([query])[0]
        query_embedding = query_embedding / np.linalg.norm(query_embedding)
        
        if FAISS_AVAILABLE and self.faiss_index:
            # Fast FAISS search
            distances, indices = self.faiss_index.search(
                query_embedding.reshape(1, -1).astype('float32'),
                top_k
            )
            results = [
                (self.entity_ids[idx], float(dist))
                for idx, dist in zip(indices[0], distances[0])
            ]
        else:
            # Numpy fallback
            normalized = self.entity_embeddings / np.linalg.norm(
                self.entity_embeddings, axis=1, keepdims=True
            )
            similarities = np.dot(normalized, query_embedding)
            top_indices = np.argsort(similarities)[-top_k:][::-1]
            results = [
                (self.entity_ids[idx], float(similarities[idx]))
                for idx in top_indices
            ]
            
        return results
        
    def retrieve_context(
        self,
        query: str,
        max_entities: int = 5,
        max_hops: int = 2,
        include_paths: bool = True
    ) -> Dict:
        """
        Retrieve relevant context for a query using GraphRAG.
        
        ‚úÖ GraphRAG = 3 b∆∞·ªõc:
        1. Semantic Search: T√¨m node g·∫ßn nh·∫•t b·∫±ng vector search (FAISS + embeddings)
        2. Expand Subgraph: T·ª´ node t√¨m ƒë∆∞·ª£c ‚Üí m·ªü r·ªông h√†ng x√≥m 1-2 hop ‚Üí l·∫•y subgraph
        3. Build Context: Chuy·ªÉn subgraph ‚Üí text/triples ƒë·ªÉ feed v√†o LLM
        
        Args:
            query: User's question
            max_entities: Maximum number of entities to retrieve
            max_hops: Maximum hops for graph traversal (subgraph expansion)
            include_paths: Whether to include relationship paths
            
        Returns:
            Context dictionary with entities, relationships, and facts
        """
        context = {
            'query': query,
            'entities': [],
            'relationships': [],
            'facts': [],
            'paths': []
        }
        
        seen_entities = set()
        
        # ============================================
        # B∆Ø·ªöC 1: SEMANTIC SEARCH
        # T√¨m c√°c node g·∫ßn nh·∫•t v·ªõi c√¢u h·ªèi b·∫±ng vector search (FAISS + embeddings)
        # ============================================
        seed_entities = []
        
        # 1a. Pattern-based extraction (fallback n·∫øu kh√¥ng c√≥ embeddings)
        extracted = self.extract_entities(query)
        for entity_info in extracted[:max_entities]:
            entity_id = entity_info['text']
            if entity_id not in seen_entities:
                seed_entities.append((entity_id, entity_info.get('score', 1.0), 'pattern'))
                seen_entities.add(entity_id)
        
        # 1b. Semantic Search (∆∞u ti√™n - t√¨m node g·∫ßn nh·∫•t b·∫±ng FAISS)
        if self.embedder:
            similar_entities = self.semantic_search(query, top_k=max_entities)
            for entity_id, score in similar_entities:
                if entity_id not in seen_entities and score > 0.5:  # Threshold
                    seed_entities.append((entity_id, score, 'semantic'))
                    seen_entities.add(entity_id)
        
        # Sort by relevance (semantic search results first)
        seed_entities.sort(key=lambda x: (x[2] == 'semantic', x[1]), reverse=True)
        seed_entities = seed_entities[:max_entities]
        
        # ============================================
        # B∆Ø·ªöC 2: EXPAND SUBGRAPH (multi-hop)
        # T·ª´ node t√¨m ƒë∆∞·ª£c ‚Üí m·ªü r·ªông h√†ng x√≥m 1-2 hop ‚Üí l·∫•y subgraph li√™n quan
        # ============================================
        subgraph_entities = set()
        subgraph_relationships = []
        
        for entity_id, relevance, method in seed_entities:
            # M·ªü r·ªông subgraph t·ª´ entity n√†y (1-2 hop)
            entity_context = self.kg.get_entity_context(entity_id, max_depth=max_hops)
            
            if entity_context:
                # Add main entity
                entity_data = entity_context.get('entity', {})
                context['entities'].append({
                    'id': entity_id,
                    'type': entity_data.get('label'),
                    'info': entity_data.get('infobox', {}),
                    'relevance': relevance,
                    'method': method
                })
                subgraph_entities.add(entity_id)
                
                # Add relationships (edges trong subgraph)
                # QUAN TR·ªåNG: Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng relationships ƒë·ªÉ tr√°nh context qu√° l·ªõn
                relationships = entity_context.get('relationships', [])
                # Ch·ªâ l·∫•y top 10 relationships quan tr·ªçng nh·∫•t cho m·ªói entity
                # ∆Øu ti√™n relationships li√™n quan ƒë·∫øn query
                query_lower = query.lower()
                scored_rels = []
                for rel in relationships:
                    score = 0.0
                    # Boost score n·∫øu entity names trong relationship xu·∫•t hi·ªán trong query
                    if rel.get('source', '').lower() in query_lower:
                        score += 1.0
                    if rel.get('target', '').lower() in query_lower:
                        score += 1.0
                    # Boost score cho c√°c relationship types quan tr·ªçng
                    rel_type = rel.get('type', '')
                    if rel_type in ['MEMBER_OF', 'MANAGED_BY', 'SINGS', 'RELEASED']:
                        score += 0.5
                    scored_rels.append((rel, score))
                
                # Sort v√† l·∫•y top 10
                scored_rels.sort(key=lambda x: x[1], reverse=True)
                for rel, _ in scored_rels[:10]:  # CH·ªà L·∫§Y TOP 10 RELATIONSHIPS
                    rel_key = (rel['source'], rel['type'], rel['target'])
                    if rel_key not in subgraph_relationships:
                        subgraph_relationships.append(rel_key)
                        context['relationships'].append(rel)
                        # Th√™m c√°c entities trong relationship v√†o subgraph
                        subgraph_entities.add(rel['source'])
                        subgraph_entities.add(rel['target'])
                        
                        # Gi·ªõi h·∫°n t·ªïng s·ªë relationships
                        if len(context['relationships']) >= 30:  # T·ªëi ƒëa 30 relationships
                            break
                
                # Add connected entities (h√†ng x√≥m trong subgraph)
                # QUAN TR·ªåNG: Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng ƒë·ªÉ tr√°nh context qu√° l·ªõn
                connected = entity_context.get('connected_entities', {})
                # Ch·ªâ l·∫•y top 5 neighbors quan tr·ªçng nh·∫•t cho m·ªói seed entity
                sorted_neighbors = sorted(
                    connected.items(),
                    key=lambda x: x[1].get('depth', 999),  # ∆Øu ti√™n 1-hop neighbors
                    reverse=False
                )[:5]  # CH·ªà L·∫§Y TOP 5 NEIGHBORS
                
                for neighbor_id, neighbor_info in sorted_neighbors:
                    if neighbor_id not in subgraph_entities:
                        neighbor_data = self.kg.get_entity(neighbor_id)
                        if neighbor_data:
                            context['entities'].append({
                                'id': neighbor_id,
                                'type': neighbor_info.get('type'),
                                'info': neighbor_data.get('infobox', {}),
                                'relevance': relevance * 0.8,  # Gi·∫£m relevance cho h√†ng x√≥m
                                'method': f'subgraph_expansion_{neighbor_info.get("depth", 1)}-hop'
                            })
                            subgraph_entities.add(neighbor_id)
                            
                            # Gi·ªõi h·∫°n t·ªïng s·ªë entities trong context
                            if len(context['entities']) >= 30:  # T·ªëi ƒëa 30 entities
                                break
                    
                    if len(context['entities']) >= 30:  # T·ªëi ƒëa 30 entities
                        break
                
                # Generate facts from entity data
                facts = self._generate_facts(entity_id, entity_data)
                context['facts'].extend(facts)
        
        # Find paths between seed entities (multi-hop paths trong subgraph)
        if include_paths and len(seed_entities) >= 2:
            for i in range(len(seed_entities) - 1):
                for j in range(i + 1, min(i + 3, len(seed_entities))):
                    source = seed_entities[i][0]
                    target = seed_entities[j][0]
                    paths = self.kg.find_all_paths(source, target, max_hops=max_hops)
                    for path in paths[:3]:  # Limit paths
                        path_details = self.kg.get_path_details(path)
                        context['paths'].append({
                            'from': source,
                            'to': target,
                            'path': path,
                            'details': path_details
                        })
                        
        # ============================================
        # B∆Ø·ªöC 2.5: GRAPH RANKING (Module B)
        # X·∫øp h·∫°ng ƒë·ªô li√™n quan c·ªßa triples v√† l·ªçc
        # ============================================
        context = self._rank_and_filter_context(context, query)
        
        return context
    
    def _rank_and_filter_context(self, context: Dict, query: str) -> Dict:
        """
        üî∂ MODULE B - GRAPH RANKING
        X·∫øp h·∫°ng ƒë·ªô li√™n quan c·ªßa triples v√† l·ªçc.
        
        L·ªçc b·∫±ng:
        1. Similarity gi·ªØa node label v·ªõi c√¢u h·ªèi
        2. ƒê·ªô quan tr·ªçng (degree / PageRank)
        3. Lo·∫°i quan h·ªá ph√π h·ª£p v·ªõi c√¢u h·ªèi
        
        Args:
            context: Context dictionary v·ªõi entities, relationships, facts
            query: User's question
            
        Returns:
            Filtered v√† ranked context
        """
        query_lower = query.lower()
        
        # 1. Rank relationships (triples) by relevance
        ranked_relationships = []
        for rel in context['relationships']:
            score = 0.0
            
            # 1a. Similarity gi·ªØa node label v·ªõi c√¢u h·ªèi
            source = rel['source']
            target = rel['target']
            rel_type = rel['type']
            
            # Check if entity names appear in query
            if source.lower() in query_lower:
                score += 0.3
            if target.lower() in query_lower:
                score += 0.3
            
            # 1b. ƒê·ªô quan tr·ªçng (degree - s·ªë l∆∞·ª£ng connections)
            source_degree = len(list(self.kg.graph.neighbors(source))) if source in self.kg.graph else 0
            target_degree = len(list(self.kg.graph.neighbors(target))) if target in self.kg.graph else 0
            # Normalize degree score (0-0.2)
            degree_score = min((source_degree + target_degree) / 50.0, 0.2)
            score += degree_score
            
            # 1c. Lo·∫°i quan h·ªá ph√π h·ª£p v·ªõi c√¢u h·ªèi
            # Map query keywords to relevant relationship types
            rel_keywords = {
                'MEMBER_OF': ['th√†nh vi√™n', 'member', 'nh√≥m', 'group', 'thu·ªôc', 'belongs'],
                'MANAGED_BY': ['c√¥ng ty', 'company', 'h√£ng ƒëƒ©a', 'label', 'qu·∫£n l√Ω', 'manage'],
                'SINGS': ['h√°t', 'sing', 'b√†i h√°t', 'song', 'ca kh√∫c'],
                'RELEASED': ['ph√°t h√†nh', 'release', 'album', 'single'],
                'COLLAB_WITH': ['h·ª£p t√°c', 'collab', 'collaborate', 'c√πng'],
                'PRODUCED_BY': ['s·∫£n xu·∫•t', 'produce', 'producer']
            }
            
            for rel_type_key, keywords in rel_keywords.items():
                if rel_type == rel_type_key:
                    for keyword in keywords:
                        if keyword in query_lower:
                            score += 0.3
                            break
            
            ranked_relationships.append({
                'relationship': rel,
                'score': score
            })
        
        # Sort by score v√† l·ªçc top relationships
        ranked_relationships.sort(key=lambda x: x['score'], reverse=True)
        # Gi·ªØ top 15 relationships c√≥ score > 0.1
        filtered_relationships = [
            item['relationship'] 
            for item in ranked_relationships 
            if item['score'] > 0.1
        ][:15]
        
        # 2. Rank entities by relevance
        ranked_entities = []
        for entity in context['entities']:
            score = entity.get('relevance', 0.0)
            entity_id = entity['id']
            
            # Boost score n·∫øu entity name xu·∫•t hi·ªán trong query
            if entity_id.lower() in query_lower:
                score += 0.5
            
            # Boost score n·∫øu entity type ph√π h·ª£p v·ªõi query
            entity_type = entity.get('type', '')
            type_keywords = {
                'Group': ['nh√≥m', 'group', 'band'],
                'Artist': ['ca sƒ©', 'artist', 'singer', 'idol'],
                'Song': ['b√†i h√°t', 'song', 'ca kh√∫c'],
                'Company': ['c√¥ng ty', 'company', 'label', 'h√£ng ƒëƒ©a']
            }
            
            for type_key, keywords in type_keywords.items():
                if entity_type == type_key:
                    for keyword in keywords:
                        if keyword in query_lower:
                            score += 0.3
                            break
            
            ranked_entities.append({
                'entity': entity,
                'score': score
            })
        
        # Sort entities by score
        ranked_entities.sort(key=lambda x: x['score'], reverse=True)
        # QUAN TR·ªåNG: Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng entities ƒë·ªÉ tr√°nh context qu√° l·ªõn (1969 entities!)
        # CH·ªà L·∫§Y TOP 20 ENTITIES - ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi nh∆∞ng kh√¥ng qu√° nhi·ªÅu
        filtered_entities = [
            item['entity'] 
            for item in ranked_entities 
            if item['score'] > 0.1  # Ch·ªâ l·∫•y entities c√≥ score > 0.1
        ][:20]  # T·ªëi ƒëa 20 entities
        
        # 3. Filter facts (keep top 10 most relevant)
        facts = context['facts'][:10]
        
        # Update context v·ªõi ranked v√† filtered data
        context['entities'] = filtered_entities
        context['relationships'] = filtered_relationships
        
        return context
        
    def _generate_facts(self, entity_id: str, entity_data: Dict) -> List[str]:
        """
        Generate natural language facts from entity data.
        
        ‚ö†Ô∏è L∆ØU √ù: ƒê√¢y KH√îNG ph·∫£i reasoning, ch·ªâ l√† format d·ªØ li·ªáu t·ª´ ƒë·ªì th·ªã.
        Method n√†y ch·ªâ chuy·ªÉn ƒë·ªïi th√¥ng tin t·ª´ entity data (infobox, relationships)
        th√†nh c√¢u vƒÉn t·ª± nhi√™n ƒë·ªÉ ƒë∆∞a v√†o context cho LLM.
        
        T·∫•t c·∫£ facts ƒë·ªÅu l·∫•y t·ª´ Knowledge Graph, kh√¥ng t·ª± nghƒ© ra.
        """
        facts = []
        entity_type = entity_data.get('label', 'Entity')
        infobox = entity_data.get('infobox', {})
        
        # Type-specific fact generation
        if entity_type == 'Group':
            if 'Th√†nh vi√™n' in infobox and infobox['Th√†nh vi√™n']:
                facts.append(f"{entity_id} c√≥ c√°c th√†nh vi√™n: {infobox['Th√†nh vi√™n']}")
            if 'NƒÉm ho·∫°t ƒë·ªông' in infobox:
                facts.append(f"{entity_id} ho·∫°t ƒë·ªông t·ª´ {infobox['NƒÉm ho·∫°t ƒë·ªông']}")
            if 'H√£ng ƒëƒ©a' in infobox:
                facts.append(f"{entity_id} thu·ªôc c√¥ng ty {infobox['H√£ng ƒëƒ©a']}")
            if 'Th·ªÉ lo·∫°i' in infobox:
                facts.append(f"{entity_id} ch∆°i nh·∫°c {infobox['Th·ªÉ lo·∫°i']}")
                
            # Get members from relationships
            members = self.kg.get_group_members(entity_id)
            if members:
                facts.append(f"Th√†nh vi√™n c·ªßa {entity_id}: {', '.join(members[:10])}")
                
        elif entity_type == 'Artist':
            groups = self.kg.get_artist_groups(entity_id)
            if groups:
                facts.append(f"{entity_id} l√† th√†nh vi√™n c·ªßa: {', '.join(groups)}")
                
        elif entity_type == 'Company':
            groups = self.kg.get_company_groups(entity_id)
            if groups:
                facts.append(f"C√°c nh√≥m nh·∫°c thu·ªôc {entity_id}: {', '.join(groups[:10])}")
                
        return facts
        
    def format_context_for_llm(self, context: Dict, max_tokens: int = 20000) -> str:
        """
        B∆Ø·ªöC 3: BUILD CONTEXT CHO LLM
        Chuy·ªÉn subgraph ‚Üí text/triples ƒë·ªÉ feed v√†o m√¥ h√¨nh 1B.
        
        Format retrieved context (subgraph) as a prompt for the LLM.
        Chuy·ªÉn ƒë·ªïi subgraph (entities, relationships, paths) th√†nh text format.
        
        Args:
            context: Retrieved context dictionary (t·ª´ subgraph expansion)
            max_tokens: Maximum tokens for context (default 20000, leaving room for query + response)
            
        Returns:
            Formatted context string (text/triples format cho LLM)
        """
        parts = []
        
        # ============================================
        # Format 1: Entities (Nodes trong subgraph)
        # ============================================
        if context['entities']:
            parts.append("=== TH√îNG TIN TH·ª∞C TH·ªÇ (T·ª´ Subgraph) ===")
            # Sort by relevance v√† gi·ªõi h·∫°n s·ªë l∆∞·ª£ng
            sorted_entities = sorted(context['entities'], key=lambda x: x.get('relevance', 0), reverse=True)
            # Gi·ªõi h·∫°n: ch·ªâ l·∫•y top 10 entities quan tr·ªçng nh·∫•t
            for entity in sorted_entities[:10]:
                entity_str = f"\nüìç {entity['id']} (Lo·∫°i: {entity['type']})"
                if entity.get('method'):
                    entity_str += f" [T√¨m b·∫±ng: {entity['method']}]"
                info = entity.get('info', {})
                if info:
                    # Gi·ªõi h·∫°n: ch·ªâ l·∫•y 3 fields quan tr·ªçng nh·∫•t
                    for key, value in list(info.items())[:3]:
                        if value:
                            entity_str += f"\n  ‚Ä¢ {key}: {value}"
                parts.append(entity_str)
                
        # ============================================
        # Format 2: Facts (Triples t·ª´ subgraph)
        # ============================================
        if context['facts']:
            parts.append("\n=== S·ª∞ KI·ªÜN (Triples t·ª´ Subgraph) ===")
            # Gi·ªõi h·∫°n: ch·ªâ l·∫•y top 5 facts quan tr·ªçng nh·∫•t
            for fact in context['facts'][:5]:
                parts.append(f"‚Ä¢ {fact}")
                
        # ============================================
        # Format 3: Relationships (Edges trong subgraph - Triples format)
        # ============================================
        if context['relationships']:
            parts.append("\n=== M·ªêI QUAN H·ªÜ (Edges trong Subgraph - Triples) ===")
            seen_rels = set()
            # Gi·ªõi h·∫°n: ch·ªâ l·∫•y top 10 relationships quan tr·ªçng nh·∫•t
            for rel in context['relationships'][:10]:
                rel_key = (rel['source'], rel['type'], rel['target'])
                if rel_key not in seen_rels:
                    seen_rels.add(rel_key)
                    # Format as triple: (source, relationship, target)
                    parts.append(f"‚Ä¢ ({rel['source']}, {rel['type']}, {rel['target']})")
                    
        # ============================================
        # Format 4: Paths (Multi-hop paths trong subgraph)
        # ============================================
        if context['paths']:
            parts.append("\n=== ƒê∆Ø·ªúNG D·∫™N QUAN H·ªÜ (Multi-hop Paths trong Subgraph) ===")
            # Gi·ªõi h·∫°n: ch·ªâ l·∫•y top 3 paths quan tr·ªçng nh·∫•t
            for path_info in context['paths'][:3]:
                path = path_info['path']
                path_str = " ‚Üí ".join(path)
                parts.append(f"‚Ä¢ Path: {path_str}")
                # Kh√¥ng th√™m path details ƒë·ªÉ gi·∫£m ƒë·ªô d√†i
                
        context_text = "\n".join(parts)
        
        # ============================================
        # Truncate n·∫øu qu√° d√†i (∆∞·ªõc t√≠nh tokens)
        # ============================================
        # ∆Ø·ªõc t√≠nh: 1 token ‚âà 4 characters (ti·∫øng Vi·ªát)
        max_chars = max_tokens * 4
        if len(context_text) > max_chars:
            # Truncate v√† th√™m th√¥ng b√°o
            context_text = context_text[:max_chars]
            # C·∫Øt ·ªü d√≤ng cu·ªëi c√πng ho√†n ch·ªânh
            last_newline = context_text.rfind('\n')
            if last_newline > max_chars * 0.9:  # N·∫øu c√≥ newline g·∫ßn cu·ªëi
                context_text = context_text[:last_newline]
            context_text += "\n\n[... Context ƒë√£ ƒë∆∞·ª£c r√∫t g·ªçn ƒë·ªÉ ph√π h·ª£p v·ªõi gi·ªõi h·∫°n model ...]"
        
        return context_text
        
    def get_multi_hop_context(
        self,
        query: str,
        hop_questions: List[str] = None,
        max_hops: int = 3
    ) -> Dict:
        """
        Get context for multi-hop reasoning questions.
        
        Args:
            query: Main query
            hop_questions: Intermediate questions for each hop
            max_hops: Maximum reasoning hops
            
        Returns:
            Multi-hop context with intermediate results
        """
        multi_hop_context = {
            'query': query,
            'hops': [],
            'final_context': None
        }
        
        # Extract entities from main query
        entities = self.extract_entities(query)
        current_entities = [e['text'] for e in entities]
        
        for hop in range(max_hops):
            hop_result = {
                'hop_number': hop + 1,
                'entities': current_entities,
                'context': {},
                'next_entities': []
            }
            
            # Get context for current entities
            for entity_id in current_entities[:3]:
                entity_context = self.kg.get_entity_context(entity_id, max_depth=1)
                if entity_context:
                    hop_result['context'][entity_id] = entity_context
                    
                    # Find next entities to explore
                    for rel in entity_context.get('relationships', []):
                        next_entity = rel['target'] if rel['source'] == entity_id else rel['source']
                        if next_entity not in current_entities:
                            hop_result['next_entities'].append(next_entity)
                            
            multi_hop_context['hops'].append(hop_result)
            
            # Move to next hop entities
            if not hop_result['next_entities']:
                break
            current_entities = list(set(hop_result['next_entities']))[:5]
            
        # Compile final context
        multi_hop_context['final_context'] = self.retrieve_context(query, max_hops=max_hops)
        
        return multi_hop_context


def main():
    """Test GraphRAG."""
    print("üîÑ Initializing GraphRAG...")
    rag = GraphRAG()
    
    # Test queries
    test_queries = [
        "BTS c√≥ bao nhi√™u th√†nh vi√™n?",
        "C√¥ng ty n√†o qu·∫£n l√Ω BLACKPINK?",
        "Ai l√† th√†nh vi√™n c·ªßa (G)I-dle?",
        "BTS v√† SEVENTEEN c√≥ c√πng c√¥ng ty kh√¥ng?",
    ]
    
    for query in test_queries:
        print(f"\n{'='*50}")
        print(f"‚ùì Query: {query}")
        
        # Extract entities
        entities = rag.extract_entities(query)
        print(f"üìç Extracted entities: {[e['text'] for e in entities]}")
        
        # Retrieve context
        context = rag.retrieve_context(query)
        print(f"üìö Facts: {context['facts'][:3]}")
        
        # Format for LLM
        formatted = rag.format_context_for_llm(context)
        print(f"üìù Formatted context preview:\n{formatted[:500]}...")


if __name__ == "__main__":
    main()



