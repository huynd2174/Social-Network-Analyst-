"""
Small Language Model Integration for K-pop Chatbot

This module integrates small language models (â‰¤1B parameters) for
the K-pop knowledge graph chatbot.

Supported models:
- Qwen2-0.5B-Instruct (500M params) - Recommended
- TinyLlama-1.1B-Chat-v1.0 (1.1B params)
- Phi-3-mini (3.8B params - optional if resources allow)
- gemma-2b-it (2B params - optional)

Features:
- Quantization support (4-bit, 8-bit) for memory efficiency
- Streaming generation
- Context-aware prompting
- Vietnamese language support
"""

import os
import torch
from typing import Dict, List, Optional, Generator, Any
from dataclasses import dataclass

# Check available backends
try:
    from transformers import (
        AutoModelForCausalLM,
        AutoTokenizer,
        BitsAndBytesConfig,
        pipeline,
        TextIteratorStreamer
    )
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("âš ï¸ transformers not installed")

try:
    from threading import Thread
    THREADING_AVAILABLE = True
except ImportError:
    THREADING_AVAILABLE = False


@dataclass
class LLMConfig:
    """Configuration for the language model."""
    model_name: str = "Qwen/Qwen2-0.5B-Instruct"
    max_new_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    top_k: int = 50
    repetition_penalty: float = 1.1
    use_4bit: bool = True
    use_8bit: bool = False
    device_map: str = "auto"
    torch_dtype: str = "float16"


# Pre-defined model configurations
MODEL_CONFIGS = {
    "qwen2-0.5b": LLMConfig(
        model_name="Qwen/Qwen2-0.5B-Instruct",
        max_new_tokens=512,
        temperature=0.7
    ),
    "qwen2.5-0.5b": LLMConfig(
        model_name="Qwen/Qwen2.5-0.5B-Instruct",
        max_new_tokens=512,
        temperature=0.7
    ),
    "tinyllama": LLMConfig(
        model_name="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        max_new_tokens=512,
        temperature=0.7
    ),
    "phi-2": LLMConfig(
        model_name="microsoft/phi-2",
        max_new_tokens=512,
        temperature=0.7
    ),
    "gemma-2b": LLMConfig(
        model_name="google/gemma-2b-it",
        max_new_tokens=512,
        temperature=0.7
    ),
    "bloomz-560m": LLMConfig(
        model_name="bigscience/bloomz-560m",
        max_new_tokens=512,
        temperature=0.7
    ),
    "vietnamese-llama": LLMConfig(
        model_name="vilm/vinallama-2.7b",
        max_new_tokens=512,
        temperature=0.7
    )
}


class SmallLLM:
    """
    Small Language Model wrapper for K-pop chatbot.
    
    Uses quantized models (â‰¤1B parameters) for efficient inference
    while maintaining good response quality for Vietnamese K-pop Q&A.
    """
    
    def __init__(
        self,
        model_key: str = "qwen2-0.5b",
        config: Optional[LLMConfig] = None,
        custom_model_path: Optional[str] = None
    ):
        """
        Initialize the small LLM.
        
        Args:
            model_key: Key from MODEL_CONFIGS or custom model name
            config: Custom LLMConfig (overrides model_key config)
            custom_model_path: Path to local model (overrides everything)
        """
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError("transformers library is required. Install with: pip install transformers")
            
        # Get configuration
        if config:
            self.config = config
        elif model_key in MODEL_CONFIGS:
            self.config = MODEL_CONFIGS[model_key]
        else:
            self.config = LLMConfig(model_name=model_key)
            
        if custom_model_path:
            self.config.model_name = custom_model_path
            
        self.model = None
        self.tokenizer = None
        self.pipe = None
        
        # System prompt for K-pop Q&A
        # LLM cÃ³ 3 nhiá»‡m vá»¥ chÃ­nh:
        # 1. Viáº¿t cÃ¢u tráº£ lá»i tá»± nhiÃªn tá»« facts (triples tá»« Ä‘á»“ thá»‹)
        # 2. Chá»n thÃ´ng tin quan trá»ng (tá»« nhiá»u triples, chá»n nhá»¯ng cÃ¡i cáº§n thiáº¿t Ä‘á»ƒ tráº£ lá»i)
        # 3. GhÃ©p reasoning + context thÃ nh cÃ¢u dá»… Ä‘á»c
        # 
        # QUAN TRá»ŒNG: LLM CHá»ˆ format context tá»« Äá»’ THá»Š TRI THá»¨C, khÃ´ng tá»± nghÄ© ra
        self.system_prompt = """Báº¡n lÃ  trá»£ lÃ½ AI chuyÃªn vá» K-pop (nháº¡c HÃ n Quá»‘c).

NHIá»†M Vá»¤ Cá»¦A Báº N:
1. Viáº¿t cÃ¢u tráº£ lá»i tá»± nhiÃªn tá»« facts (triples) Ä‘Æ°á»£c cung cáº¥p tá»« Äá»’ THá»Š TRI THá»¨C
2. Chá»n thÃ´ng tin quan trá»ng: Náº¿u cÃ³ nhiá»u triples, chá»‰ sá»­ dá»¥ng nhá»¯ng cÃ¡i liÃªn quan trá»±c tiáº¿p Ä‘áº¿n cÃ¢u há»i
3. GhÃ©p reasoning + context thÃ nh cÃ¢u tráº£ lá»i dá»… Ä‘á»c, tá»± nhiÃªn

QUAN TRá»ŒNG - Táº¤T Cáº¢ THÃ”NG TIN Äá»€U Tá»ª Äá»’ THá»Š TRI THá»¨C:
- CHá»ˆ sá»­ dá»¥ng thÃ´ng tin tá»« Ä‘á»“ thá»‹ tri thá»©c Ä‘Æ°á»£c cung cáº¥p (trong pháº§n "THÃ”NG TIN Tá»ª Äá»’ THá»Š TRI THá»¨C")
- Entities (nodes): Tá»« Ä‘á»“ thá»‹ tri thá»©c
- Relationships (edges): Tá»« Ä‘á»“ thá»‹ tri thá»©c
- Facts (triples): Tá»« Ä‘á»“ thá»‹ tri thá»©c
- Reasoning results: Tá»« graph traversal trÃªn Ä‘á»“ thá»‹ tri thá»©c
- KHÃ”NG tá»± nghÄ© ra thÃ´ng tin khÃ´ng cÃ³ trong context
- KHÃ”NG sá»­ dá»¥ng kiáº¿n thá»©c tá»« training data cá»§a báº¡n
- Náº¿u khÃ´ng cÃ³ thÃ´ng tin trong context, hÃ£y nÃ³i rÃµ lÃ  báº¡n khÃ´ng biáº¿t
- Tráº£ lá»i ngáº¯n gá»n, chÃ­nh xÃ¡c, dá»… hiá»ƒu"""

        # Load model
        self._load_model()
        
    def _load_model(self):
        """Load the language model and tokenizer."""
        print(f"ğŸ”„ Loading model: {self.config.model_name}")
        
        # Quantization config
        quantization_config = None
        if self.config.use_4bit:
            try:
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
                print("ğŸ“¦ Using 4-bit quantization")
            except Exception as e:
                print(f"âš ï¸ 4-bit quantization failed: {e}")
                quantization_config = None
        elif self.config.use_8bit:
            try:
                quantization_config = BitsAndBytesConfig(
                    load_in_8bit=True
                )
                print("ğŸ“¦ Using 8-bit quantization")
            except Exception as e:
                print(f"âš ï¸ 8-bit quantization failed: {e}")
                quantization_config = None
                
        # Determine torch dtype
        torch_dtype = torch.float16
        if self.config.torch_dtype == "float32":
            torch_dtype = torch.float32
        elif self.config.torch_dtype == "bfloat16":
            torch_dtype = torch.bfloat16
            
        try:
            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.model_name,
                trust_remote_code=True
            )
            
            # Set pad token if not set
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            # Load model
            model_kwargs = {
                "trust_remote_code": True,
                "device_map": self.config.device_map,
            }
            
            if quantization_config:
                model_kwargs["quantization_config"] = quantization_config
            else:
                model_kwargs["torch_dtype"] = torch_dtype
                
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.model_name,
                **model_kwargs
            )
            
            # Create pipeline
            self.pipe = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer
            )
            
            print(f"âœ… Model loaded successfully!")
            print(f"   Model size: {self._get_model_size()}")
            
        except Exception as e:
            print(f"âŒ Failed to load model: {e}")
            raise
            
    def _get_model_size(self) -> str:
        """Get model size in human-readable format."""
        if self.model is None:
            return "Unknown"
            
        param_count = sum(p.numel() for p in self.model.parameters())
        if param_count >= 1e9:
            return f"{param_count / 1e9:.2f}B parameters"
        elif param_count >= 1e6:
            return f"{param_count / 1e6:.2f}M parameters"
        else:
            return f"{param_count} parameters"
            
    def format_prompt(
        self,
        query: str,
        context: str = "",
        history: List[Dict] = None
    ) -> str:
        """
        Format the prompt for the model.
        
        Args:
            query: User's question
            context: Retrieved context from GraphRAG
            history: Conversation history
            
        Returns:
            Formatted prompt string
        """
        messages = []
        
        # System message
        system_content = self.system_prompt
        if context:
            system_content += f"\n\n### THÃ”NG TIN Tá»ª Äá»’ THá»Š TRI THá»¨C:\n{context}"
            
        messages.append({
            "role": "system",
            "content": system_content
        })
        
        # Conversation history
        if history:
            for msg in history[-5:]:  # Keep last 5 turns
                messages.append(msg)
                
        # Current query
        messages.append({
            "role": "user",
            "content": query
        })
        
        # Format using tokenizer's chat template
        try:
            prompt = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        except Exception:
            # Fallback for models without chat template
            prompt = self._format_prompt_fallback(messages)
            
        return prompt
        
    def _format_prompt_fallback(self, messages: List[Dict]) -> str:
        """Fallback prompt formatting for models without chat template."""
        parts = []
        for msg in messages:
            role = msg["role"]
            content = msg["content"]
            if role == "system":
                parts.append(f"System: {content}")
            elif role == "user":
                parts.append(f"User: {content}")
            elif role == "assistant":
                parts.append(f"Assistant: {content}")
        parts.append("Assistant:")
        return "\n\n".join(parts)
        
    def generate(
        self,
        query: str,
        context: str = "",
        history: List[Dict] = None,
        max_new_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        stream: bool = False
    ) -> str | Generator[str, None, None]:
        """
        Generate response for a query.
        
        Args:
            query: User's question
            context: Retrieved context from knowledge graph
            history: Conversation history
            max_new_tokens: Override max tokens
            temperature: Override temperature
            stream: Whether to stream the response
            
        Returns:
            Generated response string or generator for streaming
        """
        prompt = self.format_prompt(query, context, history)
        
        gen_kwargs = {
            "max_new_tokens": max_new_tokens or self.config.max_new_tokens,
            "temperature": temperature or self.config.temperature,
            "top_p": self.config.top_p,
            "top_k": self.config.top_k,
            "repetition_penalty": self.config.repetition_penalty,
            "do_sample": True,
            "pad_token_id": self.tokenizer.pad_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
        }
        
        if stream and THREADING_AVAILABLE:
            return self._generate_stream(prompt, gen_kwargs)
        else:
            return self._generate_sync(prompt, gen_kwargs)
            
    def _generate_sync(self, prompt: str, gen_kwargs: Dict) -> str:
        """Synchronous generation."""
        # Get model's max position embeddings (context length)
        max_length = getattr(self.model.config, 'max_position_embeddings', 32768)
        # Reserve space for generation (max_new_tokens)
        max_input_length = max_length - (gen_kwargs.get('max_new_tokens', 512))
        
        # Tokenize with truncation
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt",
            truncation=True,
            max_length=max_input_length
        ).to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                **gen_kwargs
            )
            
        # Decode only the new tokens
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return response.strip()
        
    def _generate_stream(self, prompt: str, gen_kwargs: Dict) -> Generator[str, None, None]:
        """Streaming generation."""
        # Get model's max position embeddings (context length)
        max_length = getattr(self.model.config, 'max_position_embeddings', 32768)
        # Reserve space for generation (max_new_tokens)
        max_input_length = max_length - (gen_kwargs.get('max_new_tokens', 512))
        
        # Tokenize with truncation
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt",
            truncation=True,
            max_length=max_input_length
        ).to(self.model.device)
        
        streamer = TextIteratorStreamer(
            self.tokenizer,
            skip_prompt=True,
            skip_special_tokens=True
        )
        
        gen_kwargs["streamer"] = streamer
        
        # Run generation in a separate thread
        thread = Thread(
            target=self.model.generate,
            kwargs={**inputs, **gen_kwargs}
        )
        thread.start()
        
        # Yield tokens as they're generated
        for text in streamer:
            yield text
            
        thread.join()
        
    def answer_with_reasoning(
        self,
        query: str,
        context: str,
        reasoning_steps: List[str] = None
    ) -> Dict:
        """
        Generate answer with reasoning explanation.
        
        Args:
            query: User's question
            context: Retrieved context
            reasoning_steps: Multi-hop reasoning steps
            
        Returns:
            Dictionary with answer and explanation
        """
        # Add reasoning context
        reasoning_context = ""
        if reasoning_steps:
            reasoning_context = "\n\n### QUÃ TRÃŒNH SUY LUáº¬N:\n"
            for i, step in enumerate(reasoning_steps, 1):
                reasoning_context += f"{i}. {step}\n"
                
        full_context = context + reasoning_context
        
        # Generate answer
        answer = self.generate(query, full_context)
        
        return {
            "query": query,
            "answer": answer,
            "reasoning_steps": reasoning_steps or [],
            "context_used": context[:500] + "..." if len(context) > 500 else context
        }
        
    def batch_generate(
        self,
        queries: List[str],
        contexts: List[str] = None,
        batch_size: int = 4
    ) -> List[str]:
        """
        Generate responses for multiple queries.
        
        Args:
            queries: List of questions
            contexts: List of contexts (one per query)
            batch_size: Batch size for generation
            
        Returns:
            List of generated responses
        """
        if contexts is None:
            contexts = [""] * len(queries)
            
        responses = []
        for i in range(0, len(queries), batch_size):
            batch_queries = queries[i:i + batch_size]
            batch_contexts = contexts[i:i + batch_size]
            
            for query, context in zip(batch_queries, batch_contexts):
                response = self.generate(query, context)
                responses.append(response)
                
        return responses
        
    def evaluate_yes_no(self, query: str, context: str) -> Dict:
        """
        Evaluate a Yes/No question.
        
        Returns:
            Dictionary with answer, confidence, and explanation
        """
        prompt = f"""Dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p, hÃ£y tráº£ lá»i cÃ¢u há»i sau vá»›i CÃ³ hoáº·c KhÃ´ng.
Chá»‰ tráº£ lá»i má»™t tá»«: CÃ³ hoáº·c KhÃ´ng.

CÃ¢u há»i: {query}"""

        response = self.generate(prompt, context, max_new_tokens=50, temperature=0.1)
        
        # Parse response
        response_lower = response.lower().strip()
        if "cÃ³" in response_lower or "yes" in response_lower or "Ä‘Ãºng" in response_lower:
            answer = "CÃ³"
            confidence = 0.9
        elif "khÃ´ng" in response_lower or "no" in response_lower or "sai" in response_lower:
            answer = "KhÃ´ng"
            confidence = 0.9
        else:
            answer = "KhÃ´ng cháº¯c cháº¯n"
            confidence = 0.5
            
        return {
            "query": query,
            "answer": answer,
            "confidence": confidence,
            "raw_response": response
        }
        
    def evaluate_multiple_choice(
        self,
        query: str,
        choices: List[str],
        context: str
    ) -> Dict:
        """
        Evaluate a multiple choice question.
        
        Returns:
            Dictionary with selected choice, confidence, and explanation
        """
        choices_str = "\n".join([f"{chr(65+i)}. {choice}" for i, choice in enumerate(choices)])
        
        prompt = f"""Dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p, hÃ£y chá»n Ä‘Ã¡p Ã¡n Ä‘Ãºng nháº¥t.
Chá»‰ tráº£ lá»i báº±ng má»™t chá»¯ cÃ¡i (A, B, C, D, ...).

CÃ¢u há»i: {query}

{choices_str}"""

        response = self.generate(prompt, context, max_new_tokens=50, temperature=0.1)
        
        # Parse response
        response_upper = response.upper().strip()
        selected_idx = None
        for i in range(len(choices)):
            letter = chr(65 + i)
            if letter in response_upper:
                selected_idx = i
                break
                
        if selected_idx is not None:
            return {
                "query": query,
                "selected_choice": choices[selected_idx],
                "selected_index": selected_idx,
                "selected_letter": chr(65 + selected_idx),
                "confidence": 0.85,
                "raw_response": response
            }
        else:
            return {
                "query": query,
                "selected_choice": None,
                "selected_index": None,
                "selected_letter": None,
                "confidence": 0.0,
                "raw_response": response
            }


# Fallback class when transformers is not available
class SmallLLMFallback:
    """Fallback LLM using rule-based responses."""
    
    def __init__(self, *args, **kwargs):
        print("âš ï¸ Using fallback LLM (rule-based). Install transformers for full functionality.")
        
    def generate(self, query: str, context: str = "", **kwargs) -> str:
        """Generate response using context extraction."""
        if not context:
            return "TÃ´i cáº§n thÃªm thÃ´ng tin tá»« Ä‘á»“ thá»‹ tri thá»©c Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i nÃ y."
            
        # Extract key information from context
        lines = context.split("\n")
        relevant_lines = [l for l in lines if l.strip().startswith("â€¢")]
        
        if relevant_lines:
            return "Dá»±a trÃªn thÃ´ng tin:\n" + "\n".join(relevant_lines[:5])
        else:
            return "ThÃ´ng tin liÃªn quan:\n" + context[:500]
            
    def evaluate_yes_no(self, query: str, context: str) -> Dict:
        return {"answer": "KhÃ´ng cháº¯c cháº¯n", "confidence": 0.0}
        
    def evaluate_multiple_choice(self, query: str, choices: List[str], context: str) -> Dict:
        return {"selected_choice": None, "confidence": 0.0}


def get_llm(model_key: str = "qwen2-0.5b", **kwargs):
    """Factory function to get LLM instance."""
    if TRANSFORMERS_AVAILABLE:
        return SmallLLM(model_key=model_key, **kwargs)
    else:
        return SmallLLMFallback(**kwargs)


def main():
    """Test the small LLM."""
    print("ğŸ”„ Testing Small LLM...")
    
    try:
        llm = get_llm("qwen2-0.5b")
        
        # Test generation
        context = """
=== THÃ”NG TIN THá»°C THá»‚ ===
ğŸ“ BTS (Loáº¡i: Group)
  â€¢ ThÃ nh viÃªn: RM, Jin, Suga, J-Hope, Jimin, V, Jungkook
  â€¢ NÄƒm hoáº¡t Ä‘á»™ng: 2013â€“nay
  â€¢ HÃ£ng Ä‘Ä©a: HYBE (Big Hit Entertainment)
  â€¢ Thá»ƒ loáº¡i: K-pop, hip hop, R&B

=== Sá»° KIá»†N ===
â€¢ BTS cÃ³ 7 thÃ nh viÃªn: RM, Jin, Suga, J-Hope, Jimin, V, Jungkook
â€¢ BTS thuá»™c cÃ´ng ty HYBE (trÆ°á»›c Ä‘Ã¢y lÃ  Big Hit Entertainment)
â€¢ BTS debut nÄƒm 2013
"""
        
        query = "BTS cÃ³ bao nhiÃªu thÃ nh viÃªn vÃ  há» lÃ  ai?"
        
        print(f"\nâ“ Query: {query}")
        print(f"ğŸ“ Context provided")
        
        response = llm.generate(query, context)
        print(f"\nğŸ¤– Response: {response}")
        
        # Test Yes/No
        print("\n" + "="*50)
        yn_result = llm.evaluate_yes_no(
            "BTS cÃ³ 7 thÃ nh viÃªn Ä‘Ãºng khÃ´ng?",
            context
        )
        print(f"Yes/No Answer: {yn_result['answer']}")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        print("ğŸ’¡ Try installing: pip install transformers torch accelerate")


if __name__ == "__main__":
    main()




