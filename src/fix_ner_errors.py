"""
Script Ä‘á»ƒ sá»­a cÃ¡c lá»—i NER trong merged_kpop_data.json

CÃ¡c lá»—i cáº§n sá»­a:
1. "Kpop BTS" â†’ "BTS" (loáº¡i bá» tiá»n tá»‘ "Kpop")
2. "Lee Su ji" â†’ Sá»­a type tá»« "Group" thÃ nh "Artist" (náº¿u Ä‘Ãºng)
"""

import json
import os
from typing import Dict, List


def fix_entity_name(entity_name: str) -> str:
    """Fix entity name by removing common prefixes."""
    # Remove "Kpop" prefix
    if entity_name.startswith("Kpop "):
        entity_name = entity_name[5:]  # Remove "Kpop "
    
    # Remove other common prefixes
    prefixes = ["K-pop ", "K-Pop ", "KPOP "]
    for prefix in prefixes:
        if entity_name.startswith(prefix):
            entity_name = entity_name[len(prefix):]
    
    return entity_name.strip()


def fix_entity_type(entity_name: str, current_type: str, nodes: Dict) -> str:
    """Fix entity type if wrong."""
    # Known corrections
    corrections = {
        "Lee Su ji": "Artist",  # Should be Artist, not Group
        # Add more corrections here
    }
    
    if entity_name in corrections:
        return corrections[entity_name]
    
    return current_type


def fix_merged_data(input_path: str = "data/merged_kpop_data.json", 
                   output_path: str = "data/merged_kpop_data_fixed.json",
                   backup: bool = True):
    """Fix errors in merged_kpop_data.json."""
    
    print("ğŸ”„ Äang load merged_kpop_data.json...")
    
    # Backup original file
    if backup and os.path.exists(input_path):
        backup_path = input_path + ".backup"
        import shutil
        shutil.copy2(input_path, backup_path)
        print(f"âœ… ÄÃ£ backup file gá»‘c: {backup_path}")
    
    # Load data
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    nodes = data.get('nodes', {})
    edges = data.get('edges', [])
    
    print(f"ğŸ“Š Loaded {len(nodes)} nodes and {len(edges)} edges")
    
    # Track fixes
    fixes = {
        'renamed_nodes': [],
        'type_corrections': [],
        'edge_updates': []
    }
    
    # Fix nodes
    print("\nğŸ”§ Äang sá»­a nodes...")
    new_nodes = {}
    node_mapping = {}  # old_name -> new_name
    nodes_to_merge = {}  # Track nodes that will be merged
    
    for node_id, node_data in nodes.items():
        new_id = fix_entity_name(node_id)
        new_type = fix_entity_type(new_id, node_data.get('label', ''), nodes)
        
        # Track changes
        if new_id != node_id:
            fixes['renamed_nodes'].append({
                'old': node_id,
                'new': new_id,
                'reason': 'Removed prefix'
            })
            node_mapping[node_id] = new_id
        
        if new_type != node_data.get('label', ''):
            fixes['type_corrections'].append({
                'entity': new_id,
                'old_type': node_data.get('label', ''),
                'new_type': new_type
            })
        
        # Check if node already exists (merge case - e.g., "Kpop BTS" and "BTS")
        if new_id in new_nodes:
            # Node Ä‘Ã£ tá»“n táº¡i - Ä‘Ã¡nh dáº¥u Ä‘á»ƒ xÃ³a node cÅ© (giá»¯ node gá»‘c)
            nodes_to_merge[node_id] = new_id
            print(f"   âš ï¸  Node trÃ¹ng: '{node_id}' â†’ '{new_id}' (sáº½ merge edges)")
        else:
            # Update node
            node_data['label'] = new_type
            node_data['title'] = new_id if 'title' not in node_data else fix_entity_name(node_data.get('title', new_id))
            new_nodes[new_id] = node_data
    
    if nodes_to_merge:
        print(f"   âš ï¸  PhÃ¡t hiá»‡n {len(nodes_to_merge)} nodes trÃ¹ng láº·p sáº½ Ä‘Æ°á»£c merge:")
        for old, new in list(nodes_to_merge.items())[:5]:
            print(f"      - '{old}' â†’ '{new}' (giá»¯ node '{new}')")
        if len(nodes_to_merge) > 5:
            print(f"      ... vÃ  {len(nodes_to_merge) - 5} nodes khÃ¡c")
    
    # Fix edges - update source and target names
    print("ğŸ”§ Äang sá»­a edges...")
    new_edges = []
    seen_edges = set()  # Track duplicate edges
    
    for edge in edges:
        source = edge.get('source', '')
        target = edge.get('target', '')
        
        # Map to new names (check both mapping and merge list)
        if source in nodes_to_merge:
            new_source = nodes_to_merge[source]
        else:
            new_source = node_mapping.get(source, fix_entity_name(source))
        
        if target in nodes_to_merge:
            new_target = nodes_to_merge[target]
        else:
            new_target = node_mapping.get(target, fix_entity_name(target))
        
        # Skip if edge is duplicate
        edge_key = (new_source, edge.get('type', ''), new_target)
        if edge_key in seen_edges:
            continue
        seen_edges.add(edge_key)
        
        if new_source != source or new_target != target:
            fixes['edge_updates'].append({
                'old': f"{source} -> {target}",
                'new': f"{new_source} -> {new_target}"
            })
        
        edge['source'] = new_source
        edge['target'] = new_target
        new_edges.append(edge)
    
    # Update data
    data['nodes'] = new_nodes
    data['edges'] = new_edges
    
    # Update metadata
    if 'metadata' not in data:
        data['metadata'] = {}
    data['metadata']['fixed_at'] = __import__('datetime').datetime.now().isoformat()
    data['metadata']['fixes_applied'] = {
        'renamed_nodes': len(fixes['renamed_nodes']),
        'type_corrections': len(fixes['type_corrections']),
        'edge_updates': len(fixes['edge_updates'])
    }
    
    # Save fixed data
    print(f"\nğŸ’¾ Äang lÆ°u file Ä‘Ã£ sá»­a: {output_path}")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    # Print summary
    print("\n" + "="*70)
    print("ğŸ“Š TÃ“M Táº®T CÃC Sá»¬A Äá»”I")
    print("="*70)
    print(f"\nâœ… ÄÃ£ Ä‘á»•i tÃªn {len(fixes['renamed_nodes'])} nodes:")
    for fix in fixes['renamed_nodes'][:10]:  # Show first 10
        print(f"   - '{fix['old']}' â†’ '{fix['new']}'")
    if len(fixes['renamed_nodes']) > 10:
        print(f"   ... vÃ  {len(fixes['renamed_nodes']) - 10} nodes khÃ¡c")
    
    print(f"\nâœ… ÄÃ£ sá»­a type cho {len(fixes['type_corrections'])} entities:")
    for fix in fixes['type_corrections']:
        print(f"   - '{fix['entity']}': {fix['old_type']} â†’ {fix['new_type']}")
    
    print(f"\nâœ… ÄÃ£ cáº­p nháº­t {len(fixes['edge_updates'])} edges")
    
    print(f"\nğŸ“ File Ä‘Ã£ sá»­a: {output_path}")
    print(f"ğŸ“ File backup: {input_path}.backup")
    print(f"\nğŸ’¡ Äá»ƒ sá»­ dá»¥ng file Ä‘Ã£ sá»­a, Ä‘á»•i tÃªn:")
    print(f"   {output_path} â†’ {input_path}")
    
    return fixes


def main():
    """Main function."""
    print("="*70)
    print("  ğŸ”§ FIX NER ERRORS IN MERGED DATA")
    print("="*70)
    print("\nScript nÃ y sáº½ sá»­a cÃ¡c lá»—i:")
    print("  1. Loáº¡i bá» tiá»n tá»‘ 'Kpop' (vÃ­ dá»¥: 'Kpop BTS' â†’ 'BTS')")
    print("  2. Sá»­a type sai (vÃ­ dá»¥: 'Lee Su ji' tá»« Group â†’ Artist)")
    print("\n" + "="*70)
    
    # Check if file exists
    input_path = "data/merged_kpop_data.json"
    if not os.path.exists(input_path):
        print(f"âŒ File khÃ´ng tá»“n táº¡i: {input_path}")
        return
    
    # Ask for confirmation
    print(f"\nâš ï¸  Sáº½ táº¡o file má»›i: data/merged_kpop_data_fixed.json")
    print(f"   File gá»‘c sáº½ Ä‘Æ°á»£c backup: {input_path}.backup")
    response = input("\nTiáº¿p tá»¥c? (y/n): ").strip().lower()
    
    if response != 'y':
        print("âŒ ÄÃ£ há»§y")
        return
    
    # Run fix
    fixes = fix_merged_data()
    
    print("\nâœ… HoÃ n thÃ nh!")
    print("\nğŸ’¡ BÆ°á»›c tiáº¿p theo:")
    print("   1. Kiá»ƒm tra file merged_kpop_data_fixed.json")
    print("   2. Náº¿u OK, Ä‘á»•i tÃªn: merged_kpop_data_fixed.json â†’ merged_kpop_data.json")
    print("   3. Cháº¡y láº¡i chatbot Ä‘á»ƒ test")


if __name__ == "__main__":
    main()

