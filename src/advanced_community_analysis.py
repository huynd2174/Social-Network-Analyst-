"""
Ph√¢n t√≠ch c·ªông ƒë·ªìng n√¢ng cao trong m·∫°ng x√£ h·ªôi K-pop

C√°c ph√¢n t√≠ch chuy√™n s√¢u:
1. So s√°nh nhi·ªÅu thu·∫≠t to√°n ph√°t hi·ªán c·ªông ƒë·ªìng
2. ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng c·ªông ƒë·ªìng (Internal Density, Conductance, NMI)
3. Ph√¢n t√≠ch c·∫•u tr√∫c c·ªông ƒë·ªìng (Hub nodes, Bridge nodes, Core-periphery)
4. Ph√¢n t√≠ch ng·ªØ nghƒ©a (Company communities, Generation communities)
5. Hierarchical community structure

Author: K-pop Social Network Analysis Team
"""

import sys
import io
import json
import math
import random
from typing import Dict, List, Any, Tuple, Set, Optional
from datetime import datetime
from collections import defaultdict

# Robust UTF-8 console output on Windows
if sys.platform == 'win32':
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    except Exception:
        pass

try:
    import networkx as nx
    from networkx.algorithms import community as nx_community
    NETWORKX_AVAILABLE = True
except ImportError:
    NETWORKX_AVAILABLE = False
    print("‚ö†Ô∏è  NetworkX ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Ch·∫°y: pip install networkx")

try:
    import matplotlib.pyplot as plt
    import matplotlib
    matplotlib.use('Agg')
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False


# =====================================================
# 1. MULTI-ALGORITHM COMMUNITY DETECTION
# =====================================================

def detect_communities_multi_algorithm(G: 'nx.Graph') -> Dict[str, List[Set]]:
    """
    Ph√°t hi·ªán c·ªông ƒë·ªìng b·∫±ng nhi·ªÅu thu·∫≠t to√°n kh√°c nhau ƒë·ªÉ so s√°nh.
    
    Returns:
        Dict v·ªõi key l√† t√™n thu·∫≠t to√°n, value l√† danh s√°ch c√°c communities
    """
    print("\n" + "=" * 70)
    print("üî¨ PH√ÅT HI·ªÜN C·ªòNG ƒê·ªíNG - ƒêA THU·∫¨T TO√ÅN")
    print("=" * 70)
    
    results = {}
    
    # 1. Louvain Algorithm (t·ªëi ∆∞u modularity)
    print("\n1Ô∏è‚É£  Thu·∫≠t to√°n LOUVAIN (Modularity Optimization)...")
    try:
        communities_louvain = list(nx_community.louvain_communities(G, seed=42))
        results['louvain'] = communities_louvain
        print(f"   ‚úì Ph√°t hi·ªán {len(communities_louvain)} c·ªông ƒë·ªìng")
    except Exception as e:
        print(f"   ‚úó L·ªói: {e}")
    
    # 2. Greedy Modularity Communities
    print("\n2Ô∏è‚É£  Thu·∫≠t to√°n GREEDY MODULARITY (CNM Algorithm)...")
    try:
        communities_greedy = list(nx_community.greedy_modularity_communities(G))
        results['greedy_modularity'] = communities_greedy
        print(f"   ‚úì Ph√°t hi·ªán {len(communities_greedy)} c·ªông ƒë·ªìng")
    except Exception as e:
        print(f"   ‚úó L·ªói: {e}")
    
    # 3. Label Propagation Algorithm (LPA)
    print("\n3Ô∏è‚É£  Thu·∫≠t to√°n LABEL PROPAGATION (LPA)...")
    try:
        communities_lpa = list(nx_community.label_propagation_communities(G))
        results['label_propagation'] = communities_lpa
        print(f"   ‚úì Ph√°t hi·ªán {len(communities_lpa)} c·ªông ƒë·ªìng")
    except Exception as e:
        print(f"   ‚úó L·ªói: {e}")
    
    # 4. Asynchronous Label Propagation
    print("\n4Ô∏è‚É£  Thu·∫≠t to√°n ASYNC LABEL PROPAGATION...")
    try:
        communities_async_lpa = list(nx_community.asyn_lpa_communities(G, seed=42))
        results['async_lpa'] = communities_async_lpa
        print(f"   ‚úì Ph√°t hi·ªán {len(communities_async_lpa)} c·ªông ƒë·ªìng")
    except Exception as e:
        print(f"   ‚úó L·ªói: {e}")
    
    # 5. Girvan-Newman (Edge Betweenness - ch·ªâ cho graph nh·ªè)
    if G.number_of_nodes() <= 500:
        print("\n5Ô∏è‚É£  Thu·∫≠t to√°n GIRVAN-NEWMAN (Edge Betweenness)...")
        try:
            # L·∫•y k communities (d·ª´ng sau 10 iterations)
            gn_generator = nx_community.girvan_newman(G)
            communities_gn = None
            best_modularity = -1
            for i in range(min(10, G.number_of_nodes() // 10)):
                try:
                    communities_iter = next(gn_generator)
                    mod = nx_community.modularity(G, communities_iter)
                    if mod > best_modularity:
                        best_modularity = mod
                        communities_gn = list(communities_iter)
                except StopIteration:
                    break
            if communities_gn:
                results['girvan_newman'] = communities_gn
                print(f"   ‚úì Ph√°t hi·ªán {len(communities_gn)} c·ªông ƒë·ªìng (best modularity)")
        except Exception as e:
            print(f"   ‚úó L·ªói: {e}")
    else:
        print("\n5Ô∏è‚É£  Girvan-Newman: B·ªè qua (graph qu√° l·ªõn, > 500 nodes)")
    
    # 6. K-Clique Communities (overlapping)
    print("\n6Ô∏è‚É£  Thu·∫≠t to√°n K-CLIQUE PERCOLATION (Overlapping)...")
    try:
        # Th·ª≠ k=3 (triangles)
        communities_kclique = list(nx_community.k_clique_communities(G, 3))
        if communities_kclique:
            results['k_clique_3'] = communities_kclique
            print(f"   ‚úì Ph√°t hi·ªán {len(communities_kclique)} c·ªông ƒë·ªìng (k=3, c√≥ th·ªÉ ch·ªìng l·∫•p)")
        else:
            print(f"   ‚úó Kh√¥ng t√¨m th·∫•y k-clique communities (k=3)")
    except Exception as e:
        print(f"   ‚úó L·ªói: {e}")
    
    return results


# =====================================================
# 2. COMMUNITY QUALITY METRICS
# =====================================================

def calculate_internal_density(G: 'nx.Graph', community: Set) -> float:
    """
    T√≠nh Internal Density c·ªßa m·ªôt c·ªông ƒë·ªìng.
    
    Internal Density = 2 * m_c / (n_c * (n_c - 1))
    
    Trong ƒë√≥:
    - m_c: s·ªë edges b√™n trong c·ªông ƒë·ªìng
    - n_c: s·ªë nodes trong c·ªông ƒë·ªìng
    """
    subgraph = G.subgraph(community)
    n_c = subgraph.number_of_nodes()
    m_c = subgraph.number_of_edges()
    
    if n_c <= 1:
        return 0.0
    
    max_edges = n_c * (n_c - 1) / 2
    return m_c / max_edges if max_edges > 0 else 0.0


def calculate_external_density(G: 'nx.Graph', community: Set) -> float:
    """
    T√≠nh External Density - t·ª∑ l·ªá edges n·ªëi ra b√™n ngo√†i c·ªông ƒë·ªìng.
    
    External Density = edges_out / (n_c * (N - n_c))
    """
    n_c = len(community)
    N = G.number_of_nodes()
    
    if n_c == 0 or n_c == N:
        return 0.0
    
    edges_out = 0
    for node in community:
        for neighbor in G.neighbors(node):
            if neighbor not in community:
                edges_out += 1
    
    max_external_edges = n_c * (N - n_c)
    return edges_out / max_external_edges if max_external_edges > 0 else 0.0


def calculate_conductance(G: 'nx.Graph', community: Set) -> float:
    """
    T√≠nh Conductance c·ªßa m·ªôt c·ªông ƒë·ªìng.
    
    Conductance = cut(S, SÃÑ) / min(vol(S), vol(SÃÑ))
    
    - cut(S, SÃÑ): s·ªë edges c·∫Øt gi·ªØa community v√† ph·∫ßn c√≤n l·∫°i
    - vol(S): t·ªïng degree c·ªßa nodes trong S
    
    Conductance th·∫•p = c·ªông ƒë·ªìng t·ªët (√≠t li√™n k·∫øt ra ngo√†i)
    """
    if len(community) == 0 or len(community) == G.number_of_nodes():
        return 0.0
    
    cut = 0
    vol_s = 0
    vol_not_s = 0
    
    for node in G.nodes():
        degree = G.degree(node)
        if node in community:
            vol_s += degree
            for neighbor in G.neighbors(node):
                if neighbor not in community:
                    cut += 1
        else:
            vol_not_s += degree
    
    min_vol = min(vol_s, vol_not_s)
    return cut / min_vol if min_vol > 0 else 0.0


def calculate_cut_ratio(G: 'nx.Graph', community: Set) -> float:
    """
    T√≠nh Cut Ratio c·ªßa m·ªôt c·ªông ƒë·ªìng.
    
    Cut Ratio = cut(S, SÃÑ) / (|S| * |SÃÑ|)
    """
    n_s = len(community)
    n_not_s = G.number_of_nodes() - n_s
    
    if n_s == 0 or n_not_s == 0:
        return 0.0
    
    cut = 0
    for node in community:
        for neighbor in G.neighbors(node):
            if neighbor not in community:
                cut += 1
    
    return cut / (n_s * n_not_s)


def evaluate_community_quality(G: 'nx.Graph', communities: List[Set]) -> Dict[str, Any]:
    """
    ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng to√†n di·ªán c·ªßa c√°c c·ªông ƒë·ªìng.
    """
    print("\n" + "=" * 70)
    print("üìä ƒê√ÅNH GI√Å CH·∫§T L∆Ø·ª¢NG C·ªòNG ƒê·ªíNG")
    print("=" * 70)
    
    results = {
        'num_communities': len(communities),
        'communities_detail': []
    }
    
    # T√≠nh Modularity
    try:
        modularity = nx_community.modularity(G, communities)
        results['modularity'] = modularity
        print(f"\n‚úì Modularity (Q): {modularity:.4f}")
        
        if modularity > 0.7:
            print("   ‚Üí Modularity > 0.7: C·∫•u tr√∫c c·ªông ƒë·ªìng R·∫§T M·∫†NH")
        elif modularity > 0.5:
            print("   ‚Üí Modularity > 0.5: C·∫•u tr√∫c c·ªông ƒë·ªìng M·∫†NH")
        elif modularity > 0.3:
            print("   ‚Üí Modularity > 0.3: C·∫•u tr√∫c c·ªông ƒë·ªìng R√ï R√ÄNG")
        elif modularity > 0.1:
            print("   ‚Üí Modularity > 0.1: C·∫•u tr√∫c c·ªông ƒë·ªìng TRUNG B√åNH")
        else:
            print("   ‚Üí Modularity ‚â§ 0.1: C·∫•u tr√∫c c·ªông ƒë·ªìng Y·∫æU")
    except Exception as e:
        print(f"‚úó Kh√¥ng th·ªÉ t√≠nh Modularity: {e}")
    
    # T√≠nh Coverage (t·ª∑ l·ªá edges n·ªôi b·ªô)
    total_internal_edges = 0
    for comm in communities:
        subgraph = G.subgraph(comm)
        total_internal_edges += subgraph.number_of_edges()
    
    coverage = total_internal_edges / G.number_of_edges() if G.number_of_edges() > 0 else 0
    results['coverage'] = coverage
    print(f"‚úì Coverage: {coverage:.4f} ({100*coverage:.1f}% edges n·∫±m trong c√°c c·ªông ƒë·ªìng)")
    
    # T√≠nh metrics chi ti·∫øt cho t·ª´ng c·ªông ƒë·ªìng l·ªõn
    print(f"\nüìä METRICS CHI TI·∫æT CHO TOP 10 C·ªòNG ƒê·ªíNG:")
    print("-" * 90)
    print(f"{'#':<4} {'Size':<8} {'Int.Dens':<12} {'Ext.Dens':<12} {'Conductance':<14} {'Cut Ratio':<12}")
    print("-" * 90)
    
    sorted_communities = sorted(communities, key=len, reverse=True)
    
    all_internal_densities = []
    all_conductances = []
    
    for i, comm in enumerate(sorted_communities[:10], 1):
        int_dens = calculate_internal_density(G, comm)
        ext_dens = calculate_external_density(G, comm)
        conductance = calculate_conductance(G, comm)
        cut_ratio = calculate_cut_ratio(G, comm)
        
        all_internal_densities.append(int_dens)
        all_conductances.append(conductance)
        
        print(f"{i:<4} {len(comm):<8} {int_dens:<12.4f} {ext_dens:<12.4f} {conductance:<14.4f} {cut_ratio:<12.6f}")
        
        results['communities_detail'].append({
            'rank': i,
            'size': len(comm),
            'internal_density': int_dens,
            'external_density': ext_dens,
            'conductance': conductance,
            'cut_ratio': cut_ratio
        })
    
    # Th·ªëng k√™ t·ªïng h·ª£p
    print(f"\nüìä TH·ªêNG K√ä T·ªîNG H·ª¢P:")
    print("-" * 50)
    
    if all_internal_densities:
        avg_int_dens = sum(all_internal_densities) / len(all_internal_densities)
        results['avg_internal_density'] = avg_int_dens
        print(f"   - Internal Density trung b√¨nh (top 10): {avg_int_dens:.4f}")
    
    if all_conductances:
        avg_conductance = sum(all_conductances) / len(all_conductances)
        results['avg_conductance'] = avg_conductance
        print(f"   - Conductance trung b√¨nh (top 10): {avg_conductance:.4f}")
        
        if avg_conductance < 0.3:
            print("   ‚Üí Conductance th·∫•p: C√°c c·ªông ƒë·ªìng ƒë∆∞·ª£c ph√¢n t√°ch t·ªët")
        elif avg_conductance < 0.5:
            print("   ‚Üí Conductance trung b√¨nh: C√°c c·ªông ƒë·ªìng c√≥ m·ªôt s·ªë li√™n k·∫øt ra ngo√†i")
        else:
            print("   ‚Üí Conductance cao: Ranh gi·ªõi c·ªông ƒë·ªìng kh√¥ng r√µ r√†ng")
    
    return results


def compare_algorithms(G: 'nx.Graph', communities_dict: Dict[str, List[Set]]) -> Dict[str, Any]:
    """
    So s√°nh ch·∫•t l∆∞·ª£ng c·ªßa c√°c thu·∫≠t to√°n ph√°t hi·ªán c·ªông ƒë·ªìng.
    """
    print("\n" + "=" * 70)
    print("üìä SO S√ÅNH C√ÅC THU·∫¨T TO√ÅN")
    print("=" * 70)
    
    comparison = {}
    
    print(f"\n{'Thu·∫≠t to√°n':<25} {'S·ªë Cƒê':<10} {'Modularity':<12} {'Coverage':<12} {'Max Size':<10}")
    print("-" * 80)
    
    for algo_name, communities in communities_dict.items():
        try:
            modularity = nx_community.modularity(G, communities)
            
            # Coverage
            total_internal = sum(G.subgraph(c).number_of_edges() for c in communities)
            coverage = total_internal / G.number_of_edges() if G.number_of_edges() > 0 else 0
            
            # Max size
            max_size = max(len(c) for c in communities) if communities else 0
            
            comparison[algo_name] = {
                'num_communities': len(communities),
                'modularity': modularity,
                'coverage': coverage,
                'max_community_size': max_size
            }
            
            print(f"{algo_name:<25} {len(communities):<10} {modularity:<12.4f} {coverage:<12.4f} {max_size:<10}")
        except Exception as e:
            print(f"{algo_name:<25} L·ªói: {e}")
    
    # T√¨m thu·∫≠t to√°n t·ªët nh·∫•t theo Modularity
    if comparison:
        best_algo = max(comparison.items(), key=lambda x: x[1].get('modularity', 0))
        print(f"\nüèÜ Thu·∫≠t to√°n t·ªët nh·∫•t (theo Modularity): {best_algo[0]} (Q={best_algo[1]['modularity']:.4f})")
    
    return comparison


# =====================================================
# 3. COMMUNITY STRUCTURE ANALYSIS
# =====================================================

def analyze_community_structure(G: 'nx.Graph', communities: List[Set], top_k: int = 5) -> Dict[str, Any]:
    """
    Ph√¢n t√≠ch c·∫•u tr√∫c chi ti·∫øt c·ªßa c√°c c·ªông ƒë·ªìng:
    - Hub nodes (nodes quan tr·ªçng nh·∫•t trong m·ªói community)
    - Bridge nodes (nodes k·∫øt n·ªëi gi·ªØa c√°c communities)
    - Core-periphery structure
    """
    print("\n" + "=" * 70)
    print("üî¨ PH√ÇN T√çCH C·∫§U TR√öC C·ªòNG ƒê·ªíNG")
    print("=" * 70)
    
    results = {
        'hub_nodes': [],
        'bridge_nodes': [],
        'core_periphery': []
    }
    
    # T·∫°o mapping node -> community index
    node_to_community = {}
    for i, comm in enumerate(communities):
        for node in comm:
            node_to_community[node] = i
    
    # 1. T√åM HUB NODES TRONG M·ªñI COMMUNITY
    print(f"\nüìä HUB NODES (TOP {top_k} NODES QUAN TR·ªåNG NH·∫§T TRONG M·ªñI C·ªòNG ƒê·ªíNG L·ªöN):")
    print("-" * 70)
    
    sorted_communities = sorted(communities, key=len, reverse=True)
    
    for i, comm in enumerate(sorted_communities[:5], 1):
        subgraph = G.subgraph(comm)
        
        # T√≠nh PageRank trong subgraph
        try:
            pr = nx.pagerank(subgraph)
            top_hubs = sorted(pr.items(), key=lambda x: x[1], reverse=True)[:3]
            
            print(f"\nüîπ C·ªông ƒë·ªìng {i} ({len(comm)} nodes):")
            for j, (node, score) in enumerate(top_hubs, 1):
                label = G.nodes[node].get('label', 'Unknown')
                print(f"   {j}. {node} ({label}) - Score: {score:.4f}")
            
            results['hub_nodes'].append({
                'community_id': i,
                'size': len(comm),
                'hubs': [{'node': n, 'score': s, 'label': G.nodes[n].get('label', 'Unknown')} for n, s in top_hubs]
            })
        except:
            pass
    
    # 2. T√åM BRIDGE NODES (n√∫t c·∫ßu n·ªëi gi·ªØa c√°c communities)
    print(f"\nüìä BRIDGE NODES (N√öT C·∫¶U N·ªêI GI·ªÆA C√ÅC C·ªòNG ƒê·ªíNG):")
    print("-" * 70)
    
    bridge_scores = {}
    for node in G.nodes():
        if node not in node_to_community:
            continue
            
        my_comm = node_to_community[node]
        external_connections = defaultdict(int)
        
        for neighbor in G.neighbors(node):
            if neighbor in node_to_community:
                neighbor_comm = node_to_community[neighbor]
                if neighbor_comm != my_comm:
                    external_connections[neighbor_comm] += 1
        
        if external_connections:
            # Bridge score = s·ªë communities kh√°c ƒë∆∞·ª£c k·∫øt n·ªëi * s·ªë connections
            bridge_scores[node] = {
                'communities_connected': len(external_connections),
                'total_external_edges': sum(external_connections.values()),
                'own_community': my_comm
            }
    
    # S·∫Øp x·∫øp theo s·ªë communities ƒë∆∞·ª£c k·∫øt n·ªëi
    top_bridges = sorted(
        bridge_scores.items(), 
        key=lambda x: (x[1]['communities_connected'], x[1]['total_external_edges']), 
        reverse=True
    )[:10]
    
    print(f"{'Node':<40} {'Label':<15} {'# Cƒê k·∫øt n·ªëi':<15} {'# Edges ngo√†i':<15}")
    print("-" * 85)
    
    for node, info in top_bridges:
        label = G.nodes[node].get('label', 'Unknown')
        print(f"{node[:38]:<40} {label:<15} {info['communities_connected']:<15} {info['total_external_edges']:<15}")
    
    results['bridge_nodes'] = [
        {
            'node': node,
            'label': G.nodes[node].get('label', 'Unknown'),
            'communities_connected': info['communities_connected'],
            'total_external_edges': info['total_external_edges']
        }
        for node, info in top_bridges
    ]
    
    # 3. CORE-PERIPHERY ANALYSIS
    print(f"\nüìä CORE-PERIPHERY ANALYSIS (C·∫§U TR√öC L√ïI-NGO·∫†I VI):")
    print("-" * 70)
    
    for i, comm in enumerate(sorted_communities[:3], 1):
        subgraph = G.subgraph(comm)
        
        # Ph√¢n lo·∫°i nodes: Core (degree cao), Periphery (degree th·∫•p)
        degrees = dict(subgraph.degree())
        avg_degree = sum(degrees.values()) / len(degrees) if degrees else 0
        
        core_nodes = [n for n, d in degrees.items() if d >= avg_degree]
        periphery_nodes = [n for n, d in degrees.items() if d < avg_degree]
        
        print(f"\nüîπ C·ªông ƒë·ªìng {i} ({len(comm)} nodes):")
        print(f"   - Core nodes (degree ‚â• {avg_degree:.1f}): {len(core_nodes)} nodes ({100*len(core_nodes)/len(comm):.1f}%)")
        print(f"   - Periphery nodes: {len(periphery_nodes)} nodes ({100*len(periphery_nodes)/len(comm):.1f}%)")
        
        # Hi·ªÉn th·ªã m·ªôt s·ªë core nodes
        top_core = sorted(core_nodes, key=lambda n: degrees[n], reverse=True)[:3]
        print(f"   - Top core nodes: {', '.join([f'{n} (deg={degrees[n]})' for n in top_core])}")
        
        results['core_periphery'].append({
            'community_id': i,
            'size': len(comm),
            'core_count': len(core_nodes),
            'periphery_count': len(periphery_nodes),
            'avg_degree': avg_degree,
            'top_core_nodes': [{'node': n, 'degree': degrees[n]} for n in top_core]
        })
    
    return results


# =====================================================
# 4. SEMANTIC COMMUNITY ANALYSIS
# =====================================================

def analyze_semantic_communities(G: 'nx.Graph', communities: List[Set]) -> Dict[str, Any]:
    """
    Ph√¢n t√≠ch ng·ªØ nghƒ©a c·ªßa c√°c c·ªông ƒë·ªìng:
    - X√°c ƒë·ªãnh "ch·ªß ƒë·ªÅ" c·ªßa t·ª´ng community
    - T√¨m company-based communities
    - T√¨m genre-based communities
    - T√¨m generation-based communities
    """
    print("\n" + "=" * 70)
    print("üéØ PH√ÇN T√çCH NG·ªÆ NGHƒ®A C·ªòNG ƒê·ªíNG")
    print("=" * 70)
    
    results = {
        'company_communities': [],
        'genre_communities': [],
        'group_centric_communities': []
    }
    
    sorted_communities = sorted(communities, key=len, reverse=True)
    
    # 1. PH√ÇN LO·∫†I C·ªòNG ƒê·ªíNG THEO N·ªòI DUNG
    print(f"\nüìä PH√ÇN LO·∫†I C·ªòNG ƒê·ªíNG THEO N·ªòI DUNG CH√çNH:")
    print("-" * 70)
    
    for i, comm in enumerate(sorted_communities[:15], 1):
        # ƒê·∫øm theo label
        label_counts = defaultdict(int)
        companies = []
        groups = []
        genres = []
        
        for node in comm:
            label = G.nodes[node].get('label', 'Unknown')
            label_counts[label] += 1
            
            if label == 'Company':
                companies.append(node)
            elif label == 'Group':
                groups.append(node)
            elif label == 'Genre':
                genres.append(node)
        
        # X√°c ƒë·ªãnh lo·∫°i c·ªông ƒë·ªìng
        total = len(comm)
        dominant_label, dominant_count = max(label_counts.items(), key=lambda x: x[1])
        dominant_pct = 100 * dominant_count / total
        
        # Ph√¢n lo·∫°i
        comm_type = "Mixed"
        main_entity = None
        
        if companies:
            # T√¨m company ph·ªï bi·∫øn nh·∫•t (d·ª±a tr√™n s·ªë connections)
            company_connections = {}
            for company in companies:
                connections = sum(1 for n in G.neighbors(company) if n in comm)
                company_connections[company] = connections
            if company_connections:
                main_company = max(company_connections.items(), key=lambda x: x[1])[0]
                comm_type = "Company-based"
                main_entity = main_company.replace("Company_", "")
                results['company_communities'].append({
                    'rank': i,
                    'size': len(comm),
                    'main_company': main_entity,
                    'groups': [g for g in groups[:5]],
                    'label_distribution': dict(label_counts)
                })
        
        if groups and comm_type == "Mixed":
            # T√¨m group ch√≠nh
            group_degrees = {g: G.degree(g) for g in groups}
            main_group = max(group_degrees.items(), key=lambda x: x[1])[0]
            comm_type = "Group-centric"
            main_entity = main_group
            results['group_centric_communities'].append({
                'rank': i,
                'size': len(comm),
                'main_group': main_entity,
                'label_distribution': dict(label_counts)
            })
        
        if genres and len(genres) >= 3:
            comm_type = "Genre-based"
            main_entity = ", ".join(g.replace("Genre_", "") for g in genres[:3])
            results['genre_communities'].append({
                'rank': i,
                'size': len(comm),
                'genres': [g.replace("Genre_", "") for g in genres],
                'label_distribution': dict(label_counts)
            })
        
        print(f"\nüîπ C·ªông ƒë·ªìng {i} ({len(comm)} nodes) - {comm_type}")
        print(f"   - Label ch·ªß ƒë·∫°o: {dominant_label} ({dominant_count} nodes, {dominant_pct:.1f}%)")
        if main_entity:
            print(f"   - Th·ª±c th·ªÉ ch√≠nh: {main_entity}")
        print(f"   - Ph√¢n b·ªë: {dict(label_counts)}")
        
        # Sample nodes
        sample_by_label = {}
        for node in comm:
            label = G.nodes[node].get('label', 'Unknown')
            if label not in sample_by_label:
                sample_by_label[label] = node
            if len(sample_by_label) >= 4:
                break
        print(f"   - M·∫´u: {list(sample_by_label.values())[:4]}")
    
    # 2. T√åM C√ÅC COMPANY CLUSTERS
    print(f"\nüìä COMPANY CLUSTERS (Ngh·ªá sƒ© theo c√¥ng ty):")
    print("-" * 70)
    
    # Nh√≥m c√°c artists theo c√¥ng ty
    company_artists = defaultdict(list)
    for node in G.nodes():
        if G.nodes[node].get('label') == 'Artist':
            for neighbor in G.neighbors(node):
                if G.nodes[neighbor].get('label') == 'Company':
                    company_artists[neighbor].append(node)
    
    # Top 5 c√¥ng ty c√≥ nhi·ªÅu ngh·ªá sƒ© nh·∫•t
    top_companies = sorted(company_artists.items(), key=lambda x: len(x[1]), reverse=True)[:5]
    
    for company, artists in top_companies:
        company_name = company.replace("Company_", "")
        print(f"\nüè¢ {company_name}: {len(artists)} ngh·ªá sƒ©")
        print(f"   M·∫´u: {', '.join(artists[:5])}")
    
    results['top_companies_by_artists'] = [
        {'company': c.replace("Company_", ""), 'artist_count': len(a), 'sample_artists': a[:5]}
        for c, a in top_companies
    ]
    
    return results


# =====================================================
# 5. HIERARCHICAL COMMUNITY STRUCTURE
# =====================================================

def analyze_hierarchical_structure(G: 'nx.Graph', max_levels: int = 3) -> Dict[str, Any]:
    """
    Ph√¢n t√≠ch c·∫•u tr√∫c ph√¢n c·∫•p c·ªßa c√°c c·ªông ƒë·ªìng.
    S·ª≠ d·ª•ng Girvan-Newman ƒë·ªÉ t·∫°o dendrogram.
    """
    print("\n" + "=" * 70)
    print("üå≥ PH√ÇN T√çCH C·∫§U TR√öC PH√ÇN C·∫§P (HIERARCHICAL)")
    print("=" * 70)
    
    results = {'levels': []}
    
    if G.number_of_nodes() > 500:
        print("\n‚ö†Ô∏è  Graph qu√° l·ªõn cho ph√¢n t√≠ch hierarchical. S·ª≠ d·ª•ng subgraph c·ªßa largest connected component (max 500 nodes)...")
        
        # L·∫•y largest connected component
        if nx.is_connected(G):
            subG = G
        else:
            largest_cc = max(nx.connected_components(G), key=len)
            subG = G.subgraph(largest_cc).copy()
        
        # Sample n·∫øu v·∫´n qu√° l·ªõn
        if subG.number_of_nodes() > 500:
            sample_nodes = random.sample(list(subG.nodes()), 500)
            subG = G.subgraph(sample_nodes).copy()
    else:
        subG = G
    
    print(f"\nüîç Ph√¢n t√≠ch tr√™n {subG.number_of_nodes()} nodes...")
    
    try:
        # S·ª≠ d·ª•ng Louvain v·ªõi resolution kh√°c nhau ƒë·ªÉ m√¥ ph·ªèng hierarchical
        resolutions = [0.5, 1.0, 1.5, 2.0]
        
        for res in resolutions:
            try:
                communities = list(nx_community.louvain_communities(subG, resolution=res, seed=42))
                modularity = nx_community.modularity(subG, communities)
                
                print(f"\nüìä Resolution = {res}:")
                print(f"   - S·ªë c·ªông ƒë·ªìng: {len(communities)}")
                print(f"   - Modularity: {modularity:.4f}")
                print(f"   - K√≠ch th∆∞·ªõc: {[len(c) for c in sorted(communities, key=len, reverse=True)[:5]]}")
                
                results['levels'].append({
                    'resolution': res,
                    'num_communities': len(communities),
                    'modularity': modularity,
                    'sizes': [len(c) for c in sorted(communities, key=len, reverse=True)[:10]]
                })
            except Exception as e:
                print(f"   ‚úó L·ªói t·∫°i resolution {res}: {e}")
        
        # K·∫øt lu·∫≠n
        print(f"\nüìã PH√ÇN T√çCH HIERARCHICAL:")
        print("-" * 50)
        print("   - Resolution th·∫•p ‚Üí √≠t c·ªông ƒë·ªìng l·ªõn (macro-level)")
        print("   - Resolution cao ‚Üí nhi·ªÅu c·ªông ƒë·ªìng nh·ªè (micro-level)")
        
        if results['levels']:
            best_level = max(results['levels'], key=lambda x: x['modularity'])
            print(f"\n   üèÜ Resolution t·ªëi ∆∞u: {best_level['resolution']} (Modularity = {best_level['modularity']:.4f})")
    
    except Exception as e:
        print(f"‚úó L·ªói khi ph√¢n t√≠ch hierarchical: {e}")
    
    return results


# =====================================================
# 6. VISUALIZATION
# =====================================================

def visualize_community_analysis(G: 'nx.Graph', communities: List[Set], 
                                  output_dir: str = "outputs") -> None:
    """
    T·∫°o c√°c bi·ªÉu ƒë·ªì visualization cho ph√¢n t√≠ch c·ªông ƒë·ªìng.
    """
    if not MATPLOTLIB_AVAILABLE:
        print("‚ö†Ô∏è  Matplotlib kh√¥ng kh·∫£ d·ª•ng, b·ªè qua visualization")
        return
    
    import os
    os.makedirs(output_dir, exist_ok=True)
    
    print("\n" + "=" * 70)
    print("üìä T·∫†O BI·ªÇU ƒê·ªí VISUALIZATION")
    print("=" * 70)
    
    # 1. Community Size Distribution
    fig, axes = plt.subplots(2, 2, figsize=(14, 12))
    
    # 1.1 Histogram of community sizes
    sizes = [len(c) for c in communities]
    ax1 = axes[0, 0]
    ax1.hist(sizes, bins=min(50, len(sizes)), edgecolor='black', alpha=0.7, color='steelblue')
    ax1.set_xlabel('Community Size')
    ax1.set_ylabel('Frequency')
    ax1.set_title('Ph√¢n b·ªë K√≠ch th∆∞·ªõc C·ªông ƒë·ªìng')
    ax1.grid(True, alpha=0.3)
    
    # 1.2 Top 20 communities bar chart
    ax2 = axes[0, 1]
    top_20_sizes = sorted(sizes, reverse=True)[:20]
    ax2.bar(range(1, len(top_20_sizes) + 1), top_20_sizes, color='coral')
    ax2.set_xlabel('Community Rank')
    ax2.set_ylabel('Size')
    ax2.set_title('Top 20 C·ªông ƒë·ªìng L·ªõn nh·∫•t')
    ax2.grid(True, alpha=0.3, axis='y')
    
    # 1.3 Cumulative distribution
    ax3 = axes[1, 0]
    sorted_sizes = sorted(sizes, reverse=True)
    cumulative = [sum(sorted_sizes[:i+1]) / sum(sizes) * 100 for i in range(len(sorted_sizes))]
    ax3.plot(range(1, len(cumulative) + 1), cumulative, 'b-', linewidth=2)
    ax3.axhline(y=80, color='r', linestyle='--', label='80% coverage')
    ax3.set_xlabel('Number of Communities')
    ax3.set_ylabel('Cumulative % of Nodes')
    ax3.set_title('Ph√¢n b·ªë T√≠ch l≈©y')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 1.4 Label distribution in top communities
    ax4 = axes[1, 1]
    sorted_communities = sorted(communities, key=len, reverse=True)[:5]
    
    labels_data = defaultdict(list)
    for i, comm in enumerate(sorted_communities):
        label_counts = defaultdict(int)
        for node in comm:
            label = G.nodes[node].get('label', 'Unknown')
            label_counts[label] += 1
        for label, count in label_counts.items():
            labels_data[label].append(count)
        for label in labels_data:
            if len(labels_data[label]) < i + 1:
                labels_data[label].append(0)
    
    x = range(1, 6)
    bottom = [0] * 5
    colors = plt.cm.tab10(range(len(labels_data)))
    
    for (label, counts), color in zip(labels_data.items(), colors):
        while len(counts) < 5:
            counts.append(0)
        ax4.bar(x, counts, bottom=bottom, label=label, color=color)
        bottom = [b + c for b, c in zip(bottom, counts)]
    
    ax4.set_xlabel('Community Rank')
    ax4.set_ylabel('Number of Nodes')
    ax4.set_title('Ph√¢n b·ªë Label trong Top 5 C·ªông ƒë·ªìng')
    ax4.legend(loc='upper right', fontsize=8)
    
    plt.tight_layout()
    output_path = f"{output_dir}/community_analysis_advanced.png"
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"‚úì ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: {output_path}")
    plt.close()


# =====================================================
# MAIN FUNCTION
# =====================================================

def main():
    """H√†m main ch·∫°y ph√¢n t√≠ch c·ªông ƒë·ªìng n√¢ng cao"""
    
    print("\n" + "=" * 70)
    print("üéµ PH√ÇN T√çCH C·ªòNG ƒê·ªíNG N√ÇNG CAO - M·∫†NG X√É H·ªòI K-POP üéµ")
    print("=" * 70)
    print(f"Th·ªùi gian: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    if not NETWORKX_AVAILABLE:
        print("\n‚ùå Kh√¥ng th·ªÉ ch·∫°y ph√¢n t√≠ch v√¨ NetworkX ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t")
        return
    
    # Load d·ªØ li·ªáu
    print("\nüîÑ ƒêang load d·ªØ li·ªáu...")
    try:
        with open("data/korean_artists_graph_bfs.json", 'r', encoding='utf-8') as f:
            data = json.load(f)
        nodes = data.get('nodes', {})
        edges = data.get('edges', [])
        print(f"‚úì ƒê√£ load {len(nodes)} nodes v√† {len(edges)} edges")
    except FileNotFoundError:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu")
        return
    
    # Build graph
    print("\nüîß ƒêang x√¢y d·ª±ng NetworkX graph...")
    G = nx.Graph()
    for node_id, node_data in nodes.items():
        G.add_node(node_id, **{
            'label': node_data.get('label', 'Entity'),
            'title': node_data.get('title', node_id)
        })
    for edge in edges:
        src, tgt = edge.get('source'), edge.get('target')
        if src and tgt and src in nodes and tgt in nodes:
            G.add_edge(src, tgt, type=edge.get('type', 'RELATED_TO'))
    
    print(f"‚úì Graph c√≥ {G.number_of_nodes()} nodes v√† {G.number_of_edges()} edges")
    
    # K·∫øt qu·∫£ t·ªïng h·ª£p
    all_results = {
        'analysis_time': datetime.now().isoformat(),
        'graph_info': {
            'nodes': G.number_of_nodes(),
            'edges': G.number_of_edges()
        }
    }
    
    # 1. Ph√°t hi·ªán c·ªông ƒë·ªìng b·∫±ng nhi·ªÅu thu·∫≠t to√°n
    communities_dict = detect_communities_multi_algorithm(G)
    all_results['algorithms'] = {algo: len(comms) for algo, comms in communities_dict.items()}
    
    # 2. So s√°nh c√°c thu·∫≠t to√°n
    comparison = compare_algorithms(G, communities_dict)
    all_results['comparison'] = comparison
    
    # 3. Ch·ªçn thu·∫≠t to√°n t·ªët nh·∫•t v√† ph√¢n t√≠ch chi ti·∫øt
    best_algo = max(comparison.items(), key=lambda x: x[1].get('modularity', 0))
    best_communities = communities_dict[best_algo[0]]
    
    print(f"\nüèÜ S·ª≠ d·ª•ng k·∫øt qu·∫£ t·ª´ {best_algo[0]} cho ph√¢n t√≠ch chi ti·∫øt...")
    
    # 4. ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng
    quality = evaluate_community_quality(G, best_communities)
    all_results['quality'] = quality
    
    # 5. Ph√¢n t√≠ch c·∫•u tr√∫c
    structure = analyze_community_structure(G, best_communities)
    all_results['structure'] = structure
    
    # 6. Ph√¢n t√≠ch ng·ªØ nghƒ©a
    semantic = analyze_semantic_communities(G, best_communities)
    all_results['semantic'] = semantic
    
    # 7. Ph√¢n t√≠ch hierarchical
    hierarchical = analyze_hierarchical_structure(G)
    all_results['hierarchical'] = hierarchical
    
    # 8. Visualization
    visualize_community_analysis(G, best_communities)
    
    # L∆∞u k·∫øt qu·∫£
    output_path = "data/advanced_community_analysis_results.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        # Convert sets to lists for JSON serialization
        json_results = json.loads(json.dumps(all_results, default=str))
        json.dump(json_results, f, ensure_ascii=False, indent=2)
    print(f"\n‚úì ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o: {output_path}")
    
    # T·ªïng k·∫øt
    print("\n" + "=" * 70)
    print("üìä T·ªîNG K·∫æT PH√ÇN T√çCH C·ªòNG ƒê·ªíNG N√ÇNG CAO")
    print("=" * 70)
    
    print(f"\n1Ô∏è‚É£  ƒêA THU·∫¨T TO√ÅN:")
    for algo, count in all_results['algorithms'].items():
        mod = comparison.get(algo, {}).get('modularity', 0)
        print(f"    - {algo}: {count} c·ªông ƒë·ªìng (Q={mod:.4f})")
    
    print(f"\n2Ô∏è‚É£  CH·∫§T L∆Ø·ª¢NG (thu·∫≠t to√°n {best_algo[0]}):")
    print(f"    - Modularity: {quality.get('modularity', 'N/A'):.4f}")
    print(f"    - Coverage: {quality.get('coverage', 'N/A'):.4f}")
    print(f"    - Avg Internal Density: {quality.get('avg_internal_density', 'N/A'):.4f}")
    
    print(f"\n3Ô∏è‚É£  C·∫§U TR√öC:")
    print(f"    - Bridge nodes: {len(structure.get('bridge_nodes', []))}")
    print(f"    - Hub nodes ƒë∆∞·ª£c ph√°t hi·ªán trong top communities")
    
    print(f"\n4Ô∏è‚É£  NG·ªÆ NGHƒ®A:")
    print(f"    - Company communities: {len(semantic.get('company_communities', []))}")
    print(f"    - Group-centric communities: {len(semantic.get('group_centric_communities', []))}")
    
    print("\n" + "=" * 70)
    print("‚úì HO√ÄN T·∫§T PH√ÇN T√çCH C·ªòNG ƒê·ªíNG N√ÇNG CAO")
    print("=" * 70)


if __name__ == "__main__":
    main()



