"""
PhÃ¢n tÃ­ch máº¡ng xÃ£ há»™i K-pop vá»›i cÃ¡c thuáº­t toÃ¡n:
1. Chá»©ng minh khÃ¡i niá»‡m tháº¿ giá»›i nhá» (Small World)
2. Xáº¿p háº¡ng node báº±ng PageRank
3. PhÃ¡t hiá»‡n cá»™ng Ä‘á»“ng (Community Detection)

Dá»¯ liá»‡u tá»« file merged hoáº·c cÃ¡c file gá»‘c
"""
import sys
import io
import json
import math
import random
from typing import Dict, List, Any, Tuple, Set
from datetime import datetime
from collections import defaultdict

# Robust UTF-8 console output on Windows
if sys.platform == 'win32':
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    except Exception:
        pass

try:
    import networkx as nx
    NETWORKX_AVAILABLE = True
except ImportError:
    NETWORKX_AVAILABLE = False
    print("âš ï¸  NetworkX chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t. Cháº¡y: pip install networkx")

try:
    import matplotlib.pyplot as plt
    import matplotlib
    matplotlib.use('Agg')  # Non-interactive backend
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    print("âš ï¸  Matplotlib chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t. Cháº¡y: pip install matplotlib")


def load_graph_data(
    bfs_file: str = "korean_artists_graph_bfs.json",
    ner_file: str = "kpop_ner_result.json",
    relationships_file: str = "kpop_relationships_result.json"
) -> Tuple[Dict, List]:
    """Load vÃ  merge dá»¯ liá»‡u tá»« cÃ¡c file"""
    print("=" * 70)
    print("LOAD Dá»® LIá»†U GRAPH")
    print("=" * 70)
    
    nodes = {}
    edges = []
    
    # Load BFS data
    try:
        with open(bfs_file, 'r', encoding='utf-8') as f:
            bfs_data = json.load(f)
        bfs_nodes = bfs_data.get("nodes", {})
        bfs_edges = bfs_data.get("edges", [])
        nodes.update(bfs_nodes)
        edges.extend(bfs_edges)
        print(f"âœ“ ÄÃ£ load {len(bfs_nodes)} nodes vÃ  {len(bfs_edges)} edges tá»« {bfs_file}")
    except FileNotFoundError:
        print(f"âš ï¸  KhÃ´ng tÃ¬m tháº¥y {bfs_file}")
    
    # Load NER entities (táº¡o nodes má»›i náº¿u cáº§n)
    try:
        with open(ner_file, 'r', encoding='utf-8') as f:
            ner_data = json.load(f)
        ner_entities = ner_data.get("entities", [])
        new_nodes = 0
        for entity in ner_entities:
            entity_id = entity.get("text", "")
            if entity_id and entity_id not in nodes:
                nodes[entity_id] = {
                    "label": entity.get("type", "Entity"),
                    "title": entity_id
                }
                new_nodes += 1
        print(f"âœ“ ÄÃ£ load {len(ner_entities)} entities, thÃªm {new_nodes} nodes má»›i tá»« {ner_file}")
    except FileNotFoundError:
        print(f"âš ï¸  KhÃ´ng tÃ¬m tháº¥y {ner_file}")
    
    # Load relationships
    try:
        with open(relationships_file, 'r', encoding='utf-8') as f:
            rel_data = json.load(f)
        relationships = rel_data.get("relationships", [])
        
        # Check duplicate
        existing_edges = set()
        for e in edges:
            existing_edges.add((e.get("source"), e.get("target"), e.get("type")))
        
        new_edges = 0
        for rel in relationships:
            key = (rel.get("source"), rel.get("target"), rel.get("type"))
            if key not in existing_edges:
                edges.append({
                    "source": rel.get("source"),
                    "target": rel.get("target"),
                    "type": rel.get("type", "RELATED_TO")
                })
                existing_edges.add(key)
                new_edges += 1
        print(f"âœ“ ÄÃ£ load {len(relationships)} relationships, thÃªm {new_edges} edges má»›i tá»« {relationships_file}")
    except FileNotFoundError:
        print(f"âš ï¸  KhÃ´ng tÃ¬m tháº¥y {relationships_file}")
    
    print(f"\nğŸ“Š Tá»•ng cá»™ng: {len(nodes)} nodes, {len(edges)} edges")
    return nodes, edges


def build_networkx_graph(nodes: Dict, edges: List, undirected: bool = True) -> 'nx.Graph':
    """XÃ¢y dá»±ng NetworkX graph tá»« nodes vÃ  edges"""
    if not NETWORKX_AVAILABLE:
        raise ImportError("NetworkX khÃ´ng kháº£ dá»¥ng")
    
    if undirected:
        G = nx.Graph()
    else:
        G = nx.DiGraph()
    
    # ThÃªm nodes
    for node_id, node_data in nodes.items():
        G.add_node(node_id, **{
            'label': node_data.get('label', 'Entity'),
            'title': node_data.get('title', node_id)
        })
    
    # ThÃªm edges
    for edge in edges:
        src = edge.get('source')
        tgt = edge.get('target')
        if src and tgt and src in nodes and tgt in nodes:
            G.add_edge(src, tgt, type=edge.get('type', 'RELATED_TO'))
    
    return G


# =====================================================
# 1. SMALL WORLD - KHÃI NIá»†M THáº¾ GIá»šI NHá»
# =====================================================
def analyze_small_world(G: 'nx.Graph') -> Dict[str, Any]:
    """
    PhÃ¢n tÃ­ch khÃ¡i niá»‡m Small World:
    - TÃ­nh Average Shortest Path Length (APL)
    - TÃ­nh Clustering Coefficient
    - So sÃ¡nh vá»›i random graph cÃ¹ng kÃ­ch thÆ°á»›c
    
    Small World cÃ³ Ä‘áº·c Ä‘iá»ƒm:
    - APL tháº¥p (nhÆ° random graph)
    - Clustering Coefficient cao (hÆ¡n random graph nhiá»u)
    """
    print("\n" + "=" * 70)
    print("1. PHÃ‚N TÃCH KHÃI NIá»†M THáº¾ GIá»šI NHá» (SMALL WORLD)")
    print("=" * 70)
    
    results = {
        "total_nodes": G.number_of_nodes(),
        "total_edges": G.number_of_edges(),
    }
    
    # Kiá»ƒm tra connected components
    if nx.is_connected(G):
        components = [G]
        largest_cc = G
        print(f"\nğŸ“Š Graph lÃ  connected vá»›i {G.number_of_nodes()} nodes")
    else:
        components = list(nx.connected_components(G))
        largest_cc = G.subgraph(max(components, key=len)).copy()
        print(f"\nğŸ“Š Graph cÃ³ {len(components)} connected components")
        print(f"   Largest component: {largest_cc.number_of_nodes()} nodes ({100*largest_cc.number_of_nodes()/G.number_of_nodes():.1f}%)")
    
    results["num_components"] = len(components) if not nx.is_connected(G) else 1
    results["largest_component_size"] = largest_cc.number_of_nodes()
    results["largest_component_percentage"] = 100 * largest_cc.number_of_nodes() / G.number_of_nodes()
    
    # TÃ­nh Average Shortest Path Length trÃªn largest component
    print("\nğŸ” TÃ­nh Average Shortest Path Length (APL)...")
    
    n = largest_cc.number_of_nodes()
    m = largest_cc.number_of_edges()
    
    if n > 5000:
        # Sampling cho graph lá»›n
        print(f"   Graph lá»›n ({n} nodes), sá»­ dá»¥ng sampling...")
        sample_size = min(1000, n)
        sample_nodes = random.sample(list(largest_cc.nodes()), sample_size)
        
        total_paths = 0
        path_count = 0
        
        for i, source in enumerate(sample_nodes):
            if i % 100 == 0:
                print(f"   Äang xá»­ lÃ½ node {i}/{sample_size}...")
            lengths = nx.single_source_shortest_path_length(largest_cc, source)
            for target, length in lengths.items():
                if source != target:
                    total_paths += length
                    path_count += 1
        
        if path_count > 0:
            apl = total_paths / path_count
        else:
            apl = float('inf')
        print(f"   (Æ¯á»›c lÆ°á»£ng tá»« {sample_size} nodes)")
    else:
        apl = nx.average_shortest_path_length(largest_cc)
    
    results["average_path_length"] = apl
    print(f"   âœ“ Average Shortest Path Length: {apl:.4f}")
    
    # TÃ­nh Clustering Coefficient
    print("\nğŸ” TÃ­nh Clustering Coefficient...")
    avg_clustering = nx.average_clustering(G)
    results["clustering_coefficient"] = avg_clustering
    print(f"   âœ“ Average Clustering Coefficient: {avg_clustering:.4f}")
    
    # TÃ­nh diameter (Ä‘Æ°á»ng kÃ­nh)
    print("\nğŸ” TÃ­nh Diameter...")
    if n <= 5000:
        diameter = nx.diameter(largest_cc)
    else:
        # Æ¯á»›c lÆ°á»£ng diameter báº±ng sampling
        sample_nodes = random.sample(list(largest_cc.nodes()), min(100, n))
        max_dist = 0
        for node in sample_nodes:
            eccentricity = nx.eccentricity(largest_cc, v=node)
            max_dist = max(max_dist, eccentricity)
        diameter = max_dist
        print(f"   (Æ¯á»›c lÆ°á»£ng tá»« sampling)")
    results["diameter"] = diameter
    print(f"   âœ“ Diameter: {diameter}")
    
    # So sÃ¡nh vá»›i Random Graph (ErdÅ‘sâ€“RÃ©nyi)
    print("\nğŸ” So sÃ¡nh vá»›i Random Graph (ErdÅ‘sâ€“RÃ©nyi)...")
    p = 2 * m / (n * (n - 1)) if n > 1 else 0  # Probability Ä‘á»ƒ cÃ³ cÃ¹ng sá»‘ edges
    
    # LÃ½ thuyáº¿t cho random graph:
    # APL_random â‰ˆ ln(n) / ln(k) vá»›i k = average degree
    # Clustering_random â‰ˆ p = k/n
    avg_degree = 2 * m / n if n > 0 else 0
    
    if avg_degree > 1:
        expected_apl_random = math.log(n) / math.log(avg_degree) if avg_degree > 1 else float('inf')
    else:
        expected_apl_random = float('inf')
    expected_clustering_random = avg_degree / n if n > 0 else 0
    
    results["random_graph_expected_apl"] = expected_apl_random
    results["random_graph_expected_clustering"] = expected_clustering_random
    results["average_degree"] = avg_degree
    
    print(f"   Average Degree: {avg_degree:.2f}")
    print(f"   Random Graph Expected APL: {expected_apl_random:.4f}")
    print(f"   Random Graph Expected Clustering: {expected_clustering_random:.6f}")
    
    # Small World Index
    # Ïƒ = (C/C_random) / (L/L_random)
    # Ïƒ > 1 indicates small world property
    if expected_clustering_random > 0 and expected_apl_random > 0 and expected_apl_random != float('inf'):
        sigma = (avg_clustering / expected_clustering_random) / (apl / expected_apl_random)
        results["small_world_sigma"] = sigma
        print(f"\nğŸ“Š Small World Sigma (Ïƒ): {sigma:.4f}")
        
        if sigma > 1:
            print("   âœ“ Ïƒ > 1: Máº¡ng cÃ³ tÃ­nh cháº¥t THáº¾ GIá»šI NHá» (Small World)")
        else:
            print("   âœ— Ïƒ â‰¤ 1: Máº¡ng khÃ´ng thá»ƒ hiá»‡n tÃ­nh cháº¥t Small World rÃµ rÃ ng")
    else:
        results["small_world_sigma"] = None
    
    # Káº¿t luáº­n
    print("\n" + "-" * 70)
    print("ğŸ“‹ Káº¾T LUáº¬N Vá»€ KHÃI NIá»†M THáº¾ GIá»šI NHá»:")
    print("-" * 70)
    
    conclusions = []
    
    # APL tháº¥p?
    if apl < math.log(n) * 2:
        conclusions.append(f"âœ“ APL = {apl:.2f} khÃ¡ tháº¥p (< 2*ln(n) = {math.log(n)*2:.2f})")
        results["low_apl"] = True
    else:
        conclusions.append(f"âœ— APL = {apl:.2f} khÃ¡ cao")
        results["low_apl"] = False
    
    # Clustering cao?
    if avg_clustering > expected_clustering_random * 10:
        conclusions.append(f"âœ“ Clustering = {avg_clustering:.4f} cao hÆ¡n random {avg_clustering/expected_clustering_random:.1f}x")
        results["high_clustering"] = True
    else:
        conclusions.append(f"âœ— Clustering = {avg_clustering:.4f} khÃ´ng cao hÆ¡n random Ä‘Ã¡ng ká»ƒ")
        results["high_clustering"] = False
    
    # Six Degrees of Separation?
    if apl <= 6:
        conclusions.append(f"âœ“ APL â‰¤ 6: TuÃ¢n theo 'Six Degrees of Separation'")
        results["six_degrees"] = True
    else:
        conclusions.append(f"âœ— APL > 6: KhÃ´ng tuÃ¢n theo 'Six Degrees of Separation' nghiÃªm ngáº·t")
        results["six_degrees"] = False
    
    for c in conclusions:
        print(f"   {c}")
    
    if results.get("low_apl") and results.get("high_clustering"):
        print("\nğŸ¯ Káº¾T LUáº¬N: Máº¡ng K-pop THá»A MÃƒN tÃ­nh cháº¥t THáº¾ GIá»šI NHá» (Small World)")
        print(f"   - Báº¥t ká»³ 2 node nÃ o cÅ©ng cÃ³ thá»ƒ káº¿t ná»‘i qua trung bÃ¬nh {apl:.1f} bÆ°á»›c")
        print(f"   - CÃ¡c node cÃ³ xu hÆ°á»›ng táº¡o thÃ nh cÃ¡c cá»¥m (cluster) cá»¥c bá»™")
        results["is_small_world"] = True
    else:
        print("\nğŸ¯ Káº¾T LUáº¬N: Máº¡ng cÃ³ má»™t sá»‘ Ä‘áº·c Ä‘iá»ƒm cá»§a Small World nhÆ°ng chÆ°a hoÃ n toÃ n")
        results["is_small_world"] = False
    
    return results


# =====================================================
# 2. PAGERANK - Xáº¾P Háº NG NODE
# =====================================================
def analyze_pagerank(G: 'nx.Graph', top_k: int = 50) -> Dict[str, Any]:
    """
    Xáº¿p háº¡ng nodes báº±ng thuáº­t toÃ¡n PageRank
    """
    print("\n" + "=" * 70)
    print("2. Xáº¾P Háº NG NODE Báº°NG PAGERANK")
    print("=" * 70)
    
    results = {}
    
    # TÃ­nh PageRank
    print("\nğŸ” Äang tÃ­nh PageRank...")
    pagerank = nx.pagerank(G, alpha=0.85, max_iter=100)
    
    # Sáº¯p xáº¿p theo PageRank giáº£m dáº§n
    sorted_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)
    
    results["total_nodes"] = len(pagerank)
    results["top_nodes"] = []
    
    print(f"\nğŸ“Š TOP {top_k} NODES THEO PAGERANK:")
    print("-" * 70)
    print(f"{'Rank':<6} {'Node':<40} {'PageRank':<12} {'Label'}")
    print("-" * 70)
    
    for i, (node, score) in enumerate(sorted_pagerank[:top_k], 1):
        label = G.nodes[node].get('label', 'Unknown')
        print(f"{i:<6} {node[:38]:<40} {score:.8f}   {label}")
        results["top_nodes"].append({
            "rank": i,
            "node": node,
            "pagerank": score,
            "label": label
        })
    
    # Thá»‘ng kÃª theo label
    print("\nğŸ“Š PAGERANK TRUNG BÃŒNH THEO LABEL:")
    print("-" * 50)
    
    label_scores = defaultdict(list)
    for node, score in pagerank.items():
        label = G.nodes[node].get('label', 'Unknown')
        label_scores[label].append(score)
    
    label_avg = {}
    for label, scores in label_scores.items():
        avg = sum(scores) / len(scores)
        label_avg[label] = avg
    
    results["pagerank_by_label"] = {}
    for label, avg in sorted(label_avg.items(), key=lambda x: x[1], reverse=True):
        count = len(label_scores[label])
        print(f"  {label:<15}: {avg:.8f} (n={count})")
        results["pagerank_by_label"][label] = {
            "average": avg,
            "count": count
        }
    
    # TÃ­nh thÃªm cÃ¡c centrality khÃ¡c Ä‘á»ƒ so sÃ¡nh
    print("\nğŸ” Äang tÃ­nh cÃ¡c centrality khÃ¡c...")
    
    # Degree Centrality
    degree_cent = nx.degree_centrality(G)
    sorted_degree = sorted(degree_cent.items(), key=lambda x: x[1], reverse=True)
    
    # Betweenness Centrality (sampling cho graph lá»›n)
    if G.number_of_nodes() > 1000:
        print("   Betweenness Centrality: sá»­ dá»¥ng sampling...")
        betweenness = nx.betweenness_centrality(G, k=min(500, G.number_of_nodes()))
    else:
        betweenness = nx.betweenness_centrality(G)
    sorted_betweenness = sorted(betweenness.items(), key=lambda x: x[1], reverse=True)
    
    print(f"\nğŸ“Š SO SÃNH TOP 10 THEO CÃC CENTRALITY:")
    print("-" * 90)
    print(f"{'Rank':<6} {'PageRank':<25} {'Degree':<25} {'Betweenness':<25}")
    print("-" * 90)
    
    for i in range(min(10, len(sorted_pagerank))):
        pr_node = sorted_pagerank[i][0][:23] if i < len(sorted_pagerank) else "-"
        deg_node = sorted_degree[i][0][:23] if i < len(sorted_degree) else "-"
        bet_node = sorted_betweenness[i][0][:23] if i < len(sorted_betweenness) else "-"
        print(f"{i+1:<6} {pr_node:<25} {deg_node:<25} {bet_node:<25}")
    
    results["degree_centrality_top10"] = [{"node": n, "score": s} for n, s in sorted_degree[:10]]
    results["betweenness_centrality_top10"] = [{"node": n, "score": s} for n, s in sorted_betweenness[:10]]
    
    # Káº¿t luáº­n
    print("\n" + "-" * 70)
    print("ğŸ“‹ Káº¾T LUáº¬N Vá»€ Xáº¾P Háº NG:")
    print("-" * 70)
    
    top1 = sorted_pagerank[0] if sorted_pagerank else ("N/A", 0)
    print(f"   ğŸ† Node quan trá»ng nháº¥t: {top1[0]} (PageRank: {top1[1]:.6f})")
    
    # TÃ¬m top node theo tá»«ng label
    top_by_label = {}
    for node, score in sorted_pagerank:
        label = G.nodes[node].get('label', 'Unknown')
        if label not in top_by_label:
            top_by_label[label] = (node, score)
    
    print("\n   ğŸ† Top node theo tá»«ng loáº¡i:")
    for label, (node, score) in sorted(top_by_label.items(), key=lambda x: x[1][1], reverse=True):
        print(f"      - {label}: {node} ({score:.6f})")
    
    results["top_by_label"] = {label: {"node": node, "pagerank": score} for label, (node, score) in top_by_label.items()}
    
    return results


# =====================================================
# 3. COMMUNITY DETECTION - PHÃT HIá»†N Cá»˜NG Äá»’NG
# =====================================================
def analyze_communities(G: 'nx.Graph', top_k_communities: int = 10) -> Dict[str, Any]:
    """
    PhÃ¡t hiá»‡n cá»™ng Ä‘á»“ng trong máº¡ng sá»­ dá»¥ng thuáº­t toÃ¡n Louvain
    """
    print("\n" + "=" * 70)
    print("3. PHÃT HIá»†N Cá»˜NG Äá»’NG (COMMUNITY DETECTION)")
    print("=" * 70)
    
    results = {}
    
    # Kiá»ƒm tra cÃ³ thÆ° viá»‡n community detection khÃ´ng
    try:
        from networkx.algorithms import community as nx_community
        HAS_LOUVAIN = hasattr(nx_community, 'louvain_communities')
    except ImportError:
        HAS_LOUVAIN = False
    
    if HAS_LOUVAIN:
        print("\nğŸ” Sá»­ dá»¥ng thuáº­t toÃ¡n Louvain...")
        communities = nx_community.louvain_communities(G, seed=42)
        method = "Louvain"
    else:
        print("\nğŸ” Sá»­ dá»¥ng thuáº­t toÃ¡n Greedy Modularity...")
        communities = list(nx_community.greedy_modularity_communities(G))
        method = "Greedy Modularity"
    
    # Chuyá»ƒn thÃ nh list Ä‘á»ƒ sáº¯p xáº¿p
    communities = [set(c) for c in communities]
    communities.sort(key=len, reverse=True)
    
    results["method"] = method
    results["total_communities"] = len(communities)
    
    print(f"\nâœ“ PhÃ¡t hiá»‡n Ä‘Æ°á»£c {len(communities)} cá»™ng Ä‘á»“ng")
    
    # TÃ­nh modularity
    try:
        modularity = nx_community.modularity(G, communities)
        results["modularity"] = modularity
        print(f"âœ“ Modularity: {modularity:.4f}")
        
        if modularity > 0.3:
            print("   â†’ Modularity > 0.3: Cáº¥u trÃºc cá»™ng Ä‘á»“ng RÃ• RÃ€NG")
        elif modularity > 0.1:
            print("   â†’ Modularity > 0.1: Cáº¥u trÃºc cá»™ng Ä‘á»“ng TRUNG BÃŒNH")
        else:
            print("   â†’ Modularity â‰¤ 0.1: Cáº¥u trÃºc cá»™ng Ä‘á»“ng Yáº¾U")
    except:
        results["modularity"] = None
    
    # Thá»‘ng kÃª kÃ­ch thÆ°á»›c cá»™ng Ä‘á»“ng
    community_sizes = [len(c) for c in communities]
    results["community_sizes"] = {
        "min": min(community_sizes),
        "max": max(community_sizes),
        "mean": sum(community_sizes) / len(community_sizes),
        "median": sorted(community_sizes)[len(community_sizes)//2]
    }
    
    print(f"\nğŸ“Š THá»NG KÃŠ KÃCH THÆ¯á»šC Cá»˜NG Äá»’NG:")
    print(f"   - Nhá» nháº¥t: {min(community_sizes)} nodes")
    print(f"   - Lá»›n nháº¥t: {max(community_sizes)} nodes")
    print(f"   - Trung bÃ¬nh: {sum(community_sizes)/len(community_sizes):.1f} nodes")
    
    # Chi tiáº¿t top communities
    print(f"\nğŸ“Š TOP {top_k_communities} Cá»˜NG Äá»’NG Lá»šN NHáº¤T:")
    print("-" * 70)
    
    results["top_communities"] = []
    
    for i, comm in enumerate(communities[:top_k_communities], 1):
        # Äáº¿m labels trong community
        label_counts = defaultdict(int)
        for node in comm:
            label = G.nodes[node].get('label', 'Unknown')
            label_counts[label] += 1
        
        # TÃ¬m label chá»§ Ä‘áº¡o
        dominant_label = max(label_counts.items(), key=lambda x: x[1])
        
        # Láº¥y má»™t sá»‘ node máº«u
        sample_nodes = list(comm)[:5]
        
        print(f"\nğŸ”¹ Cá»™ng Ä‘á»“ng {i}: {len(comm)} nodes")
        print(f"   Label chá»§ Ä‘áº¡o: {dominant_label[0]} ({dominant_label[1]} nodes, {100*dominant_label[1]/len(comm):.1f}%)")
        print(f"   PhÃ¢n bá»‘: {dict(label_counts)}")
        print(f"   Nodes máº«u: {', '.join(sample_nodes)}")
        
        results["top_communities"].append({
            "id": i,
            "size": len(comm),
            "dominant_label": dominant_label[0],
            "dominant_label_count": dominant_label[1],
            "dominant_label_percentage": 100 * dominant_label[1] / len(comm),
            "label_distribution": dict(label_counts),
            "sample_nodes": sample_nodes
        })
    
    # PhÃ¢n tÃ­ch cÃ¡c cá»™ng Ä‘á»“ng Ä‘áº·c biá»‡t
    print("\nğŸ“Š PHÃ‚N TÃCH Cá»˜NG Äá»’NG:")
    print("-" * 70)
    
    # TÃ¬m cÃ¡c cá»™ng Ä‘á»“ng cÃ³ tÃ­nh cháº¥t Ä‘áº·c biá»‡t
    artist_communities = []
    group_communities = []
    mixed_communities = []
    
    for i, comm in enumerate(communities):
        label_counts = defaultdict(int)
        for node in comm:
            label = G.nodes[node].get('label', 'Unknown')
            label_counts[label] += 1
        
        total = len(comm)
        if label_counts.get('Artist', 0) / total > 0.7:
            artist_communities.append((i, len(comm)))
        elif label_counts.get('Group', 0) / total > 0.5:
            group_communities.append((i, len(comm)))
        else:
            mixed_communities.append((i, len(comm)))
    
    print(f"   - Cá»™ng Ä‘á»“ng chá»§ yáº¿u Artist: {len(artist_communities)}")
    print(f"   - Cá»™ng Ä‘á»“ng chá»§ yáº¿u Group: {len(group_communities)}")
    print(f"   - Cá»™ng Ä‘á»“ng há»—n há»£p: {len(mixed_communities)}")
    
    results["community_types"] = {
        "artist_dominated": len(artist_communities),
        "group_dominated": len(group_communities),
        "mixed": len(mixed_communities)
    }
    
    # Káº¿t luáº­n
    print("\n" + "-" * 70)
    print("ğŸ“‹ Káº¾T LUáº¬N Vá»€ Cáº¤U TRÃšC Cá»˜NG Äá»’NG:")
    print("-" * 70)
    
    print(f"   1. Máº¡ng K-pop cÃ³ {len(communities)} cá»™ng Ä‘á»“ng rÃµ rÃ ng")
    
    if results.get("modularity", 0) > 0.3:
        print(f"   2. Modularity cao ({results.get('modularity', 0):.3f}) cho tháº¥y cáº¥u trÃºc cá»™ng Ä‘á»“ng máº¡nh")
    
    print(f"   3. Cá»™ng Ä‘á»“ng lá»›n nháº¥t cÃ³ {max(community_sizes)} nodes ({100*max(community_sizes)/G.number_of_nodes():.1f}% máº¡ng)")
    
    # Diá»…n giáº£i cá»™ng Ä‘á»“ng
    print("\n   ğŸ’¡ DIá»„N GIáº¢I:")
    print("   - CÃ¡c cá»™ng Ä‘á»“ng cÃ³ thá»ƒ Ä‘áº¡i diá»‡n cho:")
    print("     + Nghá»‡ sÄ© cÃ¹ng cÃ´ng ty (SM, YG, JYP, HYBE...)")
    print("     + Tháº¿ há»‡ idol (1st, 2nd, 3rd, 4th generation)")
    print("     + Thá»ƒ loáº¡i Ã¢m nháº¡c (Hip-hop, Ballad, Dance...)")
    print("     + CÃ¡c má»‘i quan há»‡ há»£p tÃ¡c, collab")
    
    return results


# =====================================================
# MAIN FUNCTION
# =====================================================
def main():
    """HÃ m main cháº¡y táº¥t cáº£ phÃ¢n tÃ­ch"""
    
    print("\n" + "=" * 70)
    print("ğŸµ PHÃ‚N TÃCH Máº NG XÃƒ Há»˜I K-POP ğŸµ")
    print("=" * 70)
    print(f"Thá»i gian: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    if not NETWORKX_AVAILABLE:
        print("\nâŒ KhÃ´ng thá»ƒ cháº¡y phÃ¢n tÃ­ch vÃ¬ NetworkX chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t")
        print("   Cháº¡y: pip install networkx")
        return
    
    # Load dá»¯ liá»‡u
    nodes, edges = load_graph_data()
    
    if not nodes:
        print("\nâŒ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ phÃ¢n tÃ­ch")
        return
    
    # Build NetworkX graph
    print("\nğŸ”§ Äang xÃ¢y dá»±ng NetworkX graph...")
    G = build_networkx_graph(nodes, edges, undirected=True)
    print(f"âœ“ Graph cÃ³ {G.number_of_nodes()} nodes vÃ  {G.number_of_edges()} edges")
    
    # Káº¿t quáº£ tá»•ng há»£p
    all_results = {
        "analysis_time": datetime.now().isoformat(),
        "graph_info": {
            "total_nodes": G.number_of_nodes(),
            "total_edges": G.number_of_edges()
        }
    }
    
    # 1. Small World Analysis
    try:
        small_world_results = analyze_small_world(G)
        all_results["small_world"] = small_world_results
    except Exception as e:
        print(f"\nâŒ Lá»—i khi phÃ¢n tÃ­ch Small World: {e}")
        all_results["small_world"] = {"error": str(e)}
    
    # 2. PageRank Analysis
    try:
        pagerank_results = analyze_pagerank(G, top_k=50)
        all_results["pagerank"] = pagerank_results
    except Exception as e:
        print(f"\nâŒ Lá»—i khi phÃ¢n tÃ­ch PageRank: {e}")
        all_results["pagerank"] = {"error": str(e)}
    
    # 3. Community Detection
    try:
        community_results = analyze_communities(G, top_k_communities=10)
        all_results["communities"] = community_results
    except Exception as e:
        print(f"\nâŒ Lá»—i khi phÃ¡t hiá»‡n cá»™ng Ä‘á»“ng: {e}")
        all_results["communities"] = {"error": str(e)}
    
    # LÆ°u káº¿t quáº£
    output_file = "network_analysis_results.json"
    print(f"\nğŸ’¾ Äang lÆ°u káº¿t quáº£ vÃ o {output_file}...")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2, default=str)
    print(f"âœ“ ÄÃ£ lÆ°u káº¿t quáº£ vÃ o {output_file}")
    
    # Tá»•ng káº¿t
    print("\n" + "=" * 70)
    print("ğŸ“Š Tá»”NG Káº¾T PHÃ‚N TÃCH Máº NG XÃƒ Há»˜I K-POP")
    print("=" * 70)
    
    print("\n1ï¸âƒ£  THáº¾ GIá»šI NHá» (SMALL WORLD):")
    if "small_world" in all_results and "error" not in all_results["small_world"]:
        sw = all_results["small_world"]
        print(f"    - Average Path Length: {sw.get('average_path_length', 'N/A'):.2f}")
        print(f"    - Clustering Coefficient: {sw.get('clustering_coefficient', 'N/A'):.4f}")
        print(f"    - LÃ  Small World: {'âœ“ CÃ“' if sw.get('is_small_world') else 'âœ— KHÃ”NG'}")
    
    print("\n2ï¸âƒ£  PAGERANK (TOP 5):")
    if "pagerank" in all_results and "error" not in all_results["pagerank"]:
        pr = all_results["pagerank"]
        for node_info in pr.get("top_nodes", [])[:5]:
            print(f"    {node_info['rank']}. {node_info['node']} ({node_info['label']})")
    
    print("\n3ï¸âƒ£  Cá»˜NG Äá»’NG:")
    if "communities" in all_results and "error" not in all_results["communities"]:
        comm = all_results["communities"]
        print(f"    - Sá»‘ cá»™ng Ä‘á»“ng: {comm.get('total_communities', 'N/A')}")
        print(f"    - Modularity: {comm.get('modularity', 'N/A'):.4f}" if comm.get('modularity') else "")
    
    print("\n" + "=" * 70)
    print("âœ“ HOÃ€N Táº¤T PHÃ‚N TÃCH")
    print("=" * 70)


if __name__ == "__main__":
    main()

