"""
Main Runner Script for K-pop Knowledge Graph Chatbot

This script provides different modes to run the chatbot:
1. Interactive CLI mode
2. Web UI mode (Gradio)
3. Evaluation mode
4. Comparison mode

Usage:
    python src/run_chatbot.py --mode cli
    python src/run_chatbot.py --mode ui
    python src/run_chatbot.py --mode eval
    python src/run_chatbot.py --mode compare
"""

import argparse
import os
import sys

# Add src to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from chatbot import KpopChatbot
from chatbot.evaluation import EvaluationDatasetGenerator
from chatbot.comparison import ChatbotComparison


def run_cli_mode(chat_mode: str = 'standard'):
    """
    Run interactive CLI chatbot.
    
    âœ… YÃŠU Cáº¦U BÃ€I Táº¬P: Pháº£i LUÃ”N dÃ¹ng Small LLM (â‰¤1B params)
    
    LLM nhá» Ä‘Æ°á»£c dÃ¹ng cho 2 nhiá»‡m vá»¥:
    1. Hiá»ƒu cÃ¢u há»i (phÃ¢n tÃ­ch, xÃ¡c Ä‘á»‹nh thá»±c thá»ƒ, nháº­n ra loáº¡i cÃ¢u há»i)
    2. GENERATION: Táº¡o cÃ¢u tráº£ lá»i tá»± nhiÃªn tá»« context (triples, paths, reasoning results)
    
    âš ï¸ QUAN TRá»ŒNG: LLM KHÃ”NG lÃ m multi-hop reasoning
    - Multi-hop reasoning do Reasoner thá»±c hiá»‡n (graph algorithm: tÃ¬m Ä‘Æ°á»ng Ä‘i, tÃ­nh scoring, xÃ¢u chuá»—i path)
    - LLM chá»‰ Ä‘á»c káº¿t quáº£ reasoning vÃ  format thÃ nh cÃ¢u tráº£ lá»i tá»± nhiÃªn
    
    Args:
        chat_mode: 'standard' (luÃ´n dÃ¹ng LLM - Ä‘Ã¡p á»©ng yÃªu cáº§u) hoáº·c 'optimized' (tá»‘i Æ°u context)
    """
    print("\n" + "="*60)
    print("ğŸ¤ K-pop Knowledge Graph Chatbot - Interactive Mode")
    print("="*60)
    
    # Show mode info
    if chat_mode == 'optimized':
        print("âš¡ Cháº¿ Ä‘á»™: OPTIMIZED MODE (Tá»‘i Æ°u context, váº«n dÃ¹ng LLM)")
        print("   - Nhanh hÆ¡n: Giáº£m context size khi reasoning confident")
        print("   - Váº«n dÃ¹ng Small LLM: ÄÃ¡p á»©ng yÃªu cáº§u bÃ i táº­p")
    else:  # standard
        print("ğŸ”„ Cháº¿ Ä‘á»™: STANDARD MODE (LuÃ´n dÃ¹ng Small LLM)")
        print("   - LLM nhá» (â‰¤1B params) dÃ¹ng cho:")
        print("     â€¢ Hiá»ƒu cÃ¢u há»i (phÃ¢n tÃ­ch, extract entities, detect intent)")
        print("     â€¢ GENERATION: Táº¡o cÃ¢u tráº£ lá»i tá»± nhiÃªn tá»« context")
        print("   - Multi-hop reasoning: Do Reasoner thá»±c hiá»‡n (graph algorithm)")
    
    print("\nNháº­p cÃ¢u há»i vá» K-pop hoáº·c gÃµ 'quit' Ä‘á»ƒ thoÃ¡t.\n")
    print("ğŸ’¡ Tip: DÃ¹ng lá»‡nh nhanh Ä‘á»ƒ trÃ¡nh chá» LLM:")
    print("   - 'members BTS' hoáº·c 'BTS members'")
    print("   - 'company BLACKPINK'")
    print("   - 'same BTS SEVENTEEN'")
    print("   - 'mode standard' hoáº·c 'mode optimized' Ä‘á»ƒ Ä‘á»•i cháº¿ Ä‘á»™")
    print("")
    print("ğŸ“Œ LÆ°u Ã½: Chatbot LUÃ”N dÃ¹ng Small LLM (â‰¤1B params) Ä‘á»ƒ:")
    print("   1. Hiá»ƒu cÃ¢u há»i (GraphRAG + LLM understanding)")
    print("   2. GENERATION: Táº¡o cÃ¢u tráº£ lá»i tá»± nhiÃªn (format context)")
    print("   âš ï¸ Multi-hop reasoning: Do Reasoner thá»±c hiá»‡n (graph algorithm)")
    print("")
    
    # Initialize
    # Check if Neo4j should be used
    use_neo4j = os.getenv("USE_NEO4J", "false").lower() == "true"
    neo4j_password = os.getenv("NEO4J_PASSWORD")
    
    if use_neo4j:
        if not neo4j_password:
            print("âš ï¸ USE_NEO4J=true but NEO4J_PASSWORD not set!")
            print("   Falling back to JSON file mode...")
            use_neo4j = False
    
    if use_neo4j:
        print("ğŸ“Š Using Neo4j Knowledge Graph...")
        chatbot = KpopChatbot(
            use_neo4j=True,
            neo4j_uri=os.getenv("NEO4J_URI", "bolt://localhost:7687"),
            neo4j_user=os.getenv("NEO4J_USER", "neo4j"),
            neo4j_password=neo4j_password,
            neo4j_database=os.getenv("NEO4J_DATABASE", None),
            verbose=True
        )
    else:
        print("ğŸ“Š Using JSON file Knowledge Graph...")
        chatbot = KpopChatbot(verbose=True)
    session_id = chatbot.create_session()
    
    print("\nâœ… Sáºµn sÃ ng! HÃ£y Ä‘áº·t cÃ¢u há»i vá» K-pop.\n")
    
    while True:
        try:
            query = input("Báº¡n: ").strip()
            
            if not query:
                continue
                
            if query.lower() in ['quit', 'exit', 'q', 'thoÃ¡t']:
                print("\nğŸ‘‹ Táº¡m biá»‡t!")
                break
                
            if query.lower() == 'help':
                print("""
ğŸ“š CÃ¡c lá»‡nh Ä‘áº·c biá»‡t:
- 'members <group>': Xem thÃ nh viÃªn nhÃ³m
- 'company <group>': Xem cÃ´ng ty quáº£n lÃ½
- 'same <group1> <group2>': Kiá»ƒm tra cÃ¹ng cÃ´ng ty
- 'path <entity1> <entity2>': TÃ¬m Ä‘Æ°á»ng Ä‘i
- 'stats': Xem thá»‘ng kÃª
- 'mode standard': Chuyá»ƒn sang Standard Mode (luÃ´n dÃ¹ng Small LLM)
- 'mode optimized': Chuyá»ƒn sang Optimized Mode (tá»‘i Æ°u context, váº«n dÃ¹ng LLM)
- 'quit': ThoÃ¡t

ğŸ“Œ LÆ°u Ã½: Chatbot LUÃ”N dÃ¹ng Small LLM (â‰¤1B params) Ä‘á»ƒ:
   1. Hiá»ƒu cÃ¢u há»i (GraphRAG + LLM understanding)
   2. GENERATION: Táº¡o cÃ¢u tráº£ lá»i tá»± nhiÃªn (format context)
   âš ï¸ Multi-hop reasoning: Do Reasoner thá»±c hiá»‡n (graph algorithm)
                """)
                continue
            
            # Handle mode switching
            if query.lower().startswith('mode '):
                new_mode = query[5:].strip().lower()
                if new_mode in ['standard', 'optimized']:
                    chat_mode = new_mode
                    mode_names = {
                        'standard': 'ğŸ”„ STANDARD MODE (LuÃ´n dÃ¹ng Small LLM)',
                        'optimized': 'âš¡ OPTIMIZED MODE (Tá»‘i Æ°u context, váº«n dÃ¹ng LLM)'
                    }
                    print(f"\nâœ… ÄÃ£ chuyá»ƒn sang: {mode_names[chat_mode]}\n")
                    print("ğŸ“Œ LÆ°u Ã½: Cáº£ 2 cháº¿ Ä‘á»™ Ä‘á»u dÃ¹ng Small LLM (Ä‘Ã¡p á»©ng yÃªu cáº§u bÃ i táº­p)\n")
                else:
                    print(f"\nâŒ Cháº¿ Ä‘á»™ khÃ´ng há»£p lá»‡. DÃ¹ng: standard hoáº·c optimized\n")
                continue
                
            if query.lower() == 'stats':
                stats = chatbot.get_statistics()
                print(f"\nğŸ“Š Thá»‘ng kÃª:")
                print(f"  - Nodes: {stats['knowledge_graph']['total_nodes']}")
                print(f"  - Edges: {stats['knowledge_graph']['total_edges']}")
                continue
                
            # Handle "members <group>" or "<group> members"
            if query.lower().startswith('members '):
                group = query[8:].strip()
                result = chatbot.get_group_members(group)
                print(f"\nğŸ¤– {result['answer']}\n")
                continue
            elif query.lower().endswith(' members'):
                group = query[:-8].strip()
                result = chatbot.get_group_members(group)
                print(f"\nğŸ¤– {result['answer']}\n")
                continue
                
            if query.lower().startswith('company '):
                group = query[8:].strip()
                result = chatbot.get_group_company(group)
                print(f"\nğŸ¤– {result['answer']}\n")
                continue
                
            if query.lower().startswith('same '):
                parts = query[5:].strip().split()
                if len(parts) >= 2:
                    result = chatbot.check_same_company(parts[0], parts[1])
                    print(f"\nğŸ¤– {result['answer']}\n")
                continue
                
            if query.lower().startswith('path '):
                parts = query[5:].strip().split()
                if len(parts) >= 2:
                    result = chatbot.find_path(parts[0], parts[1])
                    print(f"\nğŸ¤– {result['description']}\n")
                continue
                
            # Normal chat - use selected mode
            print("ğŸ”„ Äang xá»­ lÃ½...")
            
            # âœ… YÃŠU Cáº¦U BÃ€I Táº¬P: LUÃ”N dÃ¹ng Small LLM (â‰¤1B params)
            # LLM nhá» Ä‘Æ°á»£c dÃ¹ng cho 2 nhiá»‡m vá»¥:
            # 1. Hiá»ƒu cÃ¢u há»i (GraphRAG + LLM understanding)
            # 2. GENERATION: Táº¡o cÃ¢u tráº£ lá»i tá»± nhiÃªn (format context thÃ nh cÃ¢u vÄƒn)
            # 
            # âš ï¸ QUAN TRá»ŒNG: LLM KHÃ”NG lÃ m multi-hop reasoning
            # - Multi-hop reasoning do Reasoner thá»±c hiá»‡n (graph algorithm)
            # - LLM chá»‰ Ä‘á»c káº¿t quáº£ reasoning vÃ  format thÃ nh cÃ¢u tráº£ lá»i
            use_llm = True  # LUÃ”N True - Ä‘Ã¡p á»©ng yÃªu cáº§u bÃ i táº­p
            
            if chat_mode == 'optimized':
                print("   âš¡ Optimized mode: DÃ¹ng Small LLM vá»›i context tá»‘i Æ°u (cÃ³ thá»ƒ máº¥t 10-30 giÃ¢y)")
            else:  # standard
                print("   ğŸ”„ Standard mode: DÃ¹ng Small LLM vá»›i context Ä‘áº§y Ä‘á»§ (cÃ³ thá»ƒ máº¥t 10-30 giÃ¢y)")
            
            # Pipeline hoáº¡t Ä‘á»™ng (4 bÆ°á»›c):
            # 1. User Query â†’ LLM nhá» hiá»ƒu cÃ¢u há»i (extract entities, detect intent)
            # 2. GraphRAG â†’ Truy xuáº¥t thÃ´ng tin tá»« Ä‘á»“ thá»‹ (entities, relationships, paths)
            # 3. Multi-hop Reasoning (Reasoner) â†’ Suy luáº­n tá»« paths (graph algorithm: tÃ¬m Ä‘Æ°á»ng Ä‘i, tÃ­nh scoring, xÃ¢u chuá»—i)
            # 4. LLM nhá» (GENERATION) â†’ Táº¡o cÃ¢u tráº£ lá»i tá»± nhiÃªn tá»« context (triples, paths, reasoning results)
            result = chatbot.chat(
                query, 
                session_id, 
                use_multi_hop=True,
                return_details=True,
                use_llm=use_llm  # LUÃ”N True - Ä‘Ã¡p á»©ng yÃªu cáº§u bÃ i táº­p
            )
            
            print(f"\nğŸ¤– {result['response']}")
            print(f"   [Entities: {result['entities_found']}, Hops: {result['reasoning_hops']}]\n")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ Táº¡m biá»‡t!")
            break
        except Exception as e:
            print(f"\nâŒ Lá»—i: {e}\n")


def run_ui_mode(use_streamlit: bool = False):
    """
    Run web UI.
    
    Args:
        use_streamlit: If True, use Streamlit instead of Gradio
    """
    if use_streamlit:
        try:
            import streamlit.web.cli as stcli
            import sys
            import os
            
            # Get streamlit app path
            streamlit_app_path = os.path.join(
                os.path.dirname(__file__),
                "chatbot",
                "streamlit_app.py"
            )
            
            print("ğŸš€ Launching Streamlit UI...")
            print(f"   App: {streamlit_app_path}")
            print("   URL: http://localhost:8501\n")
            
            # Run streamlit
            sys.argv = ["streamlit", "run", streamlit_app_path]
            stcli.main()
        except ImportError:
            print("âŒ Streamlit not installed. Install with: pip install streamlit")
            print("   Falling back to Gradio...")
            use_streamlit = False
    
    if not use_streamlit:
        from chatbot.app import main as run_app
        run_app()


def run_eval_mode(num_questions: int = 2000, use_chatgpt: bool = False, chatgpt_ratio: float = 0.2):
    """
    Generate evaluation dataset.
    
    Args:
        num_questions: Target number of questions
        use_chatgpt: Whether to use ChatGPT for some questions
        chatgpt_ratio: Ratio of questions from ChatGPT (0.0-1.0)
    """
    print("\n" + "="*60)
    print("ğŸ“ Evaluation Dataset Generator")
    print("="*60)
    
    if use_chatgpt:
        print("\nğŸ’¡ Using ChatGPT for some questions")
        print(f"   Distribution: {int(num_questions * (1 - chatgpt_ratio))} from graph, {int(num_questions * chatgpt_ratio)} from ChatGPT")
        print("   âš ï¸  Make sure OPENAI_API_KEY is set!")
    
    generator = EvaluationDatasetGenerator()
    stats = generator.generate_full_dataset(
        target_count=num_questions,
        output_path="data/evaluation_dataset.json",
        use_chatgpt=use_chatgpt,
        chatgpt_ratio=chatgpt_ratio
    )
    
    print("\nğŸ“Š Dataset Statistics:")
    for key, value in stats.items():
        print(f"  {key}: {value}")


def run_compare_mode(max_questions: int = 500, include_gemini: bool = False, gemini_api_key: str = None):
    """
    Run chatbot comparison.
    
    Args:
        max_questions: Maximum number of questions to evaluate
        include_gemini: Whether to include Gemini in comparison
        gemini_api_key: Gemini API key (or set GOOGLE_API_KEY env var)
    """
    print("\n" + "="*60)
    print("ğŸ”¬ Chatbot Comparison Mode")
    print("="*60)
    
    # Initialize chatbot
    # Check if Neo4j should be used
    use_neo4j = os.getenv("USE_NEO4J", "false").lower() == "true"
    neo4j_password = os.getenv("NEO4J_PASSWORD")
    
    if use_neo4j:
        if not neo4j_password:
            print("âš ï¸ USE_NEO4J=true but NEO4J_PASSWORD not set!")
            print("   Falling back to JSON file mode...")
            use_neo4j = False
    
    if use_neo4j:
        print("ğŸ“Š Using Neo4j Knowledge Graph...")
        chatbot = KpopChatbot(
            use_neo4j=True,
            neo4j_uri=os.getenv("NEO4J_URI", "bolt://localhost:7687"),
            neo4j_user=os.getenv("NEO4J_USER", "neo4j"),
            neo4j_password=neo4j_password,
            neo4j_database=os.getenv("NEO4J_DATABASE", None),
            verbose=True
        )
    else:
        print("ğŸ“Š Using JSON file Knowledge Graph...")
        chatbot = KpopChatbot(verbose=True)
    
    # Check if dataset exists
    dataset_path = "data/evaluation_dataset.json"
    if not os.path.exists(dataset_path):
        print("\nğŸ“ Generating evaluation dataset first...")
        generator = EvaluationDatasetGenerator()
        generator.generate_full_dataset(output_path=dataset_path)
        
    # Run comparison
    comparison = ChatbotComparison(
        kpop_chatbot=chatbot,
        google_api_key=gemini_api_key
    )
    questions = comparison.load_evaluation_dataset(dataset_path)
    
    results = comparison.compare_chatbots(
        questions,
        include_chatgpt=False,  # Set True if OpenAI API key available
        include_gemini=include_gemini,
        include_baseline=True,
        max_questions=max_questions
    )
    
    print("\nâœ… Comparison complete!")


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="K-pop Knowledge Graph Chatbot"
    )
    
    parser.add_argument(
        '--mode',
        choices=['cli', 'ui', 'streamlit', 'eval', 'compare'],
        default='cli',
        help='Cháº¿ Ä‘á»™ cháº¡y: cli (command line), ui (Gradio web), streamlit (Streamlit web), eval (táº¡o dataset), compare (so sÃ¡nh)'
    )
    
    parser.add_argument(
        '--num-questions',
        type=int,
        default=2000,
        help='Sá»‘ cÃ¢u há»i cho eval mode (máº·c Ä‘á»‹nh: 2000)'
    )
    
    parser.add_argument(
        '--max-compare',
        type=int,
        default=500,
        help='Sá»‘ cÃ¢u há»i tá»‘i Ä‘a cho compare mode (máº·c Ä‘á»‹nh: 500)'
    )
    
    parser.add_argument(
        '--chat-mode',
        choices=['standard', 'optimized'],
        default='standard',
        help='Cháº¿ Ä‘á»™ chatbot: standard (luÃ´n dÃ¹ng LLM - Ä‘Ã¡p á»©ng yÃªu cáº§u) hoáº·c optimized (tá»‘i Æ°u context)'
    )
    
    parser.add_argument(
        '--use-chatgpt',
        action='store_true',
        help='Sá»­ dá»¥ng ChatGPT Ä‘á»ƒ generate má»™t pháº§n questions (cáº§n OPENAI_API_KEY)'
    )
    
    parser.add_argument(
        '--chatgpt-ratio',
        type=float,
        default=0.2,
        help='Tá»· lá»‡ questions tá»« ChatGPT (0.0-1.0, máº·c Ä‘á»‹nh: 0.2 = 20%%)'
    )
    
    parser.add_argument(
        '--include-gemini',
        action='store_true',
        help='Bao gá»“m Gemini trong comparison (cáº§n GOOGLE_API_KEY hoáº·c --gemini-key)'
    )
    
    parser.add_argument(
        '--gemini-key',
        type=str,
        default=None,
        help='Google API key cho Gemini (hoáº·c set GOOGLE_API_KEY env var)'
    )
    
    args = parser.parse_args()
    
    if args.mode == 'cli':
        run_cli_mode(chat_mode=args.chat_mode)
    elif args.mode == 'ui':
        run_ui_mode(use_streamlit=False)
    elif args.mode == 'streamlit':
        run_ui_mode(use_streamlit=True)
    elif args.mode == 'eval':
        run_eval_mode(args.num_questions, args.use_chatgpt, args.chatgpt_ratio)
    elif args.mode == 'compare':
        run_compare_mode(args.max_compare, args.include_gemini, args.gemini_key)


if __name__ == "__main__":
    main()

