"""
Script ƒë·ªÉ merge d·ªØ li·ªáu t·ª´ 3 file JSON v√† ƒë·∫©y v√†o Neo4j
- korean_artists_graph_bfs.json: nodes v√† edges t·ª´ BFS crawl
- kpop_ner_result.json: entities t·ª´ NER
- kpop_relationships_result.json: relationships t·ª´ relationship extraction
"""
import sys
import io
import json
from typing import Dict, List, Any, Set
from datetime import datetime
from neo4j import GraphDatabase

# Robust UTF-8 console output on Windows
if sys.platform == 'win32':
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    except Exception:
        pass


def load_json_file(filepath: str) -> Dict:
    """ƒê·ªçc file JSON"""
    print(f"üìñ ƒêang ƒë·ªçc {filepath}...")
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"  ‚úì ƒê√£ ƒë·ªçc {filepath}")
    return data


def merge_data(
    bfs_data: Dict,
    ner_data: Dict,
    relationships_data: Dict,
    output_file: str = "merged_kpop_data.json"
) -> Dict:
    """
    Merge d·ªØ li·ªáu t·ª´ 3 file v√†o 1 file duy nh·∫•t
    
    Args:
        bfs_data: D·ªØ li·ªáu t·ª´ korean_artists_graph_bfs.json
        ner_data: D·ªØ li·ªáu t·ª´ kpop_ner_result.json
        relationships_data: D·ªØ li·ªáu t·ª´ kpop_relationships_result.json
        output_file: File output
    
    Returns:
        Dict ch·ª©a merged data
    """
    print("\n" + "=" * 70)
    print("MERGE D·ªÆ LI·ªÜU T·ª™ 3 FILE JSON")
    print("=" * 70)
    
    # Kh·ªüi t·∫°o merged data
    merged = {
        "metadata": {
            "merged_at": datetime.now().isoformat(),
            "source_files": [
                "korean_artists_graph_bfs.json",
                "kpop_ner_result.json",
                "kpop_relationships_result.json"
            ]
        },
        "nodes": {},
        "edges": []
    }
    
    # 1. Th√™m nodes t·ª´ BFS graph
    print("\nüìä B∆∞·ªõc 1: Th√™m nodes t·ª´ BFS graph...")
    bfs_nodes = bfs_data.get("nodes", {})
    merged["nodes"].update(bfs_nodes)
    print(f"  ‚úì ƒê√£ th√™m {len(bfs_nodes)} nodes t·ª´ BFS graph")
    
    # 2. Th√™m edges t·ª´ BFS graph
    print("\nüìä B∆∞·ªõc 2: Th√™m edges t·ª´ BFS graph...")
    bfs_edges = bfs_data.get("edges", [])
    merged["edges"].extend(bfs_edges)
    print(f"  ‚úì ƒê√£ th√™m {len(bfs_edges)} edges t·ª´ BFS graph")
    
    # 3. Th√™m entities t·ª´ NER (t·∫°o nodes m·ªõi n·∫øu ch∆∞a c√≥)
    print("\nüìä B∆∞·ªõc 3: Th√™m entities t·ª´ NER...")
    ner_entities = ner_data.get("entities", [])
    new_entities_count = 0
    existing_entities_count = 0
    
    for entity in ner_entities:
        entity_id = entity.get("text", "")
        entity_type = entity.get("type", "Entity")
        
        if not entity_id:
            continue
        
        # N·∫øu node ch∆∞a t·ªìn t·∫°i, t·∫°o m·ªõi
        if entity_id not in merged["nodes"]:
            merged["nodes"][entity_id] = {
                "label": entity_type,
                "title": entity_id,
                "properties": {
                    "method": entity.get("method", "unknown"),
                    "confidence": entity.get("confidence", 0.0),
                    "source_node": entity.get("source_node", ""),
                    "sources": entity.get("sources", [])
                }
            }
            new_entities_count += 1
        else:
            # C·∫≠p nh·∫≠t properties n·∫øu node ƒë√£ t·ªìn t·∫°i
            existing_node = merged["nodes"][entity_id]
            if "properties" not in existing_node:
                existing_node["properties"] = {}
            existing_node["properties"].update({
                "ner_method": entity.get("method", "unknown"),
                "ner_confidence": entity.get("confidence", 0.0),
                "ner_source_node": entity.get("source_node", ""),
                "ner_sources": entity.get("sources", [])
            })
            existing_entities_count += 1
    
    print(f"  ‚úì ƒê√£ th√™m {new_entities_count} entities m·ªõi")
    print(f"  ‚úì ƒê√£ c·∫≠p nh·∫≠t {existing_entities_count} entities ƒë√£ t·ªìn t·∫°i")
    
    # 4. Th√™m relationships t·ª´ relationship extraction
    print("\nüìä B∆∞·ªõc 4: Th√™m relationships t·ª´ relationship extraction...")
    relationships = relationships_data.get("relationships", [])
    new_relationships_count = 0
    duplicate_relationships_count = 0
    
    # T·∫°o set ƒë·ªÉ check duplicate
    existing_edges_set: Set[tuple] = set()
    for edge in merged["edges"]:
        key = (
            edge.get("source", ""),
            edge.get("target", ""),
            edge.get("type", "")
        )
        existing_edges_set.add(key)
    
    for rel in relationships:
        source = rel.get("source", "")
        target = rel.get("target", "")
        rel_type = rel.get("type", "")
        
        if not source or not target or not rel_type:
            continue
        
        # Check duplicate
        key = (source, target, rel_type)
        if key in existing_edges_set:
            duplicate_relationships_count += 1
            continue
        
        # Th√™m relationship m·ªõi
        merged["edges"].append({
            "source": source,
            "target": target,
            "type": rel_type,
            "text": f"{source} {rel_type} {target}",
            "properties": {
                "confidence": rel.get("confidence", 0.0),
                "method": rel.get("method", "unknown"),
                "source_type": rel.get("source_type", ""),
                "target_type": rel.get("target_type", "")
            }
        })
        existing_edges_set.add(key)
        new_relationships_count += 1
    
    print(f"  ‚úì ƒê√£ th√™m {new_relationships_count} relationships m·ªõi")
    print(f"  ‚úì ƒê√£ b·ªè qua {duplicate_relationships_count} relationships tr√πng l·∫∑p")
    
    # 5. C·∫≠p nh·∫≠t metadata
    merged["metadata"]["total_nodes"] = len(merged["nodes"])
    merged["metadata"]["total_edges"] = len(merged["edges"])
    merged["metadata"]["nodes_by_type"] = {}
    merged["metadata"]["edges_by_type"] = {}
    
    # ƒê·∫øm nodes theo type
    for node_id, node_data in merged["nodes"].items():
        label = node_data.get("label", "Entity")
        merged["metadata"]["nodes_by_type"][label] = merged["metadata"]["nodes_by_type"].get(label, 0) + 1
    
    # ƒê·∫øm edges theo type
    for edge in merged["edges"]:
        edge_type = edge.get("type", "RELATED_TO")
        merged["metadata"]["edges_by_type"][edge_type] = merged["metadata"]["edges_by_type"].get(edge_type, 0) + 1
    
    # 6. L∆∞u file
    print(f"\nüíæ ƒêang l∆∞u merged data v√†o {output_file}...")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(merged, f, ensure_ascii=False, indent=2)
    print(f"  ‚úì ƒê√£ l∆∞u merged data v√†o {output_file}")
    
    # In th·ªëng k√™
    print("\n" + "=" * 70)
    print("TH·ªêNG K√ä D·ªÆ LI·ªÜU SAU KHI MERGE")
    print("=" * 70)
    print(f"üìä T·ªïng s·ªë nodes: {merged['metadata']['total_nodes']}")
    print(f"üìä T·ªïng s·ªë edges: {merged['metadata']['total_edges']}")
    print("\nüìä Nodes theo type:")
    for label, count in sorted(merged["metadata"]["nodes_by_type"].items(), key=lambda x: -x[1]):
        print(f"  - {label}: {count}")
    print("\nüìä Edges theo type:")
    for edge_type, count in sorted(merged["metadata"]["edges_by_type"].items(), key=lambda x: -x[1]):
        print(f"  - {edge_type}: {count}")
    
    return merged


def import_to_neo4j(
    merged_data: Dict,
    uri: str,
    user: str,
    password: str,
    database: str = None,
    batch_size: int = 1000,
    create_constraints: bool = True
) -> None:
    """
    ƒê·∫©y d·ªØ li·ªáu merged v√†o Neo4j
    
    Args:
        merged_data: D·ªØ li·ªáu ƒë√£ merge
        uri: Neo4j URI (v√≠ d·ª•: bolt://localhost:7687)
        user: Username
        password: Password
        database: T√™n database (None = default)
        batch_size: K√≠ch th∆∞·ªõc batch khi import
        create_constraints: C√≥ t·∫°o constraints kh√¥ng
    """
    print("\n" + "=" * 70)
    print("IMPORT D·ªÆ LI·ªÜU V√ÄO NEO4J")
    print("=" * 70)
    
    driver = GraphDatabase.driver(uri, auth=(user, password))
    
    try:
        def run_write(tx, query, parameters=None):
            return tx.run(query, parameters or {})
        
        # Chu·∫©n h√≥a key cho Neo4j property
        def _strip_diacritics_to_ascii(text: str) -> str:
            import unicodedata as _ud
            if not isinstance(text, str):
                text = str(text)
            norm = _ud.normalize('NFD', text)
            norm = norm.replace('ƒë', 'd').replace('ƒê', 'D')
            return ''.join(ch for ch in norm if _ud.category(ch) != 'Mn')
        
        def _norm_key(s: str) -> str:
            import re as _re
            key = _strip_diacritics_to_ascii(str(s).strip().lower())
            key = _re.sub(r"[^a-z0-9]+", "_", key)
            key = _re.sub(r"_+", "_", key).strip('_')
            return key or "field"
        
        # Prepare nodes grouped by label
        print("\nüìä B∆∞·ªõc 1: Chu·∫©n b·ªã nodes...")
        label_to_nodes: Dict[str, List[Dict[str, Any]]] = {}
        nodes_data = merged_data.get("nodes", {})
        
        for node_id, node_data in nodes_data.items():
            label = node_data.get("label", "Entity")
            name = node_data.get("title") or node_id
            
            props = {
                "id": node_id,
                "name": name
            }
            
            # Th√™m URL n·∫øu c√≥
            if "url" in node_data:
                props["url"] = node_data["url"]
            
            # Th√™m properties n·∫øu c√≥
            if "properties" in node_data:
                for k, v in node_data["properties"].items():
                    if k not in props:
                        props[k] = v
            
            # Th√™m infobox fields cho c√°c labels ch√≠nh
            if label in ("Artist", "Group", "Song", "Album", "Company", "Genre"):
                infobox = node_data.get("infobox") or {}
                if isinstance(infobox, dict) and infobox:
                    for raw_k, raw_v in infobox.items():
                        k_norm = _norm_key(raw_k)
                        if k_norm not in ("id", "name", "url"):
                            props[k_norm] = str(raw_v)
            
            label_to_nodes.setdefault(label, []).append({
                "id": node_id,
                "props": props
            })
        
        print(f"  ‚úì ƒê√£ chu·∫©n b·ªã {len(nodes_data)} nodes")
        for label, items in label_to_nodes.items():
            print(f"    - {label}: {len(items)} nodes")
        
        # Prepare relationships
        print("\nüìä B∆∞·ªõc 2: Chu·∫©n b·ªã relationships...")
        relationships: List[Dict[str, Any]] = []
        edges_data = merged_data.get("edges", [])
        
        for edge in edges_data:
            src = edge.get("source")
            tgt = edge.get("target")
            typ = edge.get("type") or "RELATED_TO"
            
            if not src or not tgt:
                continue
            
            rel_props = {"text": edge.get("text", "")}
            
            # Th√™m properties n·∫øu c√≥
            if "properties" in edge:
                rel_props.update(edge["properties"])
            
            relationships.append({
                "sourceId": src,
                "targetId": tgt,
                "type": typ,
                "props": rel_props
            })
        
        print(f"  ‚úì ƒê√£ chu·∫©n b·ªã {len(relationships)} relationships")
        
        # Group relationships by type
        type_to_rels: Dict[str, List[Dict[str, Any]]] = {}
        for r in relationships:
            type_to_rels.setdefault(r["type"], []).append(r)
        
        for rel_type, rels in type_to_rels.items():
            print(f"    - {rel_type}: {len(rels)} relationships")
        
        # Cypher templates
        node_query_tpl = lambda label: f"""
        UNWIND $batch AS n
        MERGE (x:`{label}` {{id: n.id}})
        SET x += n.props
        """
        
        rel_query_tpl = lambda rel_type: f"""
        UNWIND $batch AS r
        MATCH (s {{id: r.sourceId}}), (t {{id: r.targetId}})
        MERGE (s)-[e:`{rel_type}`]->(t)
        SET e += r.props
        """
        
        # Open session
        print("\nüìä B∆∞·ªõc 3: K·∫øt n·ªëi Neo4j v√† import d·ªØ li·ªáu...")
        with driver.session(database=database) if database else driver.session() as session:
            # T·∫°o constraints n·∫øu c·∫ßn
            if create_constraints:
                print("  üîß ƒêang t·∫°o constraints...")
                constraints = [
                    "CREATE CONSTRAINT artist_id IF NOT EXISTS FOR (n:Artist) REQUIRE n.id IS UNIQUE",
                    "CREATE CONSTRAINT group_id IF NOT EXISTS FOR (n:Group) REQUIRE n.id IS UNIQUE",
                    "CREATE CONSTRAINT album_id IF NOT EXISTS FOR (n:Album) REQUIRE n.id IS UNIQUE",
                    "CREATE CONSTRAINT song_id IF NOT EXISTS FOR (n:Song) REQUIRE n.id IS UNIQUE",
                    "CREATE CONSTRAINT company_id IF NOT EXISTS FOR (n:Company) REQUIRE n.id IS UNIQUE",
                    "CREATE CONSTRAINT genre_id IF NOT EXISTS FOR (n:Genre) REQUIRE n.id IS UNIQUE",
                ]
                for q in constraints:
                    try:
                        session.execute_write(run_write, q)
                    except Exception as e:
                        # Constraint c√≥ th·ªÉ ƒë√£ t·ªìn t·∫°i
                        pass
                print("  ‚úì ƒê√£ t·∫°o constraints")
            
            # Import nodes theo label
            print("\nüìä B∆∞·ªõc 4: ƒêang import nodes...")
            total_nodes_imported = 0
            for label, items in label_to_nodes.items():
                print(f"  üì• ƒêang import {len(items)} {label} nodes...")
                for i in range(0, len(items), batch_size):
                    batch = items[i:i+batch_size]
                    session.execute_write(run_write, node_query_tpl(label), {"batch": batch})
                    total_nodes_imported += len(batch)
                    if (i // batch_size + 1) % 10 == 0:
                        print(f"    ‚úì ƒê√£ import {total_nodes_imported} nodes...")
            print(f"  ‚úì ƒê√£ import {total_nodes_imported} nodes")
            
            # Import relationships theo type
            print("\nüìä B∆∞·ªõc 5: ƒêang import relationships...")
            total_rels_imported = 0
            for rel_type, rels in type_to_rels.items():
                print(f"  üì• ƒêang import {len(rels)} {rel_type} relationships...")
                for i in range(0, len(rels), batch_size):
                    batch = rels[i:i+batch_size]
                    session.execute_write(run_write, rel_query_tpl(rel_type), {"batch": batch})
                    total_rels_imported += len(batch)
                    if (i // batch_size + 1) % 10 == 0:
                        print(f"    ‚úì ƒê√£ import {total_rels_imported} relationships...")
            print(f"  ‚úì ƒê√£ import {total_rels_imported} relationships")
        
        print("\n" + "=" * 70)
        print("‚úì HO√ÄN T·∫§T IMPORT V√ÄO NEO4J")
        print("=" * 70)
        
    finally:
        driver.close()


def main():
    """H√†m main"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Merge 3 file JSON v√† import v√†o Neo4j")
    parser.add_argument("--bfs-file", default="korean_artists_graph_bfs.json",
                        help="File BFS graph JSON")
    parser.add_argument("--ner-file", default="kpop_ner_result.json",
                        help="File NER result JSON")
    parser.add_argument("--relationships-file", default="kpop_relationships_result.json",
                        help="File relationships result JSON")
    parser.add_argument("--output-file", default="merged_kpop_data.json",
                        help="File output merged")
    parser.add_argument("--neo4j-uri", default="bolt://localhost:7687",
                        help="Neo4j URI")
    parser.add_argument("--neo4j-user", default="neo4j",
                        help="Neo4j username")
    parser.add_argument("--neo4j-password", required=True,
                        help="Neo4j password")
    parser.add_argument("--neo4j-database", default=None,
                        help="Neo4j database name (None = default)")
    parser.add_argument("--batch-size", type=int, default=1000,
                        help="Batch size cho import")
    parser.add_argument("--no-constraints", action="store_true",
                        help="Kh√¥ng t·∫°o constraints")
    parser.add_argument("--merge-only", action="store_true",
                        help="Ch·ªâ merge, kh√¥ng import v√†o Neo4j")
    
    args = parser.parse_args()
    
    # Load d·ªØ li·ªáu
    bfs_data = load_json_file(args.bfs_file)
    ner_data = load_json_file(args.ner_file)
    relationships_data = load_json_file(args.relationships_file)
    
    # Merge d·ªØ li·ªáu
    merged_data = merge_data(bfs_data, ner_data, relationships_data, args.output_file)
    
    # Import v√†o Neo4j n·∫øu kh√¥ng ph·∫£i merge-only
    if not args.merge_only:
        import_to_neo4j(
            merged_data,
            args.neo4j_uri,
            args.neo4j_user,
            args.neo4j_password,
            args.neo4j_database,
            args.batch_size,
            not args.no_constraints
        )


if __name__ == "__main__":
    main()

