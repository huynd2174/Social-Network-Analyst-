"""
Script ƒë·ªÉ merge d·ªØ li·ªáu t·ª´ 3 file JSON v√† ƒë·∫©y v√†o Neo4j
- korean_artists_graph_bfs.json: nodes v√† edges t·ª´ BFS crawl
- kpop_ner_result.json: entities t·ª´ NER
- kpop_relationships_result.json: relationships t·ª´ relationship extraction
"""
import sys
import io
import json
import re
from typing import Dict, List, Any, Set, Tuple
from datetime import datetime
from collections import defaultdict
from neo4j import GraphDatabase

# Robust UTF-8 console output on Windows
if sys.platform == 'win32':
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    except Exception:
        pass


def normalize_node_name(name: str) -> str:
    """
    Chu·∫©n h√≥a t√™n node ƒë·ªÉ so s√°nh nh·∫•t qu√°n.
    Gi·ªëng v·ªõi normalize_node_name trong run_relationship_extraction.py
    """
    if not name:
        return ""
    
    # Lo·∫°i b·ªè c√°c pattern trong ngo·∫∑c ƒë∆°n ·ªü cu·ªëi
    name = re.sub(r'\s*\([^)]*(?:ca sƒ©|nh√≥m nh·∫°c|ban nh·∫°c|ngh·ªá sƒ©|singer|group|band)[^)]*\)\s*$', '', name, flags=re.IGNORECASE)
    
    # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
    name = re.sub(r'\s+', ' ', name)
    name = name.strip()
    
    return name


def normalize_for_comparison(name: str) -> str:
    """
    Chu·∫©n h√≥a t√™n ƒë·ªÉ so s√°nh (lo·∫°i b·ªè kho·∫£ng tr·∫Øng, d·∫•u g·∫°ch n·ªëi, lowercase)
    D√πng ƒë·ªÉ match entities gi·ªØa NER v√† Relationships
    
    X·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p:
    - "Ahn Ji-young" vs "Ahn Ji young" -> c√πng m·ªôt node
    - "Miyeon" vs "Miyeon (ca sƒ©)" -> c√πng m·ªôt node
    """
    normalized = normalize_node_name(name)
    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng, d·∫•u g·∫°ch n·ªëi, v√† lowercase ƒë·ªÉ so s√°nh
    # ƒêi·ªÅu n√†y gi√∫p match "Ahn Ji-young" v·ªõi "Ahn Ji young"
    normalized = normalized.lower().replace(' ', '').replace('-', '').replace('_', '')
    return normalized


def load_json_file(filepath: str) -> Dict:
    """ƒê·ªçc file JSON"""
    print(f"üìñ ƒêang ƒë·ªçc {filepath}...")
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"  ‚úì ƒê√£ ƒë·ªçc {filepath}")
    return data


def merge_data(
    bfs_data: Dict,
    ner_data: Dict,
    relationships_data: Dict,
    output_file: str = "merged_kpop_data.json"
) -> Dict:
    """
    Merge d·ªØ li·ªáu t·ª´ 3 file v√†o 1 file duy nh·∫•t
    
    Args:
        bfs_data: D·ªØ li·ªáu t·ª´ korean_artists_graph_bfs.json
        ner_data: D·ªØ li·ªáu t·ª´ kpop_ner_result.json
        relationships_data: D·ªØ li·ªáu t·ª´ kpop_relationships_result.json
        output_file: File output
    
    Returns:
        Dict ch·ª©a merged data
    """
    print("\n" + "=" * 70)
    print("MERGE D·ªÆ LI·ªÜU T·ª™ 3 FILE JSON")
    print("=" * 70)
    
    # Kh·ªüi t·∫°o merged data
    merged = {
        "metadata": {
            "merged_at": datetime.now().isoformat(),
            "source_files": [
                "korean_artists_graph_bfs.json",
                "kpop_ner_result.json",
                "kpop_relationships_result.json"
            ]
        },
        "nodes": {},
        "edges": []
    }
    
    # 1. Th√™m nodes t·ª´ BFS graph
    print("\nüìä B∆∞·ªõc 1: Th√™m nodes t·ª´ BFS graph...")
    bfs_nodes = bfs_data.get("nodes", {})
    merged["nodes"].update(bfs_nodes)
    print(f"  ‚úì ƒê√£ th√™m {len(bfs_nodes)} nodes t·ª´ BFS graph")
    
    # 2. Th√™m edges t·ª´ BFS graph
    print("\nüìä B∆∞·ªõc 2: Th√™m edges t·ª´ BFS graph...")
    bfs_edges = bfs_data.get("edges", [])
    merged["edges"].extend(bfs_edges)
    print(f"  ‚úì ƒê√£ th√™m {len(bfs_edges)} edges t·ª´ BFS graph")
    
    # 3. Th√™m entities t·ª´ NER (t·∫°o nodes m·ªõi n·∫øu ch∆∞a c√≥)
    print("\nüìä B∆∞·ªõc 3: Th√™m entities t·ª´ NER...")
    ner_entities = ner_data.get("entities", [])
    new_entities_count = 0
    existing_entities_count = 0
    
    # T·∫°o mapping t·ª´ (normalized name, type) -> original name ƒë·ªÉ match ch√≠nh x√°c
    # QUAN TR·ªåNG: Ph·∫£i check c·∫£ type ƒë·ªÉ tr√°nh tr√πng l·∫∑p gi·ªØa c√°c type kh√°c nhau
    # QUAN TR·ªåNG: L∆∞u th√¥ng tin node n√†o t·ª´ BFS ƒë·ªÉ ∆∞u ti√™n gi·ªØ l·∫°i
    normalized_to_original: Dict[Tuple[str, str], str] = {}
    bfs_node_ids = set(bfs_nodes.keys())  # L∆∞u danh s√°ch node IDs t·ª´ BFS
    for node_id in merged["nodes"].keys():
        normalized_key = normalize_for_comparison(node_id)
        node_type = merged["nodes"][node_id].get("label", "Entity")
        key = (normalized_key, node_type)
        if key not in normalized_to_original:
            normalized_to_original[key] = node_id
    
    for entity in ner_entities:
        entity_id = entity.get("text", "")
        entity_type = entity.get("type", "Entity")
        
        if not entity_id:
            continue
        
        # Chu·∫©n h√≥a ƒë·ªÉ t√¨m node ƒë√£ t·ªìn t·∫°i
        normalized_key = normalize_for_comparison(entity_id)
        key = (normalized_key, entity_type)
        existing_node_id = normalized_to_original.get(key)
        
        if existing_node_id:
            # Node ƒë√£ t·ªìn t·∫°i V√Ä C√ôNG TYPE (c√≥ th·ªÉ t√™n kh√°c m·ªôt ch√∫t do chu·∫©n h√≥a)
            existing_node = merged["nodes"][existing_node_id]
            existing_node_type = existing_node.get("label", "Entity")
            
            # Double check: ph·∫£i c√πng type m·ªõi c·∫≠p nh·∫≠t
            if existing_node_type == entity_type:
                # QUAN TR·ªåNG: N·∫øu node t·ª´ BFS graph (c√≥ infobox), ch·ªâ c·∫≠p nh·∫≠t properties, KH√îNG ghi ƒë√®
                if existing_node_id in bfs_node_ids:
                    # Node t·ª´ BFS -> gi·ªØ nguy√™n, ch·ªâ th√™m NER properties
                    if "properties" not in existing_node:
                        existing_node["properties"] = {}
                    existing_node["properties"].update({
                        "ner_method": entity.get("method", "unknown"),
                        "ner_confidence": entity.get("confidence", 0.0),
                        "ner_source_node": entity.get("source_node", ""),
                        "ner_sources": entity.get("sources", [])
                    })
                else:
                    # Node kh√¥ng ph·∫£i t·ª´ BFS -> c·∫≠p nh·∫≠t nh∆∞ b√¨nh th∆∞·ªùng
                    if "properties" not in existing_node:
                        existing_node["properties"] = {}
                    existing_node["properties"].update({
                        "ner_method": entity.get("method", "unknown"),
                        "ner_confidence": entity.get("confidence", 0.0),
                        "ner_source_node": entity.get("source_node", ""),
                        "ner_sources": entity.get("sources", [])
                    })
                existing_entities_count += 1
            else:
                # T√™n gi·ªëng nh∆∞ng type kh√°c -> cho ph√©p c·∫£ hai c√πng t·ªìn t·∫°i
                # Ki·ªÉm tra xem c√≥ node n√†o v·ªõi c√πng t√™n (entity_id) nh∆∞ng type kh√°c kh√¥ng
                if entity_id in merged["nodes"]:
                    # ƒê√£ c√≥ node v·ªõi t√™n n√†y (c√≥ th·ªÉ type kh√°c) -> t·∫°o node m·ªõi v·ªõi t√™n kh√°c
                    new_entity_id = f"{entity_id} ({entity_type})"
                    # Ki·ªÉm tra xem t√™n m·ªõi ƒë√£ t·ªìn t·∫°i ch∆∞a
                    if new_entity_id in merged["nodes"]:
                        # T√™n ƒë√£ t·ªìn t·∫°i -> b·ªè qua
                        continue
                    # T·∫°o node m·ªõi v·ªõi t√™n kh√°c
                    merged["nodes"][new_entity_id] = {
                        "label": entity_type,
                        "title": new_entity_id,
                        "properties": {
                            "method": entity.get("method", "unknown"),
                            "confidence": entity.get("confidence", 0.0),
                            "source_node": entity.get("source_node", ""),
                            "sources": entity.get("sources", []),
                            "original_name": entity_id  # L∆∞u t√™n g·ªëc ƒë·ªÉ reference
                        }
                    }
                    normalized_to_original[key] = new_entity_id
                    new_entities_count += 1
                else:
                    # Ch∆∞a c√≥ node v·ªõi t√™n n√†y -> t·∫°o node m·ªõi v·ªõi t√™n g·ªëc
                    merged["nodes"][entity_id] = {
                        "label": entity_type,
                        "title": entity_id,
                        "properties": {
                            "method": entity.get("method", "unknown"),
                            "confidence": entity.get("confidence", 0.0),
                            "source_node": entity.get("source_node", ""),
                            "sources": entity.get("sources", [])
                        }
                    }
                    normalized_to_original[key] = entity_id
                    new_entities_count += 1
        else:
            # Node ch∆∞a t·ªìn t·∫°i v·ªõi key (normalized_name, type) n√†y
            # Ki·ªÉm tra xem c√≥ node n√†o v·ªõi c√πng t√™n (entity_id) nh∆∞ng type kh√°c kh√¥ng
            if entity_id in merged["nodes"]:
                # ƒê√£ c√≥ node v·ªõi t√™n n√†y (c√≥ th·ªÉ type kh√°c) -> t·∫°o node m·ªõi v·ªõi t√™n kh√°c
                new_entity_id = f"{entity_id} ({entity_type})"
                # Ki·ªÉm tra xem t√™n m·ªõi ƒë√£ t·ªìn t·∫°i ch∆∞a
                if new_entity_id in merged["nodes"]:
                    # T√™n ƒë√£ t·ªìn t·∫°i -> b·ªè qua
                    continue
                # T·∫°o node m·ªõi v·ªõi t√™n kh√°c
                merged["nodes"][new_entity_id] = {
                    "label": entity_type,
                    "title": new_entity_id,
                    "properties": {
                        "method": entity.get("method", "unknown"),
                        "confidence": entity.get("confidence", 0.0),
                        "source_node": entity.get("source_node", ""),
                        "sources": entity.get("sources", []),
                        "original_name": entity_id  # L∆∞u t√™n g·ªëc ƒë·ªÉ reference
                    }
                }
                normalized_to_original[key] = new_entity_id
                new_entities_count += 1
            else:
                # Ch∆∞a c√≥ node v·ªõi t√™n n√†y -> t·∫°o node m·ªõi v·ªõi t√™n g·ªëc
                merged["nodes"][entity_id] = {
                    "label": entity_type,
                    "title": entity_id,
                    "properties": {
                        "method": entity.get("method", "unknown"),
                        "confidence": entity.get("confidence", 0.0),
                        "source_node": entity.get("source_node", ""),
                        "sources": entity.get("sources", [])
                    }
                }
                normalized_to_original[key] = entity_id
                new_entities_count += 1
    
    print(f"  ‚úì ƒê√£ th√™m {new_entities_count} entities m·ªõi")
    print(f"  ‚úì ƒê√£ c·∫≠p nh·∫≠t {existing_entities_count} entities ƒë√£ t·ªìn t·∫°i")
    
    # 4. Th√™m relationships t·ª´ relationship extraction
    print("\nüìä B∆∞·ªõc 4: Th√™m relationships t·ª´ relationship extraction...")
    relationships = relationships_data.get("relationships", [])
    new_relationships_count = 0
    duplicate_relationships_count = 0
    skipped_missing_nodes = 0
    
    # T·∫°o set ƒë·ªÉ check duplicate
    existing_edges_set: Set[tuple] = set()
    for edge in merged["edges"]:
        key = (
            edge.get("source", ""),
            edge.get("target", ""),
            edge.get("type", "")
        )
        existing_edges_set.add(key)
    
    # T·∫°o mapping normalized -> original cho t·∫•t c·∫£ nodes hi·ªán c√≥
    # D√πng ƒë·ªÉ t√¨m node ƒë√£ t·ªìn t·∫°i d·ª±a tr√™n normalized name (check tr√πng l·∫∑p t·ªët h∆°n)
    # QUAN TR·ªåNG: T·∫°o mapping SAU KHI ƒë√£ merge t·∫•t c·∫£ nodes (BFS + NER)
    normalized_to_original_rel: Dict[str, str] = {}
    # T·∫°o mapping v·ªõi t·∫•t c·∫£ c√°c bi·∫øn th·ªÉ t√™n c√≥ th·ªÉ c√≥
    # V√Ä t·∫°o mapping t·ª´ c√°c ph·∫ßn t√™n (ƒë·ªÉ match "Miyeon" v·ªõi "Cho Mi-yeon")
    name_parts_to_nodes: Dict[str, List[str]] = defaultdict(list)
    
    for node_id in merged["nodes"].keys():
        normalized_key = normalize_for_comparison(node_id)
        # N·∫øu c√≥ nhi·ªÅu node c√πng normalized name, ∆∞u ti√™n node t·ª´ BFS graph (t√™n g·ªëc)
        if normalized_key not in normalized_to_original_rel:
            normalized_to_original_rel[normalized_key] = node_id
        else:
            # ∆Øu ti√™n node t·ª´ BFS graph n·∫øu c√≥
            existing_node_id = normalized_to_original_rel[normalized_key]
            if node_id in bfs_nodes and existing_node_id not in bfs_nodes:
                normalized_to_original_rel[normalized_key] = node_id
        
        # T·∫°o mapping t·ª´ c√°c ph·∫ßn t√™n (t√°ch theo kho·∫£ng tr·∫Øng v√† d·∫•u g·∫°ch n·ªëi)
        # V√≠ d·ª•: "Cho Mi-yeon" -> ["cho", "mi", "yeon"]
        # ƒê·ªÉ match "Miyeon" -> "miyeon" v·ªõi "Cho Mi-yeon" -> "chomiyeon"
        name_parts = re.split(r'[\s\-_]+', normalized_key)
        for part in name_parts:
            if len(part) >= 3:  # Ch·ªâ l∆∞u c√°c ph·∫ßn c√≥ ƒë·ªô d√†i >= 3
                name_parts_to_nodes[part].append(node_id)
        
        # TH√äM: L∆∞u to√†n b·ªô normalized name (n·∫øu ƒë·ªß d√†i) ƒë·ªÉ t√¨m substring tr·ª±c ti·∫øp
        # V√≠ d·ª•: "chomiyeon" s·∫Ω match v·ªõi "miyeon" n·∫øu "miyeon" ƒë∆∞·ª£c t√¨m trong name_parts_to_nodes
        if len(normalized_key) >= 3:
            name_parts_to_nodes[normalized_key].append(node_id)
    
    # Th·ªëng k√™ relationships b·ªã b·ªè qua ƒë·ªÉ debug
    missing_source_stats = defaultdict(int)
    missing_target_stats = defaultdict(int)
    
    for rel in relationships:
        source_original = rel.get("source", "")
        target_original = rel.get("target", "")
        rel_type = rel.get("type", "")
        source_type = rel.get("source_type", "")
        target_type = rel.get("target_type", "")
        
        if not source_original or not target_original or not rel_type:
            continue
        
        # Chu·∫©n h√≥a ƒë·ªÉ t√¨m node ƒë√£ t·ªìn t·∫°i (c√≥ th·ªÉ t√™n kh√°c m·ªôt ch√∫t do chu·∫©n h√≥a)
        source_normalized = normalize_for_comparison(source_original)
        target_normalized = normalize_for_comparison(target_original)
        
        source_node_id = normalized_to_original_rel.get(source_normalized)
        target_node_id = normalized_to_original_rel.get(target_normalized)
        
        # N·∫øu kh√¥ng t√¨m th·∫•y b·∫±ng normalized name, th·ª≠ t√¨m b·∫±ng name parts
        # V√≠ d·ª•: "Miyeon" kh√¥ng kh·ªõp v·ªõi "Cho Mi-yeon", nh∆∞ng "miyeon" c√≥ trong "chomiyeon"
        if not source_node_id:
            candidates = []
            
            # C√ÅCH 1: T√¨m tr·ª±c ti·∫øp source_normalized trong name_parts_to_nodes
            # (n·∫øu source_normalized l√† substring c·ªßa m·ªôt normalized name)
            if source_normalized in name_parts_to_nodes:
                candidates.extend(name_parts_to_nodes[source_normalized])
            
            # C√ÅCH 2: T√°ch source th√†nh c√°c ph·∫ßn v√† t√¨m
            source_parts = re.split(r'[\s\-_]+', source_normalized)
            for part in source_parts:
                if len(part) >= 3 and part in name_parts_to_nodes:
                    candidates.extend(name_parts_to_nodes[part])
            
            # C√ÅCH 3: T√¨m trong t·∫•t c·∫£ nodes xem c√≥ node n√†o c√≥ normalized name ch·ª©a source_normalized
            if not candidates:
                for node_id in merged["nodes"].keys():
                    candidate_norm = normalize_for_comparison(node_id)
                    if source_normalized in candidate_norm:
                        candidate_type = merged["nodes"][node_id].get("label", "")
                        if not source_type or candidate_type == source_type:
                            candidates.append(node_id)
            
            # Lo·∫°i b·ªè duplicate candidates
            candidates = list(set(candidates))
            
            # T√¨m node t·ªët nh·∫•t: node c√≥ normalized name ch·ª©a source ho·∫∑c ng∆∞·ª£c l·∫°i
            best_match = None
            best_score = 0
            
            for candidate in candidates:
                candidate_norm = normalize_for_comparison(candidate)
                candidate_type = merged["nodes"][candidate].get("label", "")
                
                # Ki·ªÉm tra type c√≥ kh·ªõp kh√¥ng
                if source_type and candidate_type != source_type:
                    continue
                
                # T√≠nh ƒëi·ªÉm match:
                # - N·∫øu source l√† substring c·ªßa candidate: ƒëi·ªÉm cao
                # - N·∫øu candidate l√† substring c·ªßa source: ƒëi·ªÉm th·∫•p h∆°n
                # - N·∫øu c√≥ nhi·ªÅu ph·∫ßn kh·ªõp: ƒëi·ªÉm cao h∆°n
                score = 0
                if source_normalized in candidate_norm:
                    # Source l√† substring c·ªßa candidate (v√≠ d·ª•: "miyeon" trong "chomiyeon")
                    score = 100 + len(source_normalized)
                elif candidate_norm in source_normalized:
                    # Candidate l√† substring c·ªßa source (√≠t ph·ªï bi·∫øn h∆°n)
                    score = 50 + len(candidate_norm)
                else:
                    # ƒê·∫øm s·ªë ph·∫ßn kh·ªõp
                    matching_parts = sum(1 for part in source_parts if part in candidate_norm)
                    if matching_parts > 0:
                        score = matching_parts * 10
                
                if score > best_score:
                    best_score = score
                    best_match = candidate
            
            if best_match:
                source_node_id = best_match
        
        if not target_node_id:
            # T∆∞∆°ng t·ª± cho target
            candidates = []
            
            # C√ÅCH 1: T√¨m tr·ª±c ti·∫øp target_normalized trong name_parts_to_nodes
            if target_normalized in name_parts_to_nodes:
                candidates.extend(name_parts_to_nodes[target_normalized])
            
            # C√ÅCH 2: T√°ch target th√†nh c√°c ph·∫ßn v√† t√¨m
            target_parts = re.split(r'[\s\-_]+', target_normalized)
            for part in target_parts:
                if len(part) >= 3 and part in name_parts_to_nodes:
                    candidates.extend(name_parts_to_nodes[part])
            
            # C√ÅCH 3: T√¨m trong t·∫•t c·∫£ nodes
            if not candidates:
                for node_id in merged["nodes"].keys():
                    candidate_norm = normalize_for_comparison(node_id)
                    if target_normalized in candidate_norm:
                        candidate_type = merged["nodes"][node_id].get("label", "")
                        if not target_type or candidate_type == target_type:
                            candidates.append(node_id)
            
            # Lo·∫°i b·ªè duplicate candidates
            candidates = list(set(candidates))
            
            best_match = None
            best_score = 0
            
            for candidate in candidates:
                candidate_norm = normalize_for_comparison(candidate)
                candidate_type = merged["nodes"][candidate].get("label", "")
                
                # Ki·ªÉm tra type c√≥ kh·ªõp kh√¥ng
                if target_type and candidate_type != target_type:
                    continue
                
                # T√≠nh ƒëi·ªÉm match t∆∞∆°ng t·ª± nh∆∞ source
                score = 0
                if target_normalized in candidate_norm:
                    score = 100 + len(target_normalized)
                elif candidate_norm in target_normalized:
                    score = 50 + len(candidate_norm)
                else:
                    matching_parts = sum(1 for part in target_parts if part in candidate_norm)
                    if matching_parts > 0:
                        score = matching_parts * 10
                
                if score > best_score:
                    best_score = score
                    best_match = candidate
            
            if best_match:
                target_node_id = best_match
        
        # CH·ªà th√™m relationship n·∫øu C·∫¢ HAI nodes ƒë·ªÅu t·ªìn t·∫°i
        # KH√îNG t·∫°o node m·ªõi - ch·ªâ d√πng nodes ƒë√£ c√≥ s·∫µn
        if not source_node_id:
            skipped_missing_nodes += 1
            missing_source_stats[source_type] += 1
            continue
        
        if not target_node_id:
            skipped_missing_nodes += 1
            missing_target_stats[target_type] += 1
            continue
        
        # D√πng t√™n node g·ªëc ƒë√£ t·ªìn t·∫°i
        source = source_node_id
        target = target_node_id
        
        # Check duplicate (d√πng t√™n g·ªëc ƒë√£ t·ªìn t·∫°i)
        key = (source, target, rel_type)
        if key in existing_edges_set:
            duplicate_relationships_count += 1
            continue
        
        # Th√™m relationship m·ªõi
        merged["edges"].append({
            "source": source,
            "target": target,
            "type": rel_type,
            "text": f"{source} {rel_type} {target}",
            "properties": {
                "confidence": rel.get("confidence", 0.0),
                "method": rel.get("method", "unknown"),
                "source_type": source_type,
                "target_type": target_type
            }
        })
        existing_edges_set.add(key)
        new_relationships_count += 1
    
    print(f"  ‚úì ƒê√£ th√™m {new_relationships_count} relationships m·ªõi")
    print(f"  ‚úì ƒê√£ b·ªè qua {duplicate_relationships_count} relationships tr√πng l·∫∑p")
    print(f"  ‚úì ƒê√£ b·ªè qua {skipped_missing_nodes} relationships (thi·∫øu source/target node)")
    
    if skipped_missing_nodes > 0:
        print(f"\n  üìä Th·ªëng k√™ relationships b·ªã b·ªè qua:")
        if missing_source_stats:
            print(f"    - Thi·∫øu source node:")
            for node_type, count in sorted(missing_source_stats.items(), key=lambda x: -x[1]):
                print(f"      ‚Ä¢ {node_type}: {count}")
        if missing_target_stats:
            print(f"    - Thi·∫øu target node:")
            for node_type, count in sorted(missing_target_stats.items(), key=lambda x: -x[1]):
                print(f"      ‚Ä¢ {node_type}: {count}")
        print(f"\n  üí° L∆∞u √Ω: C√°c relationships n√†y b·ªã b·ªè qua v√¨ source/target node kh√¥ng t·ªìn t·∫°i.")
        print(f"     ƒê·∫£m b·∫£o c√°c Artist t·ª´ infobox ƒë√£ ƒë∆∞·ª£c th√™m v√†o NER result tr∆∞·ªõc khi merge.")
    
    # 5. C·∫≠p nh·∫≠t metadata
    merged["metadata"]["total_nodes"] = len(merged["nodes"])
    merged["metadata"]["total_edges"] = len(merged["edges"])
    merged["metadata"]["nodes_by_type"] = {}
    merged["metadata"]["edges_by_type"] = {}
    
    # ƒê·∫øm nodes theo type
    for node_id, node_data in merged["nodes"].items():
        label = node_data.get("label", "Entity")
        merged["metadata"]["nodes_by_type"][label] = merged["metadata"]["nodes_by_type"].get(label, 0) + 1
    
    # ƒê·∫øm edges theo type
    for edge in merged["edges"]:
        edge_type = edge.get("type", "RELATED_TO")
        merged["metadata"]["edges_by_type"][edge_type] = merged["metadata"]["edges_by_type"].get(edge_type, 0) + 1
    
    # 6. L∆∞u file
    print(f"\nüíæ ƒêang l∆∞u merged data v√†o {output_file}...")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(merged, f, ensure_ascii=False, indent=2)
    print(f"  ‚úì ƒê√£ l∆∞u merged data v√†o {output_file}")
    
    # In th·ªëng k√™
    print("\n" + "=" * 70)
    print("TH·ªêNG K√ä D·ªÆ LI·ªÜU SAU KHI MERGE")
    print("=" * 70)
    print(f"üìä T·ªïng s·ªë nodes: {merged['metadata']['total_nodes']}")
    print(f"üìä T·ªïng s·ªë edges: {merged['metadata']['total_edges']}")
    print("\nüìä Nodes theo type:")
    for label, count in sorted(merged["metadata"]["nodes_by_type"].items(), key=lambda x: -x[1]):
        print(f"  - {label}: {count}")
    print("\nüìä Edges theo type:")
    for edge_type, count in sorted(merged["metadata"]["edges_by_type"].items(), key=lambda x: -x[1]):
        print(f"  - {edge_type}: {count}")
    
    return merged


def import_to_neo4j(
    merged_data: Dict,
    uri: str,
    user: str,
    password: str,
    database: str = None,
    batch_size: int = 1000,
    create_constraints: bool = True
) -> None:
    """
    ƒê·∫©y d·ªØ li·ªáu merged v√†o Neo4j
    
    Args:
        merged_data: D·ªØ li·ªáu ƒë√£ merge
        uri: Neo4j URI (v√≠ d·ª•: bolt://localhost:7687)
        user: Username
        password: Password
        database: T√™n database (None = default)
        batch_size: K√≠ch th∆∞·ªõc batch khi import
        create_constraints: C√≥ t·∫°o constraints kh√¥ng
    """
    print("\n" + "=" * 70)
    print("IMPORT D·ªÆ LI·ªÜU V√ÄO NEO4J")
    print("=" * 70)
    
    driver = GraphDatabase.driver(uri, auth=(user, password))
    
    try:
        def run_write(tx, query, parameters=None):
            return tx.run(query, parameters or {})
        
        # Chu·∫©n h√≥a key cho Neo4j property
        def _strip_diacritics_to_ascii(text: str) -> str:
            import unicodedata as _ud
            if not isinstance(text, str):
                text = str(text)
            norm = _ud.normalize('NFD', text)
            norm = norm.replace('ƒë', 'd').replace('ƒê', 'D')
            return ''.join(ch for ch in norm if _ud.category(ch) != 'Mn')
        
        def _norm_key(s: str) -> str:
            import re as _re
            key = _strip_diacritics_to_ascii(str(s).strip().lower())
            key = _re.sub(r"[^a-z0-9]+", "_", key)
            key = _re.sub(r"_+", "_", key).strip('_')
            return key or "field"
        
        # Prepare nodes grouped by label
        print("\nüìä B∆∞·ªõc 1: Chu·∫©n b·ªã nodes...")
        label_to_nodes: Dict[str, List[Dict[str, Any]]] = {}
        nodes_data = merged_data.get("nodes", {})
        
        for node_id, node_data in nodes_data.items():
            label = node_data.get("label", "Entity")
            name = node_data.get("title") or node_id
            
            props = {
                "id": node_id,
                "name": name
            }
            
            # Th√™m URL n·∫øu c√≥
            if "url" in node_data:
                props["url"] = node_data["url"]
            
            # Th√™m properties n·∫øu c√≥
            if "properties" in node_data:
                for k, v in node_data["properties"].items():
                    if k not in props:
                        props[k] = v
            
            # Th√™m infobox fields cho c√°c labels ch√≠nh
            if label in ("Artist", "Group", "Song", "Album", "Company", "Genre"):
                infobox = node_data.get("infobox") or {}
                if isinstance(infobox, dict) and infobox:
                    for raw_k, raw_v in infobox.items():
                        k_norm = _norm_key(raw_k)
                        if k_norm not in ("id", "name", "url"):
                            props[k_norm] = str(raw_v)
            
            label_to_nodes.setdefault(label, []).append({
                "id": node_id,
                "props": props
            })
        
        print(f"  ‚úì ƒê√£ chu·∫©n b·ªã {len(nodes_data)} nodes")
        for label, items in label_to_nodes.items():
            print(f"    - {label}: {len(items)} nodes")
        
        # Prepare relationships
        print("\nüìä B∆∞·ªõc 2: Chu·∫©n b·ªã relationships...")
        relationships: List[Dict[str, Any]] = []
        edges_data = merged_data.get("edges", [])
        
        for edge in edges_data:
            src = edge.get("source")
            tgt = edge.get("target")
            typ = edge.get("type") or "RELATED_TO"
            
            if not src or not tgt:
                continue
            
            rel_props = {"text": edge.get("text", "")}
            
            # Th√™m properties n·∫øu c√≥
            if "properties" in edge:
                rel_props.update(edge["properties"])
            
            relationships.append({
                "sourceId": src,
                "targetId": tgt,
                "type": typ,
                "props": rel_props
            })
        
        print(f"  ‚úì ƒê√£ chu·∫©n b·ªã {len(relationships)} relationships")
        
        # Group relationships by type
        type_to_rels: Dict[str, List[Dict[str, Any]]] = {}
        for r in relationships:
            type_to_rels.setdefault(r["type"], []).append(r)
        
        for rel_type, rels in type_to_rels.items():
            print(f"    - {rel_type}: {len(rels)} relationships")
        
        # Cypher templates
        node_query_tpl = lambda label: f"""
        UNWIND $batch AS n
        MERGE (x:`{label}` {{id: n.id}})
        SET x += n.props
        """
        
        rel_query_tpl = lambda rel_type: f"""
        UNWIND $batch AS r
        MATCH (s {{id: r.sourceId}}), (t {{id: r.targetId}})
        MERGE (s)-[e:`{rel_type}`]->(t)
        SET e += r.props
        """
        
        # Open session
        print("\nüìä B∆∞·ªõc 3: K·∫øt n·ªëi Neo4j v√† import d·ªØ li·ªáu...")
        with driver.session(database=database) if database else driver.session() as session:
            # T·∫°o constraints n·∫øu c·∫ßn
            if create_constraints:
                print("  üîß ƒêang t·∫°o constraints...")
                constraints = [
                    "CREATE CONSTRAINT artist_id IF NOT EXISTS FOR (n:Artist) REQUIRE n.id IS UNIQUE",
                    "CREATE CONSTRAINT group_id IF NOT EXISTS FOR (n:Group) REQUIRE n.id IS UNIQUE",
                    "CREATE CONSTRAINT album_id IF NOT EXISTS FOR (n:Album) REQUIRE n.id IS UNIQUE",
                    "CREATE CONSTRAINT song_id IF NOT EXISTS FOR (n:Song) REQUIRE n.id IS UNIQUE",
                    "CREATE CONSTRAINT company_id IF NOT EXISTS FOR (n:Company) REQUIRE n.id IS UNIQUE",
                    "CREATE CONSTRAINT genre_id IF NOT EXISTS FOR (n:Genre) REQUIRE n.id IS UNIQUE",
                ]
                for q in constraints:
                    try:
                        session.execute_write(run_write, q)
                    except Exception as e:
                        # Constraint c√≥ th·ªÉ ƒë√£ t·ªìn t·∫°i
                        pass
                print("  ‚úì ƒê√£ t·∫°o constraints")
            
            # Import nodes theo label
            print("\nüìä B∆∞·ªõc 4: ƒêang import nodes...")
            total_nodes_imported = 0
            for label, items in label_to_nodes.items():
                print(f"  üì• ƒêang import {len(items)} {label} nodes...")
                for i in range(0, len(items), batch_size):
                    batch = items[i:i+batch_size]
                    session.execute_write(run_write, node_query_tpl(label), {"batch": batch})
                    total_nodes_imported += len(batch)
                    if (i // batch_size + 1) % 10 == 0:
                        print(f"    ‚úì ƒê√£ import {total_nodes_imported} nodes...")
            print(f"  ‚úì ƒê√£ import {total_nodes_imported} nodes")
            
            # Import relationships theo type
            print("\nüìä B∆∞·ªõc 5: ƒêang import relationships...")
            total_rels_imported = 0
            for rel_type, rels in type_to_rels.items():
                print(f"  üì• ƒêang import {len(rels)} {rel_type} relationships...")
                for i in range(0, len(rels), batch_size):
                    batch = rels[i:i+batch_size]
                    session.execute_write(run_write, rel_query_tpl(rel_type), {"batch": batch})
                    total_rels_imported += len(batch)
                    if (i // batch_size + 1) % 10 == 0:
                        print(f"    ‚úì ƒê√£ import {total_rels_imported} relationships...")
            print(f"  ‚úì ƒê√£ import {total_rels_imported} relationships")
        
        print("\n" + "=" * 70)
        print("‚úì HO√ÄN T·∫§T IMPORT V√ÄO NEO4J")
        print("=" * 70)
        
    finally:
        driver.close()


def main():
    """H√†m main"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Merge 3 file JSON v√† import v√†o Neo4j")
    parser.add_argument("--bfs-file", default="data/korean_artists_graph_bfs.json",
                        help="File BFS graph JSON")
    parser.add_argument("--ner-file", default="data/kpop_ner_result.json",
                        help="File NER result JSON")
    parser.add_argument("--relationships-file", default="data/kpop_relationships_result.json",
                        help="File relationships result JSON")
    parser.add_argument("--output-file", default="data/merged_kpop_data.json",
                        help="File output merged")
    parser.add_argument("--neo4j-uri", default="bolt://localhost:7687",
                        help="Neo4j URI")
    parser.add_argument("--neo4j-user", default="neo4j",
                        help="Neo4j username")
    parser.add_argument("--neo4j-password", required=True,
                        help="Neo4j password")
    parser.add_argument("--neo4j-database", default=None,
                        help="Neo4j database name (None = default)")
    parser.add_argument("--batch-size", type=int, default=1000,
                        help="Batch size cho import")
    parser.add_argument("--no-constraints", action="store_true",
                        help="Kh√¥ng t·∫°o constraints")
    parser.add_argument("--merge-only", action="store_true",
                        help="Ch·ªâ merge, kh√¥ng import v√†o Neo4j")
    
    args = parser.parse_args()
    
    # Load d·ªØ li·ªáu
    bfs_data = load_json_file(args.bfs_file)
    ner_data = load_json_file(args.ner_file)
    relationships_data = load_json_file(args.relationships_file)
    
    # Merge d·ªØ li·ªáu
    merged_data = merge_data(bfs_data, ner_data, relationships_data, args.output_file)
    
    # Import v√†o Neo4j n·∫øu kh√¥ng ph·∫£i merge-only
    if not args.merge_only:
        import_to_neo4j(
            merged_data,
            args.neo4j_uri,
            args.neo4j_user,
            args.neo4j_password,
            args.neo4j_database,
            args.batch_size,
            not args.no_constraints
        )


if __name__ == "__main__":
    main()

