# -*- coding: utf-8 -*-
"""
Script crawl infobox t·ª´ Wikipedia cho c√°c node Group/Artist.
S·ª¨ D·ª§NG BEAUTIFULSOUP ƒê·ªÇ PARSE HTML (gi·ªëng korean_music_bfs.py)

Quy tr√¨nh:
1. Load file korean_artists_graph_bfs.json
2. T√¨m c√°c node c√≥ label = "Group" ho·∫∑c "Artist" v√† c√≥ URL
3. Truy c·∫≠p trang Wikipedia, parse HTML b·∫±ng BeautifulSoup
4. L·∫•y infobox, tr√≠ch xu·∫•t c√°c tr∆∞·ªùng th√†nh vi√™n:
   - Group: Th√†nh vi√™n, C·ª±u th√†nh vi√™n, Current members, Past members...
   - Artist: Associated acts, Th√†nh vi√™n c·ªßa...
5. L∆∞u k·∫øt qu·∫£ v√†o file: infobox_members.json

∆ØU ƒêI·ªÇM:
- L·∫•y ƒë∆∞·ª£c C·∫¢ text c√≥ link v√† kh√¥ng c√≥ link
- Parse HTML rendered d·ªÖ h∆°n parse wikitext
- Ch√≠nh x√°c h∆°n v√¨ d√πng c√πng c√°ch v·ªõi korean_music_bfs.py
"""

import json
import re
import time
from typing import Dict, List, Optional, Tuple
from urllib.parse import urlparse, unquote, quote

import requests
from bs4 import BeautifulSoup


# User-Agent ƒë·ªÉ Wikipedia kh√¥ng ch·∫∑n
HEADERS = {
    "User-Agent": "KpopNetworkAnalyzer/1.0 (Educational project) Python/requests"
}

# C√°c tr∆∞·ªùng c·∫ßn l·∫•y cho GROUP
GROUP_KEYS = [
    "th√†nh vi√™n",
    "c·ª±u th√†nh vi√™n", 
    "th√†nh vi√™n hi·ªán t·∫°i",
    "th√†nh vi√™n c≈©",
    "th√†nh vi√™n ban ƒë·∫ßu",
    "members",
    "former members",
    "current members",
    "past members",
]

# C√°c tr∆∞·ªùng c·∫ßn l·∫•y cho ARTIST
ARTIST_KEYS = [
    "th√†nh vi√™n c·ªßa",
    "c·ª±u th√†nh vi√™n c·ªßa",
    "nh√≥m nh·∫°c",
    "group",
    "groups",
    "associated acts",
]


def get_title_from_url(url: str) -> Optional[str]:
    """L·∫•y title Wikipedia t·ª´ URL."""
    if not url:
        return None
    try:
        path = urlparse(url).path
        if "/wiki/" not in path:
            return None
        title = path.split("/wiki/")[-1]
        return unquote(title) if title else None
    except Exception:
        return None


def fetch_page_soup(url: str) -> Optional[BeautifulSoup]:
    """
    Truy c·∫≠p trang Wikipedia v√† tr·∫£ v·ªÅ BeautifulSoup object.
    Gi·ªëng c√°ch l√†m c·ªßa korean_music_bfs.py
    """
    try:
        response = requests.get(url, headers=HEADERS, timeout=15)
        response.raise_for_status()
        return BeautifulSoup(response.content, "html.parser")
    except Exception as e:
        print(f"    [!] L·ªói fetch: {e}")
        return None


def extract_infobox_from_soup(soup: BeautifulSoup) -> Dict[str, str]:
    """
    Tr√≠ch xu·∫•t infobox t·ª´ BeautifulSoup.
    H·ªçc h·ªèi t·ª´ korean_music_bfs.py nh∆∞ng ƒë∆°n gi·∫£n h√≥a.
    
    L·∫•y C·∫¢ text c√≥ link v√† kh√¥ng c√≥ link.
    """
    if not soup:
        return {}
    
    # T√¨m b·∫£ng infobox
    infobox = soup.find("table", class_="infobox")
    if not infobox:
        # Th·ª≠ t√¨m c√°c class kh√°c
        infobox = soup.find("table", class_=re.compile(r"infobox", re.IGNORECASE))
    
    if not infobox:
        return {}
    
    result = {}
    
    # Duy·ªát qua t·∫•t c·∫£ c√°c h√†ng trong infobox
    rows = infobox.find_all("tr")
    
    for row in rows:
        # T√¨m header (th) v√† data (td)
        th = row.find("th")
        td = row.find("td")
        
        if not th or not td:
            continue
        
        # L·∫•y key t·ª´ header
        key = th.get_text(strip=True)
        if not key:
            continue
        
        # L·∫•y value t·ª´ td - L·∫§Y C·∫¢ TEXT C√ì LINK V√Ä KH√îNG C√ì LINK
        value = extract_cell_value(td)
        
        if value:
            result[key] = value
    
    return result


def extract_cell_value(td) -> str:
    """
    Tr√≠ch xu·∫•t gi√° tr·ªã t·ª´ m·ªôt √¥ td trong infobox.
    X·ª≠ l√Ω c·∫£ tr∆∞·ªùng h·ª£p c√≥ link v√† kh√¥ng c√≥ link.
    
    V√≠ d·ª•:
    - <a href="...">Xiumin</a> -> "Xiumin"
    - Suho (kh√¥ng c√≥ link) -> "Suho"
    - <a>Lay</a>, <a>Baekhyun</a> -> "Lay, Baekhyun"
    """
    if not td:
        return ""
    
    # X√≥a c√°c ph·∫ßn t·ª≠ kh√¥ng c·∫ßn thi·∫øt
    for elem in td.find_all(["sup", "style", "script"]):
        elem.decompose()
    
    # Thay th·∫ø <br> b·∫±ng d·∫•u ph√¢n c√°ch ƒë·∫∑c bi·ªát
    for br in td.find_all("br"):
        br.replace_with(" |SEPARATOR| ")
    
    # X·ª≠ l√Ω <li> - m·ªói li l√† m·ªôt item
    for li in td.find_all("li"):
        li.insert_before(" |SEPARATOR| ")
    
    # L·∫•y text t·ª´ t·∫•t c·∫£ c√°c ph·∫ßn t·ª≠
    # ∆Øu ti√™n l·∫•y text t·ª´ <a> tags tr∆∞·ªõc
    members = []
    
    # T√¨m t·∫•t c·∫£ c√°c link <a> - ƒë√¢y th∆∞·ªùng l√† t√™n th√†nh vi√™n
    links = td.find_all("a")
    if links:
        for link in links:
            text = link.get_text(strip=True)
            if text and len(text) >= 2:
                # B·ªè qua c√°c link kh√¥ng ph·∫£i t√™n ng∆∞·ªùi
                if text.lower() not in ['edit', 's·ª≠a', 'xem', 'view', 'more']:
                    members.append(text)
    
    # N·∫øu kh√¥ng c√≥ link, l·∫•y to√†n b·ªô text v√† t√°ch theo separator
    if not members:
        full_text = td.get_text(separator=" ")
        # T√°ch theo c√°c d·∫•u ph√¢n c√°ch
        full_text = full_text.replace("|SEPARATOR|", ",")
        parts = re.split(r'[,‚Ä¢¬∑*\n]+', full_text)
        for part in parts:
            part = part.strip()
            if part and len(part) >= 2:
                members.append(part)
    
    # N·∫øu v·∫´n r·ªóng, l·∫•y raw text
    if not members:
        raw_text = td.get_text(strip=True)
        if raw_text:
            members = [raw_text]
    
    # G·ªôp th√†nh chu·ªói v·ªõi d·∫•u ph·∫©y
    raw_text = ", ".join(members)
    
    # L√†m s·∫°ch
    text = clean_member_text(raw_text)
    
    return text


def clean_member_text(text: str) -> str:
    """
    L√†m s·∫°ch text ch·ª©a danh s√°ch th√†nh vi√™n.
    """
    if not text:
        return ""
    
    # B·ªè c√°c tham chi·∫øu [1], [2], etc.
    text = re.sub(r'\[\d+\]', '', text)
    text = re.sub(r'\[.*?\]', '', text)
    
    # B·ªè separator marker
    text = text.replace("|SEPARATOR|", ",")
    
    # B·ªè c√°c ghi ch√∫ trong ngo·∫∑c ƒë∆°n c√≥ ch·ª©a nƒÉm ho·∫∑c th√¥ng tin ph·ª•
    # Nh∆∞ng gi·ªØ l·∫°i t√™n ngh·ªá sƒ© trong ngo·∫∑c
    # V√≠ d·ª•: "Hana (Zinger)" -> gi·ªØ nguy√™n, "(2006-2011)" -> b·ªè
    text = re.sub(r'\(\d{4}[‚Äì-]\d{0,4}\)', '', text)
    text = re.sub(r'\s*\(‚Ä†\)\s*', ' (‚Ä†)', text)  # Gi·ªØ l·∫°i d·∫•u ‚Ä† cho ng∆∞·ªùi ƒë√£ m·∫•t
    
    # Thay th·∫ø c√°c d·∫•u ph√¢n c√°ch th√†nh d·∫•u ph·∫©y
    text = re.sub(r'\s*[‚Ä¢¬∑]\s*', ', ', text)
    text = re.sub(r'\s*\*\s*', ', ', text)
    
    # Chu·∫©n h√≥a d·∫•u ph·∫©y - ƒë·∫£m b·∫£o c√≥ kho·∫£ng tr·∫Øng sau d·∫•u ph·∫©y
    text = re.sub(r'\s*,\s*', ', ', text)
    text = re.sub(r',\s*,+', ', ', text)
    text = re.sub(r'^[,\s]+', '', text)
    text = re.sub(r'[,\s]+$', '', text)
    
    # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    
    return text


def filter_member_keys(infobox: Dict[str, str], target_keys: List[str]) -> Dict[str, str]:
    """
    L·ªçc ch·ªâ l·∫•y c√°c tr∆∞·ªùng li√™n quan ƒë·∫øn th√†nh vi√™n t·ª´ infobox.
    
    Args:
        infobox: Dictionary ch·ª©a t·∫•t c·∫£ c√°c tr∆∞·ªùng t·ª´ infobox
        target_keys: Danh s√°ch c√°c key c·∫ßn l·∫•y (lowercase)
    
    Returns:
        Dictionary ch·ªâ ch·ª©a c√°c tr∆∞·ªùng th√†nh vi√™n
    """
    result = {}
    
    for key, value in infobox.items():
        key_lower = key.lower().strip()
        
        for target in target_keys:
            if target in key_lower or key_lower in target:
                # ƒê√£ t√¨m th·∫•y key ph√π h·ª£p
                # Chu·∫©n h√≥a t√™n key
                if 'former' in key_lower or 'past' in key_lower or 'c·ª±u' in key_lower or 'c≈©' in key_lower:
                    normalized_key = "Past members" if 'member' in key_lower or 'th√†nh vi√™n' in key_lower else "Former members"
                elif 'current' in key_lower or 'hi·ªán t·∫°i' in key_lower:
                    normalized_key = "Current members"
                elif 'th√†nh vi√™n' in key_lower or 'member' in key_lower:
                    normalized_key = "Current members"
                elif 'associated' in key_lower:
                    normalized_key = "Associated acts"
                else:
                    normalized_key = key  # Gi·ªØ nguy√™n
                
                # Ch·ªâ th√™m n·∫øu value kh√¥ng r·ªóng
                if value and len(value) > 1:
                    result[normalized_key] = value
                break
    
    return result


def main():
    print("=" * 60)
    print("CRAWL INFOBOX TH√ÄNH VI√äN T·ª™ WIKIPEDIA (HTML VERSION)")
    print("=" * 60)
    print("S·ª≠ d·ª•ng BeautifulSoup ƒë·ªÉ parse HTML - l·∫•y c·∫£ text c√≥ link v√† kh√¥ng link")

    # 1. Load file g·ªëc
    print("\nüìÇ B∆∞·ªõc 1: Load file korean_artists_graph_bfs.json...")
    with open("korean_artists_graph_bfs.json", "r", encoding="utf-8") as f:
        graph = json.load(f)

    nodes = graph.get("nodes", graph)
    print(f"   ‚úì T·ªïng c·ªông {len(nodes)} nodes")

    # 2. T√¨m c√°c node Group v√† Artist c√≥ URL
    print("\nüîç B∆∞·ªõc 2: T√¨m c√°c node Group/Artist c√≥ URL...")
    groups_to_crawl = []
    artists_to_crawl = []

    for node_id, node in nodes.items():
        if not isinstance(node, dict):
            continue

        label = node.get("label")
        url = node.get("url")

        if not url or "wikipedia.org" not in url:
            continue

        if label == "Group":
            groups_to_crawl.append((node_id, url))
        elif label == "Artist":
            artists_to_crawl.append((node_id, url))

    print(f"   ‚úì T√¨m th·∫•y {len(groups_to_crawl)} Groups c√≥ URL")
    print(f"   ‚úì T√¨m th·∫•y {len(artists_to_crawl)} Artists c√≥ URL")

    # 3. Crawl infobox
    print("\nüåê B∆∞·ªõc 3: Crawl infobox t·ª´ Wikipedia (HTML parsing)...")
    results = {
        "groups": {},
        "artists": {},
    }

    # Crawl Groups
    print(f"\n   --- Crawling {len(groups_to_crawl)} Groups ---")
    success_groups = 0
    
    for idx, (node_id, url) in enumerate(groups_to_crawl, 1):
        if idx % 20 == 0 or idx == 1:
            print(f"   [{idx}/{len(groups_to_crawl)}] {node_id[:50]}...")

        soup = fetch_page_soup(url)
        if soup:
            full_infobox = extract_infobox_from_soup(soup)
            member_info = filter_member_keys(full_infobox, GROUP_KEYS)
            
            if member_info:
                results["groups"][node_id] = {
                    "url": url,
                    "infobox": member_info,
                }
                success_groups += 1

        time.sleep(0.3)  # Tr√°nh spam

    print(f"   ‚úì Crawl th√†nh c√¥ng {success_groups}/{len(groups_to_crawl)} Groups")

    # Crawl Artists
    print(f"\n   --- Crawling {len(artists_to_crawl)} Artists ---")
    success_artists = 0
    
    for idx, (node_id, url) in enumerate(artists_to_crawl, 1):
        if idx % 20 == 0 or idx == 1:
            print(f"   [{idx}/{len(artists_to_crawl)}] {node_id[:50]}...")

        soup = fetch_page_soup(url)
        if soup:
            full_infobox = extract_infobox_from_soup(soup)
            member_info = filter_member_keys(full_infobox, ARTIST_KEYS)
            
            if member_info:
                results["artists"][node_id] = {
                    "url": url,
                    "infobox": member_info,
                }
                success_artists += 1

        time.sleep(0.3)

    print(f"   ‚úì Crawl th√†nh c√¥ng {success_artists}/{len(artists_to_crawl)} Artists")

    # 4. L∆∞u k·∫øt qu·∫£
    print("\nüíæ B∆∞·ªõc 4: L∆∞u k·∫øt qu·∫£...")
    output_file = "infobox_members.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"   ‚úì ƒê√£ l∆∞u v√†o {output_file}")
    
    # 5. Hi·ªÉn th·ªã m·ªôt s·ªë v√≠ d·ª•
    print(f"\nüìä K·∫øt qu·∫£:")
    print(f"   - Groups c√≥ infobox th√†nh vi√™n: {len(results['groups'])}")
    print(f"   - Artists c√≥ infobox li√™n quan: {len(results['artists'])}")
    
    # Hi·ªÉn th·ªã v√≠ d·ª•
    print(f"\nüìã M·ªôt s·ªë v√≠ d·ª• Groups:")
    count = 0
    for group_name, data in results["groups"].items():
        if count >= 5:
            break
        infobox = data.get("infobox", {})
        members = infobox.get("Current members", infobox.get("Past members", "N/A"))
        if len(members) > 80:
            members = members[:80] + "..."
        print(f"   ‚Ä¢ {group_name}: {members}")
        count += 1
    
    print("\nüéâ Ho√†n t·∫•t!")


if __name__ == "__main__":
    main()
